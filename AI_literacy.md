# äººå·¥æ™ºèƒ½æ¦‚è§ˆ

*Updated 2025-11-27 15:00 GMT+8*  
*Compiled by Hongfei Yan (2025 Summer)*    
https://github.com/GMyhf/2025fall-cs201/





äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯ç ”ç©¶å’Œå¼€å‘èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æŠ€æœ¯å’Œæ–¹æ³•çš„å­¦ç§‘ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€å›¾åƒç†è§£ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨äººç­‰[1] [2]ã€‚æ—©åœ¨1950å¹´ï¼Œè‰¾ä¼¦Â·å›¾çµæå‡ºäº†æ£€éªŒæœºå™¨æ™ºèƒ½çš„â€œæ¨¡ä»¿æ¸¸æˆâ€ï¼ˆå³**å›¾çµæµ‹è¯•**ï¼‰ï¼Œæ£€éªŒæœºå™¨æ˜¯å¦èƒ½è®©äººåˆ†ä¸æ¸…å…¶ä¸äººç±»å¯¹è¯çš„åŒºåˆ«ã€‚1956å¹´è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ï¼ˆ**Dartmouth AI Workshop**ï¼‰å¬å¼€ï¼Œè¢«è®¤ä¸ºæ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„åˆ›å§‹æ—¶åˆ»ï¼Œçº¦ç¿°Â·éº¦å¡é”¡ç­‰äººé¦–æ¬¡æ­£å¼æå‡ºâ€œäººå·¥æ™ºèƒ½â€è¿™ä¸€æœ¯è¯­[1]ã€‚æ­¤åï¼ŒAIå‘å±•ç»å†äº†å¤šæ¬¡é«˜æ½®ä¸ä½è°·ï¼Œåˆ°21ä¸–çºªä¾èµ–äºå¼ºå¤§çš„è®¡ç®—èµ„æºã€æµ·é‡æ•°æ®å’Œæ–°ç®—æ³•çš„**æ·±åº¦å­¦ä¹ **æŠ€æœ¯å®ç°çªç ´ï¼Œæ¨åŠ¨AIè¿›å…¥å¹¿æ³›åº”ç”¨é˜¶æ®µã€‚



# 1. AIä¸‰å¤§æµæ´¾

äººå·¥æ™ºèƒ½çš„å‘å±•æ€æƒ³åˆ†ä¸ºä¸‰ä¸ªä¸»è¦æµæ´¾ï¼š

- **ç¬¦å·ä¸»ä¹‰ï¼ˆSymbolic AIï¼‰**ï¼šæ ¸å¿ƒè§‚ç‚¹æ˜¯é€šè¿‡ç¬¦å·è¡¨ç¤ºå’Œé€»è¾‘æ¨ç†æ¥å®ç°æ™ºèƒ½ã€‚å…¸å‹æ–¹æ³•åŒ…æ‹¬ä¸“å®¶ç³»ç»Ÿã€æœç´¢ç®—æ³•ã€é€»è¾‘æ¨ç†ï¼ˆå¦‚ä¸€é˜¶é€»è¾‘ï¼‰ã€è§„åˆ’ç³»ç»Ÿç­‰ã€‚ä»£è¡¨äººç‰©æœ‰è‰¾ä¼¦Â·çº½å„å°”ã€èµ«ä¼¯ç‰¹Â·è¥¿è’™ã€çº¦ç¿°Â·éº¦å¡é”¡ç­‰ã€‚ç¬¦å·ä¸»ä¹‰å¼ºè°ƒå¯è§£é‡Šæ€§å¼ºï¼Œé€‚ç”¨äºæœ‰æ˜ç¡®è§„åˆ™çš„ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦æ¨ç†ã€æ£‹ç±»æ¸¸æˆï¼‰ã€‚å…¶ä»£è¡¨æˆæœåŒ…æ‹¬20ä¸–çºª80å¹´ä»£çš„ä¸“å®¶ç³»ç»Ÿï¼ˆå¦‚DECå…¬å¸çš„XCONç³»ç»Ÿï¼Œæ˜¾è‘—æé«˜äº†é…ç½®æ•ˆç‡ï¼‰ä»¥åŠIBMçš„æ£‹ç±»ç¨‹åº**æ·±è“ï¼ˆDeep Blueï¼‰**ï¼Œ1997å¹´å‡»è´¥å›½é™…è±¡æ£‹å† å†›å¡æ–¯å¸•ç½—å¤«ã€‚ä½†ç¬¦å·ä¸»ä¹‰çš„ç¼ºç‚¹æ˜¯å­¦ä¹ èƒ½åŠ›å¼±ï¼Œéš¾ä»¥å¤„ç†æ¨¡ç³Šä¿¡æ¯ï¼Œéœ€è¦å¤§é‡æ‰‹å·¥ç¼–ç è§„åˆ™ã€‚

- **è¿æ¥ä¸»ä¹‰ï¼ˆConnectionismï¼‰**ï¼šæ ¸å¿ƒè§‚ç‚¹æ˜¯é€šè¿‡æ¨¡æ‹Ÿäººè„‘ç¥ç»å…ƒç½‘ç»œç»“æ„æ¥å®ç°æ™ºèƒ½ï¼Œä¾èµ–æ•°æ®é©±åŠ¨å­¦ä¹ ã€‚ä¸»è¦æ–¹æ³•æ˜¯å„ç§äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼ˆå¦‚å·ç§¯ç¥ç»ç½‘ç»œCNNã€å¾ªç¯ç¥ç»ç½‘ç»œRNNã€Transformerç­‰ï¼‰ã€‚ä»£è¡¨äººç‰©åŒ…æ‹¬Geoffrey Hintonã€Yann LeCunã€Ilya Sutskeverã€David Rumelhartç­‰ã€‚çº¦ç¿°Â·éœæ™®è²å°”å¾·ï¼ˆJohn Hopfieldï¼‰ä¸Geoffrey Hintonäº2024å¹´è·å¾—è¯ºè´å°”ç‰©ç†å­¦å¥–ï¼Œä»¥è¡¨å½°ä»–ä»¬åœ¨ç¥ç»ç½‘ç»œé¢†åŸŸçš„å¥ åŸºæ€§è´¡çŒ®ã€‚è¿æ¥ä¸»ä¹‰æµæ´¾å¼•é¢†äº†è¿‘å¹´æ¥AIçš„ä¸»è¦çªç ´ï¼ˆâ€œå¼ºæ•°æ®ã€å¼±è§„åˆ™â€ï¼‰ã€‚å…¶ä»£è¡¨æˆæœåŒ…æ‹¬æœ€æ—©çš„å•å±‚æ„ŸçŸ¥æœºï¼ˆPerceptronï¼Œ1958å¹´ï¼‰ä»¥åŠ1986å¹´Rumelhartç­‰äººæå‡ºçš„**è¯¯å·®åå‘ä¼ æ’­ç®—æ³•**ï¼ˆBackpropagationï¼‰ï¼Œä½¿å¤šå±‚ç¥ç»ç½‘ç»œå¾—ä»¥é«˜æ•ˆè®­ç»ƒã€‚ç°ä»£è¿æ¥ä¸»ä¹‰ç³»ç»Ÿåœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå‡å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä¾‹å¦‚Googleçš„AlphaGoå’Œå„ç§è§†è§‰æ¨¡å‹ã€OpenAIçš„GPTç³»åˆ—è¯­è¨€æ¨¡å‹ç­‰[3]ã€‚
- **è¡Œä¸ºä¸»ä¹‰ï¼ˆBehaviorismï¼‰**ï¼šåœ¨AIé¢†åŸŸå¸¸æŒ‡â€œæœºå™¨äººè¡ŒåŠ¨æ´¾â€æˆ–å¼ºåŒ–å­¦ä¹ æ€æƒ³ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å¹¶æ ¹æ®åé¦ˆï¼ˆå¥–æƒ©ï¼‰è‡ªä¸»å­¦ä¹ ï¼Œæ— éœ€äº‹å…ˆå‡è®¾å†…éƒ¨çŸ¥è¯†ç»“æ„ã€‚ä»£è¡¨äººç‰©æœ‰ç½—å¾·å°¼Â·å¸ƒé²å…‹æ–¯ï¼ˆRodney Brooksï¼‰ã€æå¼€å¤ç­‰ã€‚è¡Œä¸ºä¸»ä¹‰AIå¼ºè°ƒâ€œè¡ŒåŠ¨ä¼˜å…ˆâ€ï¼Œå¯¹ç°ä»£æœºå™¨äººå­¦å’Œå¼ºåŒ–å­¦ä¹ å½±å“æ·±è¿œã€‚å…¸å‹åº”ç”¨æ˜¯åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç³»ç»Ÿï¼Œå¦‚**AlphaGo/AlphaZero**ï¼ˆé€šè¿‡è‡ªæˆ‘åšå¼ˆå­¦ä¹ å›´æ£‹ç­–ç•¥ï¼‰å’Œè‡ªåŠ¨é©¾é©¶ç­‰ã€‚è¡Œä¸ºä¸»ä¹‰æµæ´¾ä¸‹çš„æ™ºèƒ½ä½“å¯è§†ä¸ºé€šè¿‡è¯•é”™å’Œç¯å¢ƒåé¦ˆæ¥ä¼˜åŒ–å†³ç­–ã€‚



# 2. AIçš„â€œä¸‰è¦ç´ â€ï¼šç®—æ³•ã€ç®—åŠ›ã€æ•°æ®

äººå·¥æ™ºèƒ½çš„å‘å±•ä¾èµ–äºä¸‰å¤§è¦ç´ ï¼š**ç®—æ³•**ï¼ˆAlgorithmï¼‰ã€**ç®—åŠ›**ï¼ˆComputeï¼‰å’Œ**æ•°æ®**ï¼ˆDataï¼‰ã€‚

- **ç®—æ³•ï¼ˆçµé­‚ï¼‰**ï¼šä¸åŒä»»åŠ¡ç±»å‹å¯¹åº”ä¸åŒç®—æ³•èŒƒå¼ã€‚å¸¸è§åˆ†ç±»åŒ…æ‹¬ç›‘ç£å­¦ä¹ ï¼ˆä½¿ç”¨æ ‡æ³¨æ•°æ®è®­ç»ƒæ¨¡å‹è¿›è¡Œåˆ†ç±»æˆ–å›å½’ï¼‰ã€æ— ç›‘ç£å­¦ä¹ ï¼ˆä»æ— æ ‡ç­¾æ•°æ®ä¸­æŒ–æ˜ç»“æ„ï¼Œå¦‚èšç±»ï¼‰ã€å¼ºåŒ–å­¦ä¹ ï¼ˆä½¿ç”¨å¥–æƒ©æœºåˆ¶ä¼˜åŒ–ç­–ç•¥ï¼‰ã€‚ä¾‹å¦‚ï¼Œé€»è¾‘å›å½’ç”¨äºåˆ†ç±»ï¼ˆç›‘ç£å­¦ä¹ ï¼‰ï¼ŒKMeansç”¨äºèšç±»ï¼ˆæ— ç›‘ç£å­¦ä¹ ï¼‰ã€‚æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„ç®—æ³•ä¸æ–­æ¼”è¿›ï¼Œå¼•å…¥äº†å¤šå±‚ç¥ç»ç½‘ç»œã€æ³¨æ„åŠ›æœºåˆ¶ç­‰åˆ›æ–°æ¶æ„ã€‚
- **ç®—åŠ›ï¼ˆå¼•æ“ï¼‰**ï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹å¾€å¾€éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºã€‚GPU/TPUç­‰é«˜æ€§èƒ½ç¡¬ä»¶ä½¿å¾—è®­ç»ƒå¤§è§„æ¨¡ç¥ç»ç½‘ç»œæˆä¸ºå¯èƒ½ã€‚ä»¥PyTorchä¸ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•æ£€æµ‹å½“å‰ç¡¬ä»¶ç¯å¢ƒä¸­GPUæˆ–Apple MPSçš„å¯ç”¨æ€§ã€‚
- **æ•°æ®ï¼ˆç‡ƒæ–™ï¼‰**ï¼šè®­ç»ƒæ¨¡å‹éœ€è¦å¤§é‡é«˜è´¨é‡çš„æ•°æ®ã€‚ç›‘ç£å­¦ä¹ å°¤å…¶ä¾èµ–æ ‡æ³¨æ•°æ®ã€‚ä¾‹å¦‚ï¼Œæé£é£ç­‰äººåœ¨2006å¹´å‘èµ·çš„ImageNetè®¡åˆ’æ”¶é›†äº†æ•°åƒä¸‡å¼ å›¾åƒï¼Œå¹¶ä¾æ‰˜ä¼—åŒ…æ ‡æ³¨åˆ›é€ äº†åŒ…å«1400ä¸‡å¼ æ ‡æ³¨å›¾ç‰‡çš„å¤§å‹æ•°æ®é›†ï¼Œå¤§å¤§æ¨åŠ¨äº†è®¡ç®—æœºè§†è§‰ç®—æ³•çš„å‘å±•ã€‚ä¸æ­¤åŒæ—¶ï¼Œæ— æ ‡ç­¾æ•°æ®ä¹Ÿé€šè¿‡è‡ªåŠ¨è®°å½•ç­‰æ–¹å¼æä¾›äº†æµ·é‡ä¿¡æ¯ï¼Œå¯ç”¨äºæ— ç›‘ç£å­¦ä¹ ã€‚æ€»ä¹‹ï¼Œç®—æ³•ã€ç®—åŠ›ä¸æ•°æ®ä¸‰è€…å…±åŒæ„æˆAIç³»ç»Ÿçš„åŸºç¡€ã€‚



# 3. å‰æ²¿åº”ç”¨æ¡ˆä¾‹

- **æ™ºèƒ½åšå¼ˆï¼šAlphaGo/AlphaZero**ï¼šDeepMindçš„AlphaGoç»“åˆäº†æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€å¼ºåŒ–å­¦ä¹ å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œæˆä¸ºé¦–ä¸ªæˆ˜èƒœå›´æ£‹äººç±»å† å†›çš„AIç³»ç»Ÿã€‚2016å¹´AlphaGoä»¥4:1å‡»è´¥æä¸–çŸ³ï¼Œ2017å¹´ä»¥3:0æˆ˜èƒœæŸ¯æ´ã€‚å…¶å¼ºåŒ–å­¦ä¹ ç‰ˆæœ¬AlphaGo Zeroæ— éœ€äººç±»æ£‹è°±ï¼Œä»éšæœºå¯¹å¼ˆä¸­è‡ªå­¦ï¼Œç»è¿‡æ•°å‘¨è®­ç»ƒä¾¿è¶…è¶Šäº†åŸç‰ˆAlphaGoã€‚è¿›ä¸€æ­¥çš„AlphaZeroç”šè‡³èƒ½ä»é›¶å¼€å§‹è‡ªå­¦å¤šç§æ£‹ç±»ï¼ˆå›´æ£‹ã€å›½é™…è±¡æ£‹ã€æ—¥æœ¬å°†æ£‹ç­‰ï¼‰ï¼Œå±•ç°å‡ºè¶…å¼ºçš„ç­–ç•¥å­¦ä¹ èƒ½åŠ›ã€‚å®ƒè¯æ˜äº†æ”¾å¼ƒäººç±»ç»éªŒã€æœ‰æ¡ä»¶çš„è‡ªæˆ‘å¯¹å¼ˆï¼ˆself-playï¼‰å­¦ä¹ åœ¨æŸäº›é¢†åŸŸèƒ½å¸¦æ¥æ›´ä¼˜è§£ã€‚
- **è‡ªç„¶è¯­è¨€å¤„ç†ï¼šTransformerä¸GPT**ï¼šTransformeræ¨¡å‹ç”±Googleç ”ç©¶è€…åœ¨2017å¹´æå‡ºï¼ˆè‘—åè®ºæ–‡*Attention Is All You Need*ï¼‰ï¼Œå…¶æ ¸å¿ƒæ˜¯**è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼Œå…è®¸æ¨¡å‹å¹¶è¡Œå¤„ç†åºåˆ—å¹¶æ•æ‰è¿œè·ç¦»ä¾èµ–[4]ã€‚Transformeræ¶æ„å¹¿æ³›åº”ç”¨äºå¤§è§„æ¨¡è‡ªç„¶è¯­è¨€å¤„ç†å’Œå…¶å®ƒé¢†åŸŸï¼Œå‚¬ç”Ÿäº†ä¼—å¤šé¢„è®­ç»ƒæ¨¡å‹å¦‚GPTç³»åˆ—å’ŒBERT[4]ã€‚GPTï¼ˆGenerative Pre-trained Transformerï¼‰æ˜¯OpenAIæ¨å‡ºçš„ä¸€ç±»è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨å·¨å¤§çš„Transformerè§£ç å™¨ç»“æ„è¿›è¡Œæ— ç›‘ç£é¢„è®­ç»ƒåå†å¾®è°ƒã€‚GPT-3äº2020å¹´é—®ä¸–ï¼Œæ‹¥æœ‰çº¦1750äº¿å‚æ•°[2]ï¼Œèƒ½å¤Ÿç”Ÿæˆè¿è´¯æµç•…çš„æ–‡æœ¬ï¼Œæ”¯æŒé›¶æ ·æœ¬å­¦ä¹ ï¼ˆzero-shotï¼‰å’Œå°‘æ ·æœ¬å­¦ä¹ ï¼ˆfew-shotï¼‰ï¼Œåœ¨ç¿»è¯‘ã€å¯¹è¯ã€å†™ä½œç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚**GPT-4**ï¼ˆ2023å¹´å‘å¸ƒï¼‰åœ¨GPT-3.5åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ‰©å±•è§„æ¨¡å’Œèƒ½åŠ›ï¼Œæ˜¯ä¸€ä¸ªæ”¯æŒæ–‡æœ¬å’Œå›¾åƒè¾“å…¥çš„**å¤šæ¨¡æ€å¤§æ¨¡å‹**[5] [3]ã€‚GPT-4åœ¨åŒ…æ‹¬æ¨¡æ‹Ÿå¾‹å¸ˆèµ„æ ¼è€ƒè¯•ï¼ˆbar examï¼‰åœ¨å†…çš„å¤šé¡¹ä¸“ä¸šæµ‹è¯•ä¸­è¡¨ç°å‡ºç±»äººæ°´å¹³ï¼ˆæˆç»©åœ¨å‰10%ï¼‰[3]ã€‚ä¸å‰ä»£æ¨¡å‹ç›¸æ¯”ï¼ŒGPT-4æ›´åŠ å¯é ã€å¯Œæœ‰åˆ›é€ åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†æ›´å¤æ‚ã€æ›´é•¿çš„æŒ‡ä»¤[5]ã€‚GPTç³»åˆ—æ¨¡å‹è¢«å¹¿æ³›åº”ç”¨äºå¯¹è¯æœºå™¨äººã€å†…å®¹ç”Ÿæˆã€ç¼–ç¨‹è¾…åŠ©ã€æ•™è‚²è¾…å¯¼ç­‰åœºæ™¯ã€‚

**ç¤ºä¾‹ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé—®ç­”**ã€‚ä»¥Hugging Face Transformerä¸ºä¾‹ï¼Œä¸‹è¿° `first_qa.py`ä»£ç è½½å…¥æœ¬åœ°ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé—®ç­”æ¨ç†ï¼š



> | æ¨¡å‹                                     | é€‚ç”¨è¯­è¨€ | ç”¨é€”     |
> | ---------------------------------------- | -------- | -------- |
> | `distilbert-base-cased-distilled-squad`  | è‹±æ–‡     | è‹±æ–‡é—®ç­” |
> | `uer/roberta-base-chinese-extractive-qa` | ä¸­æ–‡     | ä¸­æ–‡é—®ç­” |
>
> Q: å¦‚ä½•ç”¨**æµè§ˆå™¨æ‰‹åŠ¨ä¸‹è½½** `uer/roberta-base-chinese-extractive-qa` æ¨¡å‹ï¼Œåšåˆ°å®Œå…¨ **ç¦»çº¿éƒ¨ç½²** çš„æ­¥éª¤ï¼š
>
> ğŸ”— 1. æ‰“å¼€æ¨¡å‹é¡µé¢
>
> æµè§ˆå™¨è®¿é—®ï¼šhttps://huggingface.co/uer/roberta-base-chinese-extractive-qa
>
> ------
>
> ğŸ“ 2. è¿›å…¥ â€œFiles and versionsâ€ é¡µé¢ï¼Œæ‰‹åŠ¨ä¸‹è½½ä»¥ä¸‹å‡ ä¸ªå…³é”®æ–‡ä»¶ï¼š
>
> | æ–‡ä»¶å                    | è¯´æ˜                           |
> | ------------------------- | ------------------------------ |
> | `config.json`             | æ¨¡å‹ç»“æ„é…ç½®                   |
> | `pytorch_model.bin`       | æ¨¡å‹æƒé‡ï¼ˆå¾ˆå¤§ï¼Œ400MB å·¦å³ï¼‰   |
> | `tokenizer_config.json`   | tokenizer é…ç½®                 |
> | `vocab.txt`               | ä¸­æ–‡è¯è¡¨ï¼ˆå¿…éœ€ï¼‰               |
> | `special_tokens_map.json` | ç‰¹æ®Šç¬¦å·å®šä¹‰ï¼ˆå¯é€‰ä½†æ¨èï¼‰     |
> | `tokenizer.json`          | tokenizer çš„äºŒè¿›åˆ¶å½¢å¼ï¼ˆå¯é€‰ï¼‰ |
>
> ä½ å¯ä»¥åœ¨ç½‘é¡µä¸­ä¾æ¬¡ç‚¹å‡»è¿™äº›æ–‡ä»¶ï¼Œç„¶åç‚¹å‡»å³ä¸Šè§’ â€œDownloadâ€ã€‚
>
> 
>
> æˆ–è€…è¿™é‡Œä¸‹è½½ï¼š
>
> https://disk.pku.edu.cn/link/AA5F507BA7BC504334ACA7FCBECFE64995
> Name: model-roberta-chinese-qa.zip
> Expires: Never
> Pickup Code: zXih



ğŸ—‚ï¸ æˆ‘çš„clabäº‘è™šæ‹Ÿæœºbeijing.zhengmao.ltdï¼Œ`AI_literacy`æ–‡ä»¶å¤¹

```
[rocky@jensen AI_literacy]$ tree
.
â”œâ”€â”€ first_qa.py
â””â”€â”€ models
    â””â”€â”€ roberta-chinese-qa
        â”œâ”€â”€ config.json
        â”œâ”€â”€ pytorch_model.bin
        â”œâ”€â”€ special_tokens_map.json
        â”œâ”€â”€ tokenizer_config.json
        â””â”€â”€ vocab.txt

```



**åˆ›å»ºç‹¬ç«‹çš„è™šæ‹Ÿç¯å¢ƒ & å®‰è£… æ·±åº¦å­¦ä¹ æ¡†æ¶ PyTorch**

> ä¸ºæ¯ä¸ªé¡¹ç›®åˆ›å»ºç‹¬ç«‹çš„è™šæ‹Ÿç¯å¢ƒï¼Œé¿å…ä¾èµ–å†²çªã€‚
>
> ```
> cd ~/AI_literacy  # æ›¿æ¢ä¸ºä½ çš„é¡¹ç›®è·¯å¾„
> python3 -m venv .venv
> source .venv/bin/activate
> ```
>
> > é€€å‡ºè™šæ‹Ÿç¯å¢ƒå‘½ä»¤ï¼š`deactivate`
>
> 
>
> å®‰è£… æ·±åº¦å­¦ä¹ æ¡†æ¶ PyTorch  
> pip install -U transformers  
> pip install torch



`first_qa.py`

```python
from transformers import pipeline

qa = pipeline(
    "question-answering",
    #model="./models/distilbert-base-cased-distilled-squad",
    model="./models/roberta-chinese-qa",
    tokenizer="./models/roberta-chinese-qa",
    framework="pt"  # æ˜¾å¼è¦æ±‚ä½¿ç”¨ PyTorch
)

result = qa(
    question="è°æ˜¯äººå·¥æ™ºèƒ½ä¹‹çˆ¶ï¼Ÿ",
    context="è‰¾ä¼¦Â·å›¾çµæ˜¯äººå·¥æ™ºèƒ½ä¹‹çˆ¶ï¼Œè¢«èª‰ä¸ºè®¡ç®—æœºç§‘å­¦çš„å¥ åŸºäººã€‚"
)

print(result)       # çœ‹å®Œæ•´ç»“æœ
print(result["answer"])  # è¾“å‡ºåº”ä¸ºï¼šè‰¾ä¼¦Â·å›¾çµ
```

ä¸Šè¿°ä»£ç ä¸­ï¼ŒæŒ‡å®šäº†æœ¬åœ°ä¸‹è½½çš„ä¸­æ–‡é—®ç­”æ¨¡å‹ç›®å½•ï¼Œé€šè¿‡pipelineæ¥å£ç›´æ¥è¿›è¡ŒæŠ½å–å¼é—®ç­”æ¨ç†ã€‚åœ¨çœŸå®åº”ç”¨ä¸­ï¼Œå¯æ›¿æ¢ä¸ºæ›´å¼ºå¤§çš„æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰å¹¶ç»“åˆè¯­è¨€æç¤ºï¼ˆPromptï¼‰å®ç°æ›´å¤æ‚çš„è‡ªç„¶è¯­è¨€ä»»åŠ¡ã€‚

è¿è¡Œç»“æœå±•ç¤º

```
(.venv) [rocky@jensen AI_literacy]$ python first_qa.py 
Device set to use cpu
{'score': 0.31832563877105713, 'start': 0, 'end': 5, 'answer': 'è‰¾ä¼¦Â·å›¾çµ'}
è‰¾ä¼¦Â·å›¾çµ
```



> ç”¨ä¸€ä¸ªæœ¬åœ°çš„ä¸­æ–‡é—®ç­”æ¨¡å‹ï¼Œåœ¨ä¸€æ®µæ–‡æœ¬é‡Œæå–é—®é¢˜çš„ç­”æ¡ˆã€‚
>
> âœ… 1. å¼•å…¥ Hugging Face çš„ `pipeline`
>
> ```python
>from transformers import pipeline
> ```
> 
> è¿™æ˜¯ Hugging Face `transformers` æä¾›çš„ç®€æ´ APIï¼Œç”¨äºå¿«é€Ÿæ„å»º NLP ä»»åŠ¡ç®¡é“ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ã€ç¿»è¯‘ç­‰ã€‚
>
> 
>
> âœ… 2. æ„é€ é—®ç­”ä»»åŠ¡çš„ pipelineï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰
>
> ```python
>qa = pipeline(
>  "question-answering",
>  #model="./distilbert-base-cased-distilled-squad",
>     model="./roberta-chinese-qa",      # ä½¿ç”¨æœ¬åœ°ä¸­æ–‡æ¨¡å‹ç›®å½•
>     framework="pt"                     # å¼ºåˆ¶ä½¿ç”¨ PyTorch
>    )
>    ```
> 
> å‚æ•°è¯´æ˜ï¼š
>
> | å‚æ•°                           | å«ä¹‰                                                         |
>| ------------------------------ | ------------------------------------------------------------ |
> | `"question-answering"`         | æŒ‡å®šä»»åŠ¡ç±»å‹æ˜¯æŠ½å–å¼é—®ç­”ï¼ˆextractive QAï¼‰                    |
> | `model="./roberta-chinese-qa"` | æŒ‡å‘æœ¬åœ°ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶å¤¹ï¼Œé‡Œé¢åŒ…å« `pytorch_model.bin`ã€`config.json` ç­‰ |
> | `framework="pt"`               | æ˜¾å¼è¦æ±‚ä½¿ç”¨ PyTorchï¼Œè€Œä¸æ˜¯ TensorFlowï¼Œé˜²æ­¢æ„å¤–åŠ è½½ TF æ¨¡å‹å¼•å‘é”™è¯¯ |
> 
> ğŸ“ `tokenizer` ä¼šè‡ªåŠ¨ä»æ¨¡å‹ç›®å½•ä¸­åŠ è½½ï¼Œæ— éœ€å•ç‹¬æŒ‡å®šã€‚
>
> ------
>
> âœ… 3. è¿è¡Œé—®ç­”æ¨ç†
>
> ```python
>result = qa(
>  question="è°æ˜¯äººå·¥æ™ºèƒ½ä¹‹çˆ¶ï¼Ÿ",
>  context="è‰¾ä¼¦Â·å›¾çµæ˜¯äººå·¥æ™ºèƒ½ä¹‹çˆ¶ï¼Œè¢«èª‰ä¸ºè®¡ç®—æœºç§‘å­¦çš„å¥ åŸºäººã€‚"
>    )
>    ```
> 
> è¯´æ˜ï¼š
>
> - è¾“å…¥çš„ `question` æ˜¯ç”¨æˆ·è¦é—®çš„é—®é¢˜ï¼›
>- `context` æ˜¯åŒ…å«ç­”æ¡ˆçš„ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ˆæ¨¡å‹ä¼šåœ¨é‡Œé¢æŸ¥æ‰¾ç­”æ¡ˆï¼‰ï¼›
> - è¿”å›çš„æ˜¯ä¸€ä¸ªåŒ…å« **é¢„æµ‹ç­”æ¡ˆä½ç½®ã€å†…å®¹ã€ç½®ä¿¡åº¦** çš„å­—å…¸ç»“æ„ã€‚
> 
> 
>
> ğŸ›  å¸¸è§è¡¥å……å»ºè®®ï¼š
>
> - **æ›´å¤æ‚ context**ï¼šä½ å¯ä»¥ç»™å®ƒæ›´å¤šæ®µè½ï¼Œå®ƒä¼šæ‰¾æœ€æœ‰å¯èƒ½çš„ç­”æ¡ˆï¼›
>
> - **ä¸­æ–‡ç²¾åº¦æé«˜**ï¼šä½ å¯ä»¥å°è¯• `hfl/chinese-roberta-wwm-ext-large` ä¹‹ç±»çš„æ¨¡å‹ï¼›
>
> 



# 4. æ·±åº¦å­¦ä¹ ä¸ç¥ç»ç½‘ç»œ

æ·±åº¦å­¦ä¹ æ˜¯è¿æ¥ä¸»ä¹‰æµæ´¾çš„é‡è¦ç»„æˆï¼Œä¸»è¦ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œè‡ªåŠ¨å­¦ä¹ ç‰¹å¾å’Œæ¨¡å¼ã€‚æœ¬èŠ‚é‡ç‚¹ä»‹ç»ç¥ç»ç½‘ç»œçš„å…³é”®ç®—æ³•ä¸å®æˆ˜ç¤ºä¾‹ã€‚

## 4.1 åå‘ä¼ æ’­ç®—æ³•

åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•ã€‚å…¶æ€æƒ³æ˜¯é€šè¿‡å‰å‘ä¼ æ’­è®¡ç®—è¾“å‡ºï¼Œç„¶ååå‘ä¼ æ’­è¯¯å·®å¹¶æ›´æ–°ç½‘ç»œæƒé‡ï¼Œä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°[4]ã€‚å‰å‘ä¼ æ’­é˜¶æ®µï¼Œä»è¾“å…¥å±‚ç»è¿‡åŠ æƒæ±‚å’Œã€æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ReLUã€Softmaxï¼‰é€å±‚è®¡ç®—è¾“å‡ºï¼›åå‘ä¼ æ’­é˜¶æ®µï¼Œåˆ©ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æŸå¤±å¯¹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ï¼Œç„¶åé‡‡ç”¨æ¢¯åº¦ä¸‹é™æˆ–è‡ªé€‚åº”ä¼˜åŒ–å™¨ï¼ˆå¦‚Adamï¼‰æ›´æ–°æƒé‡ã€‚åå‘ä¼ æ’­ä½¿å¾—å¤šå±‚æ·±åº¦ç½‘ç»œçš„è®­ç»ƒæˆä¸ºå¯èƒ½ï¼Œæ˜¯æ·±åº¦å­¦ä¹ å…´èµ·çš„åŸºçŸ³[3]ã€‚

**ç®—æ³•æµç¨‹æ¦‚è¿°**ï¼š 

1. **å‰å‘ä¼ æ’­**ï¼šå°†è¾“å…¥æ•°æ®é€å±‚ä¼ é€’ï¼Œè®¡ç®—æ¯å±‚ç¥ç»å…ƒè¾“å‡ºå¹¶æœ€ç»ˆå¾—åˆ°é¢„æµ‹ç»“æœã€‚ 
2. **è¯¯å·®è®¡ç®—**ï¼šä½¿ç”¨æŸå¤±å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µã€å‡æ–¹è¯¯å·®ï¼‰è®¡ç®—é¢„æµ‹è¾“å‡ºä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„è¯¯å·®ã€‚ 
3. **åå‘ä¼ æ’­**ï¼šä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚åå‘ä¼ æ’­è¯¯å·®ï¼Œé€šè¿‡é“¾å¼æ³•åˆ™è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚ 
4. **å‚æ•°æ›´æ–°**ï¼šæ ¹æ®æ¢¯åº¦å¯¹æƒé‡å’Œåç½®è¿›è¡Œæ›´æ–°ï¼ˆå¦‚$w \leftarrow w - \eta \frac{\partial L}{\partial w}$ï¼‰ï¼Œå¸¸ç”¨ä¼˜åŒ–ç®—æ³•æœ‰éšæœºæ¢¯åº¦ä¸‹é™ã€Adamç­‰ã€‚ 
5. **é‡å¤è¿­ä»£**ï¼šå¯¹æ‰€æœ‰è®­ç»ƒæ ·æœ¬å¤šæ¬¡è¿­ä»£ï¼ˆå¤šä¸ªepochï¼‰ï¼Œç›´åˆ°æŸå¤±æ”¶æ•›æˆ–è¾¾åˆ°è®­ç»ƒè½®æ¬¡ä¸Šé™ã€‚

åå‘ä¼ æ’­çš„å¼•å…¥æå¤§æé«˜äº†ç½‘ç»œè®­ç»ƒæ•ˆç‡ï¼Œä½¿å¾—å¤šå±‚æ·±åº¦ç½‘ç»œæˆä¸ºå¯è¡Œã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ·±å±‚ç½‘ç»œå¯èƒ½é¢ä¸´**æ¢¯åº¦æ¶ˆå¤±**æˆ–**æ¢¯åº¦çˆ†ç‚¸**ç­‰é—®é¢˜ï¼ˆå°¤å…¶ä½¿ç”¨Sigmoid/Tanhæ¿€æ´»å‡½æ•°æ—¶ï¼‰ï¼Œç°ä»£å®è·µå¸¸ç”¨ReLUåŠæ‰¹å½’ä¸€åŒ–ç­‰æ–¹æ³•ç¼“è§£ã€‚



### Q6.äº¤äº’å¯è§†åŒ–neural network

https://developers.google.com/machine-learning/crash-course/neural-networks/interactive-exercises?hl=zh-cn

**æ‚¨çš„ä»»åŠ¡**ï¼šé…ç½®ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œä½¿å…¶èƒ½å¤Ÿå°†ä¸‹å›¾ä¸­çš„æ©™ç‚¹ä¸è“ç‚¹åˆ†å¼€ï¼Œå¹¶åœ¨è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ä¸Šå®ç°ä½äº 0.2 çš„æŸå¤±ã€‚

**è¯´æ˜ï¼š**

åœ¨ä¸‹æ–¹çš„äº’åŠ¨å¼ widget ä¸­ï¼š

1. é€šè¿‡å°è¯•ä»¥ä¸‹éƒ¨åˆ†é…ç½®è®¾ç½®æ¥ä¿®æ”¹ç¥ç»ç½‘ç»œè¶…å‚æ•°ï¼š
   - ç‚¹å‡»ç½‘ç»œå›¾ä¸­çš„**éšè—å±‚**æ ‡é¢˜å·¦ä¾§çš„ **+** å’Œ **-** æŒ‰é’®ï¼Œæ·»åŠ æˆ–ç§»é™¤éšè—å±‚ã€‚
   - ç‚¹å‡»éšè—å±‚åˆ—ä¸Šæ–¹çš„ **+** å’Œ **-** æŒ‰é’®ï¼Œå³å¯åœ¨éšè—å±‚ä¸­æ·»åŠ æˆ–ç§»é™¤ç¥ç»å…ƒã€‚
   - å¦‚éœ€æ›´æ”¹å­¦ä¹ ç‡ï¼Œè¯·ä»å›¾è¡¨ä¸Šæ–¹çš„**å­¦ä¹ ç‡**ä¸‹æ‹‰èœå•ä¸­é€‰æ‹©ä¸€ä¸ªæ–°å€¼ã€‚
   - é€šè¿‡ä»å›¾è¡¨ä¸Šæ–¹çš„**æ¿€æ´»**ä¸‹æ‹‰èœå•ä¸­é€‰æ‹©æ–°å€¼æ¥æ›´æ”¹æ¿€æ´»å‡½æ•°ã€‚
2. ç‚¹å‡»å›¾è¡¨ä¸Šæ–¹çš„â€œæ’­æ”¾â€(â–¶ï¸) æŒ‰é’®ï¼Œä½¿ç”¨æŒ‡å®šçš„å‚æ•°è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹ã€‚
3. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè§‚å¯Ÿæ¨¡å‹æ‹Ÿåˆæ•°æ®çš„å¯è§†åŒ–æ•ˆæœï¼Œä»¥åŠ**è¾“å‡º**éƒ¨åˆ†ä¸­çš„**æµ‹è¯•æŸå¤±**å’Œ**è®­ç»ƒæŸå¤±**å€¼ã€‚
4. å¦‚æœæ¨¡å‹åœ¨æµ‹è¯•æ•°æ®å’Œè®­ç»ƒæ•°æ®ä¸Šçš„æŸå¤±æœªè¾¾åˆ° 0.2 ä»¥ä¸‹ï¼Œè¯·ç‚¹å‡»â€œé‡ç½®â€ï¼Œç„¶åä½¿ç”¨å¦ä¸€ç»„é…ç½®è®¾ç½®é‡å¤æ‰§è¡Œç¬¬ 1-3 æ­¥ã€‚é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°è·å¾—ç†æƒ³çš„ç»“æœã€‚

> **Your task:** configure a neural network that can separate the orange dots from the blue dots in the diagram, achieving a loss of less than 0.2 on both the training and test data.
>
> **Instructions:**
>
> In the interactive widget:
>
> 1. Modify the neural network hyperparameters by experimenting with some of the following config settings:
>    - Add or remove hidden layers by clicking the **+** and **-** buttons to the left of the **HIDDEN LAYERS** heading in the network diagram.
>    - Add or remove neurons from a hidden layer by clicking the **+** and **-** buttons above a hidden-layer column.
>    - Change the learning rate by choosing a new value from the **Learning rate** drop-down above the diagram.
>    - Change the activation function by choosing a new value from the **Activation** drop-down above the diagram.
> 2. Click the Play button above the diagram to train the neural network model using the specified parameters.
> 3. Observe the visualization of the model fitting the data as training progresses, as well as the **Test loss** and **Training loss** values in the **Output** section.
> 4. If the model does not achieve loss below 0.2 on the test and training data, click reset, and repeat steps 1â€“3 with a different set of configuration settings. Repeat this process until you achieve the preferred results.
>

ç»™å‡ºæ»¡è¶³çº¦æŸæ¡ä»¶çš„<mark>æˆªå›¾</mark>ï¼ŒåŒå­¦å¯ä»¥é¢†æ‚Ÿæ¦‚å¿µå’ŒåŸç†ã€‚

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/6e8ec7f85c470b44edc373985d94337c.png" alt="6e8ec7f85c470b44edc373985d94337c" style="zoom: 50%;" />





> é˜…è¯»ï¼šPyTorch æ•™ç¨‹ï¼Œhttps://www.runoob.com/pytorch/pytorch-tutorial.html
>
> æˆ‘ä½¿ç”¨PyTorchå®ç°5ä¸ªä»åŸºç¡€æ¨¡å‹åˆ°è¾ƒå¤æ‚æ¨¡å‹çš„è®­ç»ƒä¸åº”ç”¨ã€‚ç›¸å…³ä»£ç åŠè¯´æ˜æ–‡æ¡£å·²æ•´ç†äº Markdown æ–‡ä»¶ä¸­ï¼Œè¯¦è§é¡¹ç›®ä»“åº“ï¼šhttps://github.com/GMyhf/2025spring-cs201/tree/main/LLM
>
> 1. `0_xor_bp_neural_net_manual`ï¼šæ‰‹åŠ¨å®ç°åå‘ä¼ æ’­çš„ç®€å•ç¥ç»ç½‘ç»œï¼Œç”¨äºå¼‚æˆ–é—®é¢˜ã€‚
> 2. `1_iris_neural_network`ï¼šæ„å»ºå¹¶è®­ç»ƒç”¨äºé¸¢å°¾èŠ±åˆ†ç±»çš„æ•°æ®é©±åŠ¨ç¥ç»ç½‘ç»œã€‚
> 3. `2_mnist_resnet18`ï¼šä½¿ç”¨ ResNet18 æ¨¡å‹å¯¹ MNIST æ‰‹å†™æ•°å­—è¿›è¡Œåˆ†ç±»ã€‚
> 4. `3_cifar10_resnet18`ï¼šå°† ResNet18 åº”ç”¨äº CIFAR-10 å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚
> 5. `4_tiny_imagenet_resnet50`ï¼šåŸºäº ResNet50 æ¨¡å‹å¤„ç† Tiny ImageNet å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚

## 4.2 å®ä¾‹ï¼šå¼‚æˆ–é—®é¢˜ï¼ˆXORï¼‰

å¼‚æˆ–é—®é¢˜æ˜¯ç»å…¸çš„éçº¿æ€§å¯åˆ†é—®é¢˜ï¼Œç”¨æ¥æ¼”ç¤ºç¥ç»ç½‘ç»œçš„å­¦ä¹ èƒ½åŠ›ã€‚ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œå¯æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­æ¥è§£å†³å¼‚æˆ–ã€‚ä»¥ä¸‹å…ˆç»™å‡ºç®€æ´çš„ä¼ªä»£ç ï¼Œå†ç»™å‡ºå¯ä»¥è¿è¡Œçš„Pythonä»£ç ç¤ºä¾‹å±•ç¤ºäº†åå‘ä¼ æ’­æ›´æ–°æƒé‡çš„æ–¹å¼ï¼š

```python
# å‡è®¾ç½‘ç»œç»“æ„ï¼šè¾“å…¥å±‚2ä¸ªèŠ‚ç‚¹ï¼Œéšè—å±‚2ä¸ªèŠ‚ç‚¹ï¼Œè¾“å‡ºå±‚1ä¸ªèŠ‚ç‚¹
# åˆå§‹åŒ–æƒé‡
W1 = random([...])  # è¾“å…¥åˆ°éšè—å±‚
W2 = random([...])  # éšè—å±‚åˆ°è¾“å‡ºå±‚
learning_rate = 0.1

for epoch in range(epochs):
    # å‰å‘è®¡ç®—
    hidden = sigmoid(X @ W1)       # Xä¸ºè¾“å…¥[å››ç»„XORè¾“å…¥]
    output = sigmoid(hidden @ W2)  # é¢„æµ‹
    # è®¡ç®—è¯¯å·®
    error = (y - output)           # yä¸ºçœŸå®æ ‡ç­¾
    # åå‘ä¼ æ’­ï¼ˆé“¾å¼æ³•åˆ™ï¼‰
    dW2 = hidden.T @ (error * output * (1 - output))
    dW1 = X.T @ ((error * output * (1 - output)) @ W2.T * hidden * (1 - hidden))
    # æ›´æ–°æƒé‡
    W2 += learning_rate * dW2
    W1 += learning_rate * dW1

```



```python
# å¯¹äºXORé—®é¢˜ï¼ˆè¾“å…¥ä¸º[0,0], [0,1], [1,0], [1,1]ï¼‰ï¼ŒæœŸæœ›è¾“å‡ºä¸º[0,1,1,0]
# æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­ï¼Œæ²¡æœ‰ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¿™æœ‰åŠ©äºç†è§£åº•å±‚åŸç†
# https://www.geeksforgeeks.org/backpropagation-in-neural-network/
import numpy as np


class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size  # è¾“å…¥ç‰¹å¾ç»´åº¦
        self.hidden_size = hidden_size  # éšè—å±‚ç¥ç»å…ƒæ•°é‡
        self.output_size = output_size  # è¾“å‡ºå±‚ç¥ç»å…ƒæ•°é‡

        # è¾“å…¥å±‚åˆ°éšè—å±‚çš„æƒé‡ï¼Œå½¢çŠ¶ä¸º (è¾“å…¥ç»´åº¦, éšè—å±‚ç»´åº¦)
        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)
        # éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡ï¼Œå½¢çŠ¶ä¸º (éšè—å±‚ç»´åº¦, è¾“å‡ºå±‚ç»´åº¦)
        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)

        # éšè—å±‚çš„åç½®ï¼Œå½¢çŠ¶ä¸º (1, éšè—å±‚ç»´åº¦)
        self.bias_hidden = np.zeros((1, self.hidden_size))
        # è¾“å‡ºå±‚çš„åç½®ï¼Œå½¢çŠ¶ä¸º (1, è¾“å‡ºå±‚ç»´åº¦)
        self.bias_output = np.zeros((1, self.output_size))

    def sigmoid(self, x):  # æ¿€æ´»å‡½æ•°ï¼Œå°†è¾“å…¥å‹ç¼©åˆ°(0,1)åŒºé—´
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)  # Sigmoidçš„å¯¼æ•°ï¼Œç”¨äºåå‘ä¼ æ’­ä¸­çš„æ¢¯åº¦è®¡ç®—

    def feedforward(self, X):
        # éšè—å±‚è®¡ç®—
        self.hidden_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden  # çº¿æ€§å˜æ¢
        self.hidden_output = self.sigmoid(self.hidden_activation)  # æ¿€æ´»å‡½æ•°

        # è¾“å‡ºå±‚è®¡ç®—
        self.output_activation = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output
        self.predicted_output = self.sigmoid(self.output_activation)

        return self.predicted_output

    def backward(self, X, y, learning_rate):
        # è®¡ç®—è¾“å‡ºå±‚è¯¯å·®
        output_error = y - self.predicted_output  # è¯¯å·® = çœŸå®å€¼ - é¢„æµ‹å€¼
        # è®¡ç®—è¾“å‡ºå±‚çš„deltaï¼ˆæ¢¯åº¦çš„ä¸€éƒ¨åˆ†ï¼ŒæŸå¤±å¯¹æ¿€æ´»è¾“å…¥çš„æ¢¯åº¦ï¼‰
        output_delta = output_error * self.sigmoid_derivative(self.predicted_output)  # Delta = è¯¯å·® Ã— æ¿€æ´»å‡½æ•°å¯¼æ•°
        # output_delta = (y - Å·) * Ïƒ'(z_output)

        # è®¡ç®—éšè—å±‚è¯¯å·®ï¼ˆåå‘ä¼ æ’­ï¼‰
        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)  # å°†è¯¯å·®ä»è¾“å‡ºå±‚åå‘ä¼ æ’­åˆ°éšè—å±‚
        # hidden_error = output_delta @ W_hidden_output^T
        # è®¡ç®—éšè—å±‚çš„deltaï¼ˆæŸå¤±å¯¹éšè—å±‚æ¿€æ´»è¾“å…¥çš„æ¢¯åº¦ï¼‰
        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)  # Delta = è¯¯å·® Ã— æ¿€æ´»å‡½æ•°å¯¼æ•°
        # hidden_delta = (hidden_error) * Ïƒ'(z_hidden)

        # æ›´æ–°æƒé‡å’Œåç½®ï¼ˆä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ï¼‰
        # è®¡ç®—å¹¶æ›´æ–°éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡
        self.weights_hidden_output += np.dot(self.hidden_output.T,
                                             output_delta) * learning_rate  # æƒé‡æ›´æ–°é‡ = å­¦ä¹ ç‡ Ã— (éšè—å±‚è¾“å‡ºè½¬ç½® Ã— è¾“å‡ºå±‚delta)
        # W_hidden_output += learning_rate * (hidden_output^T @ output_delta)

        # æ›´æ–°è¾“å‡ºå±‚åç½®ï¼ŒåŸºäºæ‰€æœ‰æ ·æœ¬çš„è¾“å‡ºå±‚deltaæ²¿åˆ—æ±‚å’Œ
        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate  # åç½®æ›´æ–°é‡ = å­¦ä¹ ç‡ Ã— (æ²¿åˆ—æ±‚å’Œè¾“å‡ºå±‚delta)
        # b_output += learning_rate * sum(output_delta)

        # è®¡ç®—å¹¶æ›´æ–°ä»è¾“å…¥å±‚åˆ°éšè—å±‚çš„æƒé‡çš„æ¢¯åº¦
        self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate  # æƒé‡æ›´æ–°é‡ = å­¦ä¹ ç‡ Ã— (è¾“å…¥æ•°æ®è½¬ç½® Ã— éšè—å±‚delta)
        # W_input_hidden += learning_rate * (X^T @ hidden_delta)

        # æ›´æ–°éšè—å±‚åç½®ï¼ŒåŸºäºæ‰€æœ‰æ ·æœ¬çš„éšè—å±‚deltaæ²¿åˆ—æ±‚å’Œ
        # axis=0ï¼šæ²¿åˆ—æ±‚å’Œï¼Œèšåˆæ‰€æœ‰æ ·æœ¬çš„æ¢¯åº¦
        # keepdims=Trueï¼šä¿æŒåŸçŸ©é˜µçš„è¡Œæ•°ç»´åº¦ï¼Œç¡®ä¿åç½®æ›´æ–°çš„å½¢çŠ¶å…¼å®¹æ€§
        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate  # åç½®æ›´æ–°é‡ = å­¦ä¹ ç‡ Ã— (æ²¿åˆ—æ±‚å’Œéšè—å±‚delta)
        # b_hidden += learning_rate * sum(hidden_delta)

    def train(self, X, y, epochs, learning_rate):
        for epoch in range(epochs):
            output = self.feedforward(X)  # å‰å‘ä¼ æ’­
            self.backward(X, y, learning_rate)  # åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°
            if epoch % 4000 == 0:
                loss = np.mean(np.square(y - output))  # è®¡ç®—å‡æ–¹è¯¯å·®
                print(f"Epoch {epoch}, Loss:{loss}")


X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# è¾“å…¥ç»´åº¦ 2ï¼ˆäºŒç»´äºŒè¿›åˆ¶ç‰¹å¾ï¼‰ï¼Œéšè—å±‚4ä¸ªç¥ç»å…ƒï¼Œè¾“å‡ºå±‚1ä¸ªç¥ç»å…ƒï¼ˆäºŒåˆ†ç±»é—®é¢˜ï¼‰
nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)
# è®­ç»ƒæ€»è½®æ¬¡, å­¦ä¹ ç‡
nn.train(X, y, epochs=10000, learning_rate=0.1)

output = nn.feedforward(X)
print("Predictions after training:")
print(output)
"""
Epoch 0, Loss:0.2653166263520884
Epoch 4000, Loss:0.007000926683956338
Epoch 8000, Loss:0.001973630232951721
Predictions after training:
[[0.03613239]
 [0.96431351]
 [0.96058291]
 [0.03919372]]
"""
```

æœ€ç»ˆè®­ç»ƒåï¼Œè¯¥ç½‘ç»œå¯ä»¥å‡†ç¡®å­¦ä¹ XORé€»è¾‘ï¼ˆè®­ç»ƒæ•°æ®ï¼š${([0,0]\to0),([0,1]\to1),([1,0]\to1),([1,1]\to0)}$ï¼‰ï¼Œè¾“å‡ºæ¥è¿‘é¢„æœŸã€‚è¯¥ç¤ºä¾‹éªŒè¯äº†å¤šå±‚ç½‘ç»œå’Œåå‘ä¼ æ’­èƒ½è§£å†³çº¿æ€§æ¨¡å‹æ— æ³•å¤„ç†çš„é—®é¢˜ã€‚



## 4.3 å®ä¾‹ï¼šIrisæ•°æ®é›†åˆ†ç±»

**ä»»åŠ¡æè¿°**ï¼šä½¿ç”¨å…¨è¿æ¥ç¥ç»ç½‘ç»œå¯¹ç»å…¸çš„Irisï¼ˆé¸¢å°¾èŠ±ï¼‰æ•°æ®é›†è¿›è¡Œå¤šåˆ†ç±»ã€‚æ•°æ®é›†åŒ…å«150ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬4ä¸ªç‰¹å¾ï¼ˆèŠ±è¼å’ŒèŠ±ç“£é•¿åº¦/å®½åº¦ï¼‰ï¼Œåˆ†ä¸º3ä¸ªç±»åˆ«ã€‚

**å…³é”®æ­¥éª¤**ï¼šæ•°æ®é¢„å¤„ç†ï¼ˆæ ‡å‡†åŒ–ã€è®­ç»ƒ/æµ‹è¯•é›†åˆ’åˆ†ï¼‰ã€æ¨¡å‹æ„å»ºã€è®­ç»ƒä¸è¯„ä¼°ã€‚ç¤ºä¾‹ä»£ç ï¼ˆPyTorchï¼‰ï¼š

å®‰è£…éœ€è¦çš„åŒ… pip install torchvision matplotlib

> $ python iris_nn.py 
>
> Traceback (most recent call last):
>  File "/home/rocky/AI_literacy/iris_nn.py", line 3, in <module>
>   from sklearn.datasets import load_iris
> ModuleNotFoundError: No module named 'sklearn'

```python
import torch, torch.nn as nn, torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


# å®šä¹‰æ¨¡å‹ç»“æ„
class IrisNet(nn.Module):
    def __init__(self, input_size=4, hidden_size=10, num_classes=3):
        super(IrisNet, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, num_classes)
        )

    def forward(self, x):
        return self.net(x)


# è®­ç»ƒå‡½æ•°
def train(model, dataloader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    for batch_X, batch_y in dataloader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)

        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * batch_X.size(0)

    return running_loss / len(dataloader.dataset)


# æµ‹è¯•å‡½æ•°
def evaluate(model, X, y, device):
    model.eval()
    with torch.no_grad():
        X, y = X.to(device), y.to(device)
        outputs = model(X)
        _, predicted = torch.max(outputs, 1)
        accuracy = (predicted == y).float().mean().item()
    return accuracy, predicted


# ä¸»ç¨‹åº
def main():
    # è®¾ç½®è®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device("mps")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")

    # åŠ è½½æ•°æ®
    iris = load_iris()
    X, y = iris.data, iris.target

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    """
    random_state=42
    è®¾å®šéšæœºæ•°ç§å­ï¼Œä»è€Œç¡®ä¿æ¯æ¬¡è¿è¡Œä»£ç æ—¶æ•°æ®åˆ’åˆ†çš„ç»“æœéƒ½æ˜¯ç›¸åŒçš„ã€‚è¿™æ ·åšå¯ä»¥ä½¿å®éªŒå…·æœ‰å¯é‡å¤æ€§ï¼Œ
    æœ‰åˆ©äºè°ƒè¯•å’Œç»“æœå¯¹æ¯”ã€‚

    stratify=y
    è¿™ä¸ªå‚æ•°è¡¨ç¤ºæŒ‰ç…§ y ä¸­çš„æ ‡ç­¾è¿›è¡Œåˆ†å±‚æŠ½æ ·ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­å„ç±»åˆ«çš„
    æ¯”ä¾‹ä¼šä¸åŸå§‹æ•°æ®ä¸­çš„ç±»åˆ«æ¯”ä¾‹ä¿æŒä¸€è‡´ã€‚è¿™å¯¹äºç±»åˆ«ä¸å¹³è¡¡çš„æ•°æ®é›†å°¤ä¸ºé‡è¦ï¼Œå¯ä»¥
    é¿å…æŸä¸€ç±»åˆ«åœ¨åˆ’åˆ†æ—¶è¢«ä¸¥é‡ä½ä¼°æˆ–è¿‡é‡‡æ ·ã€‚
    """

    # æ ‡å‡†åŒ–ï¼šåªåœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼Œå†å°†ç›¸åŒçš„å˜æ¢åº”ç”¨åˆ°æµ‹è¯•é›†ä¸Š
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # è½¬æ¢ä¸º Tensor
    X_train = torch.tensor(X_train, dtype=torch.float32)
    X_test = torch.tensor(X_test, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.long)
    y_test = torch.tensor(y_test, dtype=torch.long)

    # æ„é€  DataLoader
    train_dataset = TensorDataset(X_train, y_train)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

    # æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
    model = IrisNet().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    # è®­ç»ƒ
    num_epochs = 100
    for epoch in range(1, num_epochs + 1):
        loss = train(model, train_loader, criterion, optimizer, device)
        if epoch % 10 == 0:
            print(f"Epoch [{epoch:3d}/{num_epochs}], Loss: {loss:.4f}")

    # è¯„ä¼°
    test_acc, test_pred = evaluate(model, X_test, y_test, device)
    print(f"\nâœ… Test Accuracy: {test_acc * 100:.2f}%")

    # ç¤ºä¾‹é¢„æµ‹
    sample = X_test[0].unsqueeze(0)
    sample_pred = model(sample.to(device))
    pred_class = torch.argmax(sample_pred, dim=1).item()
    print(f"ğŸ” Sample Prediction: True = {y_test[0].item()}, Predicted = {pred_class}")


if __name__ == "__main__":
    main()

```

> å¦‚æœæ— æ³•ä½¿ç”¨GPU
>
> **åœ¨è¿è¡Œæ—¶å¼ºåˆ¶ä½¿ç”¨CPUè°ƒè¯•**
>
> ```
> CUDA_VISIBLE_DEVICES="" python iris_neural_network.py
> ```
>
> è¿™æ ·ç¦ç”¨CUDAï¼Œä½¿ç”¨CPUã€‚



> äº‘è™šæ‹Ÿæœºè¿è¡Œç»“æœï¼š
>
> ![image-20250915152854981](https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250915152854981.png)



**ä»£ç è¯´æ˜ï¼š**

1. **æ•°æ®å‡†å¤‡**ï¼š
   - ä½¿ç”¨scikit-learnåŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†
   - å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆ80%ï¼‰å’Œæµ‹è¯•é›†ï¼ˆ20%ï¼‰
   - ä½¿ç”¨æ ‡å‡†åŒ–å¤„ç†ï¼ˆStandardScalerï¼‰å¯¹ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–

2. **ç¥ç»ç½‘ç»œç»“æ„**ï¼š
   - è¾“å…¥å±‚ï¼š4ä¸ªç¥ç»å…ƒï¼ˆå¯¹åº”4ä¸ªç‰¹å¾ï¼‰
   - éšè—å±‚ï¼š10ä¸ªç¥ç»å…ƒï¼ˆä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°ï¼‰
   - è¾“å‡ºå±‚ï¼š3ä¸ªç¥ç»å…ƒï¼ˆå¯¹åº”3ä¸ªç±»åˆ«ï¼‰

3. **è®­ç»ƒé…ç½®**ï¼š
   - ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆCrossEntropyLossï¼‰
   - ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼ˆå­¦ä¹ ç‡0.01ï¼‰
   - è®­ç»ƒ100ä¸ªepoch

4. **è®­ç»ƒè¿‡ç¨‹**ï¼š
   - æ¯ä¸ªepochè®°å½•æŸå¤±å’Œå‡†ç¡®ç‡
   - æ¯10ä¸ªepochæ‰“å°è®­ç»ƒè¿›åº¦

5. **è¯„ä¼°ä¸é¢„æµ‹**ï¼š
   - æœ€ç»ˆåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹å‡†ç¡®ç‡
   - åŒ…å«ä¸€ä¸ªé¢„æµ‹ç¤ºä¾‹å±•ç¤º

**è¾“å‡ºç¤ºä¾‹ï¼š**

```
$ python iris_neural_network.py 
Epoch [ 10/100], Loss: 0.2363
Epoch [ 20/100], Loss: 0.0899
Epoch [ 30/100], Loss: 0.0614
Epoch [ 40/100], Loss: 0.0634
Epoch [ 50/100], Loss: 0.0498
Epoch [ 60/100], Loss: 0.0492
Epoch [ 70/100], Loss: 0.0492
Epoch [ 80/100], Loss: 0.0451
Epoch [ 90/100], Loss: 0.0479
Epoch [100/100], Loss: 0.0436

âœ… Test Accuracy: 100.00%
ğŸ” Sample Prediction: True = 0, Predicted = 0
```

è¯¥ç½‘ç»œç»è¿‡è®­ç»ƒåï¼Œé€šå¸¸èƒ½åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°90%ä»¥ä¸Šçš„å‡†ç¡®ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¤šå±‚å…¨è¿æ¥ç½‘ç»œå³å¯è¾ƒå¥½è§£å†³è¯¥å¤šåˆ†ç±»ä»»åŠ¡ï¼ˆIrisæ•°æ®é›†è§„æ¨¡å°ï¼Œç½‘ç»œä¸éœ€è¿‡æ·±ï¼‰ã€‚



**å¯è§†åŒ–ï¼šç›‘ç£å­¦ä¹  + æ— ç›‘ç£å­¦ä¹ ï¼ˆIris æ•°æ®é›†ï¼‰**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# 1. åŠ è½½æ•°æ®
iris = load_iris()
X = iris.data
y = iris.target

# 2. æ ‡å‡†åŒ–æ•°æ®
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆç”¨äºç›‘ç£å­¦ä¹ ï¼‰
train_x, test_x, train_y, test_y = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# 4. ç›‘ç£å­¦ä¹ ï¼šé€»è¾‘å›å½’åˆ†ç±»
clf = LogisticRegression(max_iter=200)
clf.fit(train_x, train_y)
pred = clf.predict(test_x)
print("Logistic Regression Accuracy:", accuracy_score(test_y, pred))

# 5. æ— ç›‘ç£å­¦ä¹ ï¼šKMeans èšç±»ï¼ˆèšæˆ3ç±»ï¼‰
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# 6. å¯è§†åŒ–èšç±»ï¼ˆé™ç»´åˆ°äºŒç»´ï¼‰
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_2d = pca.fit_transform(X_scaled)

plt.figure(figsize=(10, 5))

# èšç±»ç»“æœ
plt.subplot(1, 2, 1)
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=clusters, cmap='viridis', s=50)
plt.title("KMeans Clustering (unsupervised)")

# åŸå§‹æ ‡ç­¾
plt.subplot(1, 2, 2)
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='Set1', s=50)
plt.title("Ground Truth Labels (supervised)")

plt.show()

```

> ä½¿ç”¨ `LogisticRegression` è®­ç»ƒä¸€ä¸ªæœ‰ç›‘ç£åˆ†ç±»å™¨ï¼Œå¹¶è¾“å‡ºæµ‹è¯•é›†å‡†ç¡®ç‡ï¼›
>
> ä½¿ç”¨ `KMeans` è¿›è¡Œæ— ç›‘ç£èšç±»ï¼›
>
> ä½¿ç”¨ PCA å°† 4 ç»´æ•°æ®é™ç»´ä¸º 2 ç»´ï¼Œä»¥ä¾¿å¯è§†åŒ–èšç±»ç»“æœå’ŒçœŸå®æ ‡ç­¾ï¼›
>

å¦‚å›¾æ‰€ç¤ºï¼Œé€šè¿‡ä½¿ç”¨PCAå°†ç‰¹å¾é™è‡³äºŒç»´ï¼Œå¯è§†åŒ–èšç±»æ•ˆæœä¸çœŸå®åˆ†ç±»çš„å¯¹æ¯”ï¼š



<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250727143806445.png" alt="image-20250727143806445" style="zoom: 67%;" />

<center>å›¾ï¼šIris æ•°æ®èšç±»ï¼ˆå·¦ï¼šç½‘ç»œèšç±»ç»“æœï¼›å³ï¼šçœŸå®ç±»åˆ«ï¼‰</center>



## 4.4 å®ä¾‹ï¼šMNISTæ‰‹å†™æ•°å­—è¯†åˆ«

MNISTæ˜¯æ‰‹å†™æ•°å­—åˆ†ç±»çš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«60000å¼ 28Ã—28çš„è®­ç»ƒæ‰‹å†™æ•°å­—å›¾ç‰‡ï¼ˆ0â€“9å…±10ç±»ï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨ç»å…¸çš„CNNï¼ˆå¦‚ResNet18ï¼‰æ¥è¿›è¡Œåˆ†ç±»è®­ç»ƒã€‚

å…³é”®ç‚¹ï¼šåŠ è½½MNISTæ•°æ®é›†ï¼Œå®šä¹‰å·ç§¯ç¥ç»ç½‘ç»œï¼ˆä¾‹å¦‚é¢„è®­ç»ƒResNet18æˆ–è‡ªå®šä¹‰å°å‹CNNï¼‰ï¼Œè®­ç»ƒå¤šä¸ªepochåè¯„ä¼°ã€‚ä¸‹é¢æ˜¯ç¤ºä¾‹ä»£ç ï¼š

å®‰è£…éœ€è¦çš„åŒ… 

> $ python MNIST_nn.py 
>
> Traceback (most recent call last):
>  File "/home/rocky/AI_literacy/MNIST_nn.py", line 2, in <module>
>   import torchvision
> ModuleNotFoundError: No module named 'torchvision'

clabè™šæ‹Ÿæœºéœ€è¦ç™»å½•ç½‘å…³ï¼Œèƒ½è®¿é—®å¤–ç½‘ï¼Œå› ä¸ºè¦ä¸‹è½½æ•°æ®

> å¦åˆ™ï¼ŒæŠ¥302é”™è¯¯
>
> (.venv) [rocky@jensen AI_literacy]$ python MNIST_nn.py 
> Traceback (most recent call last):
>  File "/home/rocky/AI_literacy/MNIST_nn.py", line 166, in <module>
>   main()
>  File "/home/rocky/AI_literacy/MNIST_nn.py", line 25, in main
>   trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)
>  File "/home/rocky/AI_literacy/.venv/lib64/python3.9/site-packages/torchvision/datasets/mnist.py", line 100, in __init__
>
>   self.download()
>
>  File "/home/rocky/AI_literacy/.venv/lib64/python3.9/site-packages/torchvision/datasets/mnist.py", line 197, in download
>   raise RuntimeError(s)
> RuntimeError: Error downloading train-images-idx3-ubyte.gz:
> Tried https://ossci-datasets.s3.amazonaws.com/mnist/, got:
> <urlopen error [Errno 110] Connection timed out>
> Tried http://yann.lecun.com/exdb/mnist/, got:
> HTTP Error 302: Moved Temporarily



```python
import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import matplotlib.pyplot as plt
import numpy as np
import time

def main():
    # 1. æ•°æ®å¢å¼º + é¢„å¤„ç†
    transform_train = transforms.Compose([
        transforms.RandomRotation(10),  # éšæœºæ—‹è½¬
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))  # MNIST æ˜¯å•é€šé“ï¼Œä½¿ç”¨ (0.5,) æ¥è§„èŒƒåŒ–
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])

    # åŠ è½½ MNIST æ•°æ®é›†
    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)

    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)
    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2, pin_memory=True)

    classes = [str(i) for i in range(10)]  # MNIST ç±»åˆ«æ˜¯ 0 åˆ° 9

    # 2. è®¾ç½®è®¾å¤‡å’Œæ¨¡å‹
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print("Using device:", device)

    # åŠ è½½é¢„å®šä¹‰çš„ ResNet18 å¹¶ä¿®æ”¹è¾“å…¥å±‚å’Œè¾“å‡ºå±‚
    net = models.resnet18(weights=None)
    # ä¿®æ”¹è¾“å…¥å±‚çš„ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼Œä½¿å…¶æ¥å—å•é€šé“ï¼ˆ1é€šé“ç°åº¦å›¾åƒï¼‰
    net.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    net.fc = nn.Linear(net.fc.in_features, 10)  # MNIST 10 ç±»
    net.to(device)

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

    # 3. è®­ç»ƒè¿‡ç¨‹
    best_loss = float('inf')
    patience = 10  # æé«˜è€å¿ƒ
    patience_counter = 0

    start_time = time.time()
    print("Starting training with early stopping...")
    for epoch in range(800):  # å¯é€‚å½“å¢å¤§ epoch
        net.train()
        epoch_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            if i % 100 == 99:
                print(f"[{epoch + 1}, {i + 1:5d}] loss: {loss.item():.3f}")

        avg_loss = epoch_loss / len(trainloader)
        print(f"[{epoch+1}] Avg Loss: {avg_loss:.3f}")

        if avg_loss < best_loss - 1e-4:
            best_loss = avg_loss
            patience_counter = 0
        else:
            patience_counter += 1
            print(f"No improvement. Patience: {patience_counter}/{patience}")
            if patience_counter >= patience:
                print("Early stopping triggered.")
                break

    end_time = time.time()
    execution_time_minutes = (end_time - start_time) / 60
    print(f"âœ… Training completed in {execution_time_minutes:.2f} minutes.")

    # ä¿å­˜æ¨¡å‹
    torch.save(net.state_dict(), './resnet18_mnist.pth')

    # 4. æµ‹è¯•å‡†ç¡®ç‡
    correct = 0
    total = 0
    net.eval()
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f"Accuracy on test images: {100 * correct / total:.2f}%")

    # æ¯ç±»å‡†ç¡®ç‡
    class_correct = list(0. for _ in range(10))
    class_total = list(0. for _ in range(10))
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            c = (predicted == labels).squeeze()
            for i in range(len(labels)):
                class_correct[labels[i]] += c[i].item()
                class_total[labels[i]] += 1

    for i in range(10):
        print(f'Accuracy of {classes[i]:5s}: {100 * class_correct[i] / class_total[i]:.2f}%')

    # --- å¯è§†åŒ–é¢„æµ‹ ---

    def imshow_grid(images, labels, preds=None, classes=None, rows=8, cols=8):
        images = images.cpu() / 2 + 0.5  # unnormalize
        npimg = images.numpy()
        fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.5, rows * 1.5))
        for i in range(rows * cols):
            r, c = divmod(i, cols)
            ax = axes[r, c]
            img = np.transpose(npimg[i], (1, 2, 0))
            ax.imshow(img.squeeze(), cmap="gray")
            title = f'{classes[labels[i]]}'
            if preds is not None:
                title += f'\nâ†’ {classes[preds[i]]}'
            ax.set_title(title, fontsize=8)
            ax.axis('off')
        plt.tight_layout()
        plt.show()

    # è·å–ä¸€æ‰¹å›¾åƒç”¨äºæ˜¾ç¤º
    dataiter = iter(testloader)
    images, labels = next(dataiter)
    while images.size(0) < 64:
        more_images, more_labels = next(dataiter)
        images = torch.cat([images, more_images], dim=0)
        labels = torch.cat([labels, more_labels], dim=0)
    images = images[:64]
    labels = labels[:64]

    # é¢„æµ‹
    net.eval()
    with torch.no_grad():
        outputs = net(images.to(device))
        _, predicted = torch.max(outputs, 1)

    # æ˜¾ç¤ºå›¾åƒç½‘æ ¼
    imshow_grid(images, labels, predicted.cpu(), classes=classes, rows=8, cols=8)

if __name__ == "__main__":
    import torch.multiprocessing
    torch.multiprocessing.set_start_method('spawn', force=True)
    main()


```

å…¸å‹ç»“æœï¼šä½¿ç”¨ResNet18èƒ½åœ¨MNISTä¸Šè¾¾åˆ°99%ä»¥ä¸Šçš„å‡†ç¡®ç‡ã€‚è¯¥ä»»åŠ¡å±•ç¤ºäº†æ·±åº¦å·ç§¯ç½‘ç»œåœ¨å›¾åƒåˆ†ç±»ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚

> å¦‚æœæ— æ³•ä½¿ç”¨GPU
> 
> **åœ¨è¿è¡Œæ—¶å¼ºåˆ¶ä½¿ç”¨CPUè°ƒè¯•**
> 
> ```
>  CUDA_VISIBLE_DEVICES="" python mnist_resnet18.py
> ```
> 
> è¿™æ ·ç¦ç”¨CUDAï¼Œä½¿ç”¨CPUã€‚
>
> 



### 4.4.1 åœ¨16GBå†…å­˜çš„ Mac mini è¿è¡Œ

> è¿è¡Œæœºå™¨
>
> <img src="https://raw.githubusercontent.com/GMyhf/img/main/img/202507261935350.png" alt="b452b39cfb47eb8bf5b640c828b6b71b" style="zoom:50%;" />
>
> 
>
> è¯¦ç»†è®­ç»ƒæ—¥å¿—
>
> ```
> /Users/hfyan/miniconda3/bin/python /Users/hfyan/git/2025spring-cs201/LLM/mnist_resnet18.py 
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [02:52<00:00, 57.6kB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 97.2kB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:04<00:00, 374kB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 6.74kB/s]
> Using device: mps
> Starting training with early stopping...
> /Users/hfyan/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.
>   warnings.warn(warn_msg)
> [1,   100] loss: 0.136
> [1,   200] loss: 0.132
> [1,   300] loss: 0.035
> [1,   400] loss: 0.098
> [1] Avg Loss: 0.150
> [2,   100] loss: 0.137
> [2,   200] loss: 0.030
> [2,   300] loss: 0.030
> [2,   400] loss: 0.015
> [2] Avg Loss: 0.052
> [3,   100] loss: 0.018
> [3,   200] loss: 0.105
> [3,   300] loss: 0.078
> [3,   400] loss: 0.026
> [3] Avg Loss: 0.039
> [4,   100] loss: 0.032
> [4,   200] loss: 0.056
> [4,   300] loss: 0.008
> [4,   400] loss: 0.013
> [4] Avg Loss: 0.031
> [5,   100] loss: 0.003
> [5,   200] loss: 0.025
> [5,   300] loss: 0.029
> [5,   400] loss: 0.022
> [5] Avg Loss: 0.027
> [6,   100] loss: 0.041
> [6,   200] loss: 0.022
> [6,   300] loss: 0.047
> [6,   400] loss: 0.005
> [6] Avg Loss: 0.023
> [7,   100] loss: 0.039
> [7,   200] loss: 0.000
> [7,   300] loss: 0.022
> [7,   400] loss: 0.014
> [7] Avg Loss: 0.018
> [8,   100] loss: 0.001
> [8,   200] loss: 0.044
> [8,   300] loss: 0.021
> [8,   400] loss: 0.002
> [8] Avg Loss: 0.019
> No improvement. Patience: 1/10
> [9,   100] loss: 0.002
> [9,   200] loss: 0.020
> [9,   300] loss: 0.002
> [9,   400] loss: 0.007
> [9] Avg Loss: 0.017
> [10,   100] loss: 0.027
> [10,   200] loss: 0.034
> [10,   300] loss: 0.031
> [10,   400] loss: 0.004
> [10] Avg Loss: 0.016
> [11,   100] loss: 0.003
> [11,   200] loss: 0.004
> [11,   300] loss: 0.005
> [11,   400] loss: 0.003
> [11] Avg Loss: 0.015
> [12,   100] loss: 0.011
> [12,   200] loss: 0.000
> [12,   300] loss: 0.031
> [12,   400] loss: 0.003
> [12] Avg Loss: 0.015
> No improvement. Patience: 1/10
> [13,   100] loss: 0.002
> [13,   200] loss: 0.002
> [13,   300] loss: 0.002
> [13,   400] loss: 0.019
> [13] Avg Loss: 0.013
> [14,   100] loss: 0.019
> [14,   200] loss: 0.004
> [14,   300] loss: 0.025
> [14,   400] loss: 0.003
> [14] Avg Loss: 0.013
> No improvement. Patience: 1/10
> [15,   100] loss: 0.003
> [15,   200] loss: 0.001
> [15,   300] loss: 0.011
> [15,   400] loss: 0.056
> [15] Avg Loss: 0.013
> [16,   100] loss: 0.034
> [16,   200] loss: 0.008
> [16,   300] loss: 0.001
> [16,   400] loss: 0.003
> [16] Avg Loss: 0.011
> [17,   100] loss: 0.008
> [17,   200] loss: 0.001
> [17,   300] loss: 0.001
> [17,   400] loss: 0.001
> [17] Avg Loss: 0.011
> [18,   100] loss: 0.009
> [18,   200] loss: 0.015
> [18,   300] loss: 0.002
> [18,   400] loss: 0.036
> [18] Avg Loss: 0.013
> No improvement. Patience: 1/10
> [19,   100] loss: 0.019
> [19,   200] loss: 0.001
> [19,   300] loss: 0.023
> [19,   400] loss: 0.005
> [19] Avg Loss: 0.011
> [20,   100] loss: 0.002
> [20,   200] loss: 0.007
> [20,   300] loss: 0.007
> [20,   400] loss: 0.005
> [20] Avg Loss: 0.011
> No improvement. Patience: 1/10
> [21,   100] loss: 0.001
> [21,   200] loss: 0.008
> [21,   300] loss: 0.012
> [21,   400] loss: 0.005
> [21] Avg Loss: 0.011
> [22,   100] loss: 0.007
> [22,   200] loss: 0.001
> [22,   300] loss: 0.001
> [22,   400] loss: 0.002
> [22] Avg Loss: 0.011
> No improvement. Patience: 1/10
> [23,   100] loss: 0.003
> [23,   200] loss: 0.002
> [23,   300] loss: 0.001
> [23,   400] loss: 0.014
> [23] Avg Loss: 0.011
> No improvement. Patience: 2/10
> [24,   100] loss: 0.003
> [24,   200] loss: 0.001
> [24,   300] loss: 0.003
> [24,   400] loss: 0.002
> [24] Avg Loss: 0.010
> [25,   100] loss: 0.016
> [25,   200] loss: 0.002
> [25,   300] loss: 0.010
> [25,   400] loss: 0.000
> [25] Avg Loss: 0.009
> [26,   100] loss: 0.001
> [26,   200] loss: 0.002
> [26,   300] loss: 0.006
> [26,   400] loss: 0.021
> [26] Avg Loss: 0.008
> [27,   100] loss: 0.002
> [27,   200] loss: 0.002
> [27,   300] loss: 0.017
> [27,   400] loss: 0.000
> [27] Avg Loss: 0.010
> No improvement. Patience: 1/10
> [28,   100] loss: 0.001
> [28,   200] loss: 0.012
> [28,   300] loss: 0.009
> [28,   400] loss: 0.000
> [28] Avg Loss: 0.008
> No improvement. Patience: 2/10
> [29,   100] loss: 0.001
> [29,   200] loss: 0.008
> [29,   300] loss: 0.009
> [29,   400] loss: 0.031
> [29] Avg Loss: 0.010
> No improvement. Patience: 3/10
> [30,   100] loss: 0.038
> [30,   200] loss: 0.001
> [30,   300] loss: 0.031
> [30,   400] loss: 0.001
> [30] Avg Loss: 0.011
> No improvement. Patience: 4/10
> [31,   100] loss: 0.017
> [31,   200] loss: 0.013
> [31,   300] loss: 0.029
> [31,   400] loss: 0.032
> [31] Avg Loss: 0.010
> No improvement. Patience: 5/10
> [32,   100] loss: 0.002
> [32,   200] loss: 0.000
> [32,   300] loss: 0.003
> [32,   400] loss: 0.001
> [32] Avg Loss: 0.009
> No improvement. Patience: 6/10
> [33,   100] loss: 0.009
> [33,   200] loss: 0.018
> [33,   300] loss: 0.001
> [33,   400] loss: 0.007
> [33] Avg Loss: 0.010
> No improvement. Patience: 7/10
> [34,   100] loss: 0.001
> [34,   200] loss: 0.001
> [34,   300] loss: 0.001
> [34,   400] loss: 0.011
> [34] Avg Loss: 0.010
> No improvement. Patience: 8/10
> [35,   100] loss: 0.004
> [35,   200] loss: 0.005
> [35,   300] loss: 0.009
> [35,   400] loss: 0.010
> [35] Avg Loss: 0.011
> No improvement. Patience: 9/10
> [36,   100] loss: 0.001
> [36,   200] loss: 0.004
> [36,   300] loss: 0.013
> [36,   400] loss: 0.007
> [36] Avg Loss: 0.008
> [37,   100] loss: 0.008
> [37,   200] loss: 0.003
> [37,   300] loss: 0.007
> [37,   400] loss: 0.002
> [37] Avg Loss: 0.010
> No improvement. Patience: 1/10
> [38,   100] loss: 0.002
> [38,   200] loss: 0.002
> [38,   300] loss: 0.011
> [38,   400] loss: 0.004
> [38] Avg Loss: 0.009
> No improvement. Patience: 2/10
> [39,   100] loss: 0.006
> [39,   200] loss: 0.003
> [39,   300] loss: 0.002
> [39,   400] loss: 0.001
> [39] Avg Loss: 0.008
> No improvement. Patience: 3/10
> [40,   100] loss: 0.000
> [40,   200] loss: 0.012
> [40,   300] loss: 0.011
> [40,   400] loss: 0.001
> [40] Avg Loss: 0.009
> No improvement. Patience: 4/10
> [41,   100] loss: 0.010
> [41,   200] loss: 0.008
> [41,   300] loss: 0.006
> [41,   400] loss: 0.002
> [41] Avg Loss: 0.008
> No improvement. Patience: 5/10
> [42,   100] loss: 0.005
> [42,   200] loss: 0.003
> [42,   300] loss: 0.014
> [42,   400] loss: 0.005
> [42] Avg Loss: 0.010
> No improvement. Patience: 6/10
> [43,   100] loss: 0.010
> [43,   200] loss: 0.000
> [43,   300] loss: 0.012
> [43,   400] loss: 0.002
> [43] Avg Loss: 0.008
> No improvement. Patience: 7/10
> [44,   100] loss: 0.001
> [44,   200] loss: 0.004
> [44,   300] loss: 0.035
> [44,   400] loss: 0.000
> [44] Avg Loss: 0.011
> No improvement. Patience: 8/10
> [45,   100] loss: 0.002
> [45,   200] loss: 0.014
> [45,   300] loss: 0.010
> [45,   400] loss: 0.014
> [45] Avg Loss: 0.010
> No improvement. Patience: 9/10
> [46,   100] loss: 0.001
> [46,   200] loss: 0.076
> [46,   300] loss: 0.001
> [46,   400] loss: 0.004
> [46] Avg Loss: 0.009
> No improvement. Patience: 10/10
> Early stopping triggered.
> âœ… Training completed in 27.72 minutes.
> Accuracy on test images: 99.57%
> Accuracy of 0    : 99.59%
> Accuracy of 1    : 99.91%
> Accuracy of 2    : 99.71%
> Accuracy of 3    : 99.80%
> Accuracy of 4    : 99.49%
> Accuracy of 5    : 99.33%
> Accuracy of 6    : 99.37%
> Accuracy of 7    : 99.22%
> Accuracy of 8    : 99.90%
> Accuracy of 9    : 99.31%
> 
> Process finished with exit code 0
> 
> ```
>
> 
>
> <img src="https://raw.githubusercontent.com/GMyhf/img/main/img/202507261936331.png" alt="22485e1e277b7dfea954fe0cd8a1af4f" style="zoom:50%;" />
>
> 



### 4.4.2 åœ¨16GBå†…å­˜çš„ window æœºå™¨è¿è¡Œ

> åœ¨windowæœºå™¨ï¼Œç”¨ WSL å®‰è£… Ubuntuï¼Œç”¨cpuè¿è¡Œ
>
> ç¯å¢ƒè®¾ç½®ï¼Œå¯ä»¥å‚è€ƒï¼šhttps://github.com/GMyhf/2025fall-cs201/blob/main/LLM/Build%20LLM%20Setup_window.md
>
> ```
> $ CUDA_VISIBLE_DEVICES="" python mnist_resnet18.py
> ```
>
> Windows 10 ä¸“ä¸šç‰ˆï¼Œç‰ˆæœ¬å·22H2ï¼Œå®‰è£…æ—¥æœŸ 2021/6/12
>
> å¤„ç†å™¨ Intel(R)Xeon(R)W-2223 CPU @ 3.60GHz 3.60GHz
> æœºå¸¦ RAM 16.0 GB (15.7 GB å¯ç”¨)
> ç³»ç»Ÿç±»å‹ 64 ä½æ“ä½œç³»ç»Ÿ, åŸºäº x64 çš„å¤„ç†å™¨
>
> ```
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:06<00:00, 1.52MB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 133kB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:01<00:00, 905kB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 9.23MB/s]
> Using device: cpu
> Starting training with early stopping...
> /home/yhf/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
>   warnings.warn(warn_msg)
> [1,   100] loss: 0.099
> [1,   200] loss: 0.072
> [1,   300] loss: 0.038
> [1,   400] loss: 0.079
> [1] Avg Loss: 0.149
> [2,   100] loss: 0.028
> [2,   200] loss: 0.010
> [2,   300] loss: 0.012
> [2,   400] loss: 0.112
> [2] Avg Loss: 0.053
> [3,   100] loss: 0.017
> [3,   200] loss: 0.032
> [3,   300] loss: 0.009
> [3,   400] loss: 0.027
> [3] Avg Loss: 0.037
> [4,   100] loss: 0.024
> [4,   200] loss: 0.087
> [4,   300] loss: 0.010
> [4,   400] loss: 0.045
> [4] Avg Loss: 0.032
> [5,   100] loss: 0.034
> [5,   200] loss: 0.009
> [5,   300] loss: 0.019
> [5,   400] loss: 0.038
> [5] Avg Loss: 0.026
> [6,   100] loss: 0.032
> [6,   200] loss: 0.046
> [6,   300] loss: 0.037
> [6,   400] loss: 0.039
> [6] Avg Loss: 0.023
> [7,   100] loss: 0.038
> [7,   200] loss: 0.018
> [7,   300] loss: 0.002
> [7,   400] loss: 0.079
> [7] Avg Loss: 0.022
> [8,   100] loss: 0.002
> [8,   200] loss: 0.005
> [8,   300] loss: 0.044
> [8,   400] loss: 0.004
> [8] Avg Loss: 0.018
> [9,   100] loss: 0.016
> [9,   200] loss: 0.009
> [9,   300] loss: 0.030
> [9,   400] loss: 0.014
> [9] Avg Loss: 0.015
> [10,   100] loss: 0.004
> [10,   200] loss: 0.023
> [10,   300] loss: 0.027
> [10,   400] loss: 0.023
> [10] Avg Loss: 0.017
> No improvement. Patience: 1/10
> [11,   100] loss: 0.025
> [11,   200] loss: 0.034
> [11,   300] loss: 0.012
> [11,   400] loss: 0.016
> [11] Avg Loss: 0.014
> [12,   100] loss: 0.004
> [12,   200] loss: 0.002
> [12,   300] loss: 0.014
> [12,   400] loss: 0.005
> [12] Avg Loss: 0.013
> [13,   100] loss: 0.002
> [13,   200] loss: 0.022
> [13,   300] loss: 0.019
> [13,   400] loss: 0.003
> [13] Avg Loss: 0.013
> [14,   100] loss: 0.010
> [14,   200] loss: 0.004
> [14,   300] loss: 0.028
> [14,   400] loss: 0.018
> [14] Avg Loss: 0.013
> No improvement. Patience: 1/10
> [15,   100] loss: 0.009
> [15,   200] loss: 0.002
> [15,   300] loss: 0.118
> [15,   400] loss: 0.011
> [15] Avg Loss: 0.012
> [16,   100] loss: 0.025
> [16,   200] loss: 0.001
> [16,   300] loss: 0.058
> [16,   400] loss: 0.005
> [16] Avg Loss: 0.011
> [17,   100] loss: 0.000
> [17,   200] loss: 0.008
> [17,   300] loss: 0.015
> [17,   400] loss: 0.034
> [17] Avg Loss: 0.013
> No improvement. Patience: 1/10
> [18,   100] loss: 0.002
> [18,   200] loss: 0.015
> [18,   300] loss: 0.001
> [18,   400] loss: 0.001
> [18] Avg Loss: 0.010
> [19,   100] loss: 0.011
> [19,   200] loss: 0.011
> [19,   300] loss: 0.001
> [19,   400] loss: 0.010
> [19] Avg Loss: 0.011
> No improvement. Patience: 1/10
> [20,   100] loss: 0.013
> [20,   200] loss: 0.000
> [20,   300] loss: 0.006
> [20,   400] loss: 0.001
> [20] Avg Loss: 0.010
> No improvement. Patience: 2/10
> [21,   100] loss: 0.008
> [21,   200] loss: 0.005
> [21,   300] loss: 0.036
> [21,   400] loss: 0.047
> [21] Avg Loss: 0.012
> No improvement. Patience: 3/10
> [22,   100] loss: 0.001
> [22,   200] loss: 0.006
> [22,   300] loss: 0.003
> [22,   400] loss: 0.001
> [22] Avg Loss: 0.011
> No improvement. Patience: 4/10
> [23,   100] loss: 0.001
> [23,   200] loss: 0.006
> [23,   300] loss: 0.001
> [23,   400] loss: 0.002
> [23] Avg Loss: 0.010
> No improvement. Patience: 5/10
> [24,   100] loss: 0.008
> [24,   200] loss: 0.010
> [24,   300] loss: 0.047
> [24,   400] loss: 0.056
> [24] Avg Loss: 0.009
> [25,   100] loss: 0.009
> [25,   200] loss: 0.012
> [25,   300] loss: 0.002
> [25,   400] loss: 0.027
> [25] Avg Loss: 0.010
> No improvement. Patience: 1/10
> [26,   100] loss: 0.017
> [26,   200] loss: 0.000
> [26,   300] loss: 0.011
> [26,   400] loss: 0.003
> [26] Avg Loss: 0.012
> No improvement. Patience: 2/10
> [27,   100] loss: 0.024
> [27,   200] loss: 0.004
> [27,   300] loss: 0.009
> [27,   400] loss: 0.019
> [27] Avg Loss: 0.009
> No improvement. Patience: 3/10
> [28,   100] loss: 0.004
> [28,   200] loss: 0.005
> [28,   300] loss: 0.029
> [28,   400] loss: 0.029
> [28] Avg Loss: 0.011
> No improvement. Patience: 4/10
> [29,   100] loss: 0.031
> [29,   200] loss: 0.008
> [29,   300] loss: 0.022
> [29,   400] loss: 0.003
> [29] Avg Loss: 0.009
> [30,   100] loss: 0.055
> [30,   200] loss: 0.009
> [30,   300] loss: 0.002
> [30,   400] loss: 0.010
> [30] Avg Loss: 0.010
> No improvement. Patience: 1/10
> [31,   100] loss: 0.001
> [31,   200] loss: 0.006
> [31,   300] loss: 0.019
> [31,   400] loss: 0.005
> [31] Avg Loss: 0.010
> No improvement. Patience: 2/10
> [32,   100] loss: 0.001
> [32,   200] loss: 0.007
> [32,   300] loss: 0.001
> [32,   400] loss: 0.001
> [32] Avg Loss: 0.009
> No improvement. Patience: 3/10
> [33,   100] loss: 0.000
> [33,   200] loss: 0.004
> [33,   300] loss: 0.001
> [33,   400] loss: 0.001
> [33] Avg Loss: 0.009
> [34,   100] loss: 0.002
> [34,   200] loss: 0.003
> [34,   300] loss: 0.020
> [34,   400] loss: 0.002
> [34] Avg Loss: 0.010
> No improvement. Patience: 1/10
> [35,   100] loss: 0.002
> [35,   200] loss: 0.000
> [35,   300] loss: 0.019
> [35,   400] loss: 0.015
> [35] Avg Loss: 0.009
> No improvement. Patience: 2/10
> [36,   100] loss: 0.002
> [36,   200] loss: 0.080
> [36,   300] loss: 0.001
> [36,   400] loss: 0.010
> [36] Avg Loss: 0.012
> No improvement. Patience: 3/10
> [37,   100] loss: 0.016
> [37,   200] loss: 0.028
> [37,   300] loss: 0.004
> [37,   400] loss: 0.007
> [37] Avg Loss: 0.008
> [38,   100] loss: 0.001
> [38,   200] loss: 0.001
> [38,   300] loss: 0.002
> [38,   400] loss: 0.004
> [38] Avg Loss: 0.008
> [39,   100] loss: 0.001
> [39,   200] loss: 0.008
> [39,   300] loss: 0.002
> [39,   400] loss: 0.003
> [39] Avg Loss: 0.008
> No improvement. Patience: 1/10
> [40,   100] loss: 0.001
> [40,   200] loss: 0.008
> [40,   300] loss: 0.006
> [40,   400] loss: 0.003
> [40] Avg Loss: 0.010
> No improvement. Patience: 2/10
> [41,   100] loss: 0.006
> [41,   200] loss: 0.001
> [41,   300] loss: 0.006
> [41,   400] loss: 0.006
> [41] Avg Loss: 0.009
> No improvement. Patience: 3/10
> [42,   100] loss: 0.004
> [42,   200] loss: 0.005
> [42,   300] loss: 0.010
> [42,   400] loss: 0.002
> [42] Avg Loss: 0.009
> No improvement. Patience: 4/10
> [43,   100] loss: 0.001
> [43,   200] loss: 0.002
> [43,   300] loss: 0.021
> [43,   400] loss: 0.001
> [43] Avg Loss: 0.008
> No improvement. Patience: 5/10
> [44,   100] loss: 0.046
> [44,   200] loss: 0.002
> [44,   300] loss: 0.023
> [44,   400] loss: 0.008
> [44] Avg Loss: 0.010
> No improvement. Patience: 6/10
> [45,   100] loss: 0.021
> [45,   200] loss: 0.001
> [45,   300] loss: 0.006
> [45,   400] loss: 0.001
> [45] Avg Loss: 0.011
> No improvement. Patience: 7/10
> [46,   100] loss: 0.008
> [46,   200] loss: 0.006
> [46,   300] loss: 0.002
> [46,   400] loss: 0.049
> [46] Avg Loss: 0.010
> No improvement. Patience: 8/10
> [47,   100] loss: 0.001
> [47,   200] loss: 0.015
> [47,   300] loss: 0.001
> [47,   400] loss: 0.026
> [47] Avg Loss: 0.008
> No improvement. Patience: 9/10
> [48,   100] loss: 0.002
> [48,   200] loss: 0.002
> [48,   300] loss: 0.002
> [48,   400] loss: 0.004
> [48] Avg Loss: 0.010
> No improvement. Patience: 10/10
> Early stopping triggered.
> âœ… Training completed in 75.11 minutes.
> Accuracy on test images: 99.35%
> Accuracy of 0    : 99.59%
> Accuracy of 1    : 99.91%
> Accuracy of 2    : 99.22%
> Accuracy of 3    : 99.50%
> Accuracy of 4    : 99.59%
> Accuracy of 5    : 99.22%
> Accuracy of 6    : 99.37%
> Accuracy of 7    : 99.22%
> Accuracy of 8    : 99.38%
> Accuracy of 9    : 98.41%
> /home/yhf/NNCode/mnist_resnet18.py:142: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
>   plt.show()
> ```
>
> 



### 4.4.3 åœ¨32GBå†…å­˜çš„ clab äº‘è™šæ‹Ÿæœºè¿è¡Œ

2025å¹´11æœˆ26æ—¥ï¼Œåœ¨clabäº‘è™šæ‹Ÿæœºè·‘ã€‚è™šæ‹Ÿæœºåªæœ‰CPUã€‚

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/33caace9ba4252b9bcc6707b67f18746.png" alt="33caace9ba4252b9bcc6707b67f18746" style="zoom: 33%;" />

> 
>
> ```
> (.venv) [rocky@jensen AI_literacy]$ python MNIST_nn.py 
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [06:09<00:00, 26.8kB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 113kB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:01<00:00, 1.12MB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 4.41MB/s]
> Using device: cpu
> Starting training with early stopping...
> /home/rocky/AI_literacy/.venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
>   warnings.warn(warn_msg)
> [1,   100] loss: 0.200
> [1,   200] loss: 0.084
> [1,   300] loss: 0.060
> [1,   400] loss: 0.100
> [1] Avg Loss: 0.150
> [2,   100] loss: 0.095
> [2,   200] loss: 0.102
> [2,   300] loss: 0.010
> [2,   400] loss: 0.032
> [2] Avg Loss: 0.049
> [3,   100] loss: 0.024
> [3,   200] loss: 0.051
> [3,   300] loss: 0.064
> [3,   400] loss: 0.019
> [3] Avg Loss: 0.039
> [4,   100] loss: 0.040
> [4,   200] loss: 0.071
> [4,   300] loss: 0.068
> [4,   400] loss: 0.052
> [4] Avg Loss: 0.030
> [5,   100] loss: 0.039
> [5,   200] loss: 0.031
> [5,   300] loss: 0.005
> [5,   400] loss: 0.002
> [5] Avg Loss: 0.025
> [6,   100] loss: 0.081
> [6,   200] loss: 0.019
> [6,   300] loss: 0.040
> [6,   400] loss: 0.006
> [6] Avg Loss: 0.023
> [7,   100] loss: 0.026
> [7,   200] loss: 0.004
> [7,   300] loss: 0.038
> [7,   400] loss: 0.020
> [7] Avg Loss: 0.021
> [8,   100] loss: 0.002
> [8,   200] loss: 0.009
> [8,   300] loss: 0.011
> [8,   400] loss: 0.005
> [8] Avg Loss: 0.019
> [9,   100] loss: 0.024
> [9,   200] loss: 0.002
> [9,   300] loss: 0.037
> [9,   400] loss: 0.010
> [9] Avg Loss: 0.016
> [10,   100] loss: 0.005
> [10,   200] loss: 0.020
> [10,   300] loss: 0.020
> [10,   400] loss: 0.002
> [10] Avg Loss: 0.017
> No improvement. Patience: 1/10
> [11,   100] loss: 0.001
> [11,   200] loss: 0.049
> [11,   300] loss: 0.004
> [11,   400] loss: 0.089
> [11] Avg Loss: 0.016
> [12,   100] loss: 0.011
> [12,   200] loss: 0.008
> [12,   300] loss: 0.001
> [12,   400] loss: 0.005
> [12] Avg Loss: 0.014
> [13,   100] loss: 0.005
> [13,   200] loss: 0.001
> [13,   300] loss: 0.004
> [13,   400] loss: 0.002
> [13] Avg Loss: 0.013
> [14,   100] loss: 0.011
> [14,   200] loss: 0.030
> [14,   300] loss: 0.001
> [14,   400] loss: 0.016
> [14] Avg Loss: 0.013
> No improvement. Patience: 1/10
> [15,   100] loss: 0.002
> [15,   200] loss: 0.002
> [15,   300] loss: 0.000
> [15,   400] loss: 0.004
> [15] Avg Loss: 0.012
> [16,   100] loss: 0.019
> [16,   200] loss: 0.011
> [16,   300] loss: 0.010
> [16,   400] loss: 0.007
> [16] Avg Loss: 0.012
> No improvement. Patience: 1/10
> [17,   100] loss: 0.000
> [17,   200] loss: 0.004
> [17,   300] loss: 0.001
> [17,   400] loss: 0.001
> [17] Avg Loss: 0.011
> [18,   100] loss: 0.002
> [18,   200] loss: 0.005
> [18,   300] loss: 0.003
> [18,   400] loss: 0.027
> [18] Avg Loss: 0.012
> No improvement. Patience: 1/10
> [19,   100] loss: 0.001
> [19,   200] loss: 0.002
> [19,   300] loss: 0.008
> [19,   400] loss: 0.013
> [19] Avg Loss: 0.012
> No improvement. Patience: 2/10
> [20,   100] loss: 0.003
> [20,   200] loss: 0.018
> [20,   300] loss: 0.005
> [20,   400] loss: 0.017
> [20] Avg Loss: 0.011
> No improvement. Patience: 3/10
> [21,   100] loss: 0.012
> [21,   200] loss: 0.007
> [21,   300] loss: 0.001
> [21,   400] loss: 0.019
> [21] Avg Loss: 0.011
> No improvement. Patience: 4/10
> [22,   100] loss: 0.003
> [22,   200] loss: 0.006
> [22,   300] loss: 0.004
> [22,   400] loss: 0.007
> [22] Avg Loss: 0.009
> [23,   100] loss: 0.014
> [23,   200] loss: 0.015
> [23,   300] loss: 0.024
> [23,   400] loss: 0.005
> [23] Avg Loss: 0.011
> No improvement. Patience: 1/10
> [24,   100] loss: 0.032
> [24,   200] loss: 0.015
> [24,   300] loss: 0.006
> [24,   400] loss: 0.001
> [24] Avg Loss: 0.011
> No improvement. Patience: 2/10
> [25,   100] loss: 0.004
> [25,   200] loss: 0.001
> [25,   300] loss: 0.011
> [25,   400] loss: 0.000
> [25] Avg Loss: 0.010
> No improvement. Patience: 3/10
> [26,   100] loss: 0.001
> [26,   200] loss: 0.001
> [26,   300] loss: 0.011
> [26,   400] loss: 0.001
> [26] Avg Loss: 0.009
> No improvement. Patience: 4/10
> [27,   100] loss: 0.009
> [27,   200] loss: 0.011
> [27,   300] loss: 0.001
> [27,   400] loss: 0.005
> [27] Avg Loss: 0.009
> No improvement. Patience: 5/10
> [28,   100] loss: 0.001
> [28,   200] loss: 0.001
> [28,   300] loss: 0.000
> [28,   400] loss: 0.030
> [28] Avg Loss: 0.008
> [29,   100] loss: 0.005
> [29,   200] loss: 0.002
> [29,   300] loss: 0.003
> [29,   400] loss: 0.003
> [29] Avg Loss: 0.010
> No improvement. Patience: 1/10
> [30,   100] loss: 0.001
> [30,   200] loss: 0.030
> [30,   300] loss: 0.021
> [30,   400] loss: 0.012
> [30] Avg Loss: 0.011
> No improvement. Patience: 2/10
> [31,   100] loss: 0.004
> [31,   200] loss: 0.002
> [31,   300] loss: 0.010
> [31,   400] loss: 0.006
> [31] Avg Loss: 0.009
> No improvement. Patience: 3/10
> [32,   100] loss: 0.056
> [32,   200] loss: 0.036
> [32,   300] loss: 0.014
> [32,   400] loss: 0.005
> [32] Avg Loss: 0.009
> No improvement. Patience: 4/10
> [33,   100] loss: 0.006
> [33,   200] loss: 0.011
> [33,   300] loss: 0.050
> [33,   400] loss: 0.075
> [33] Avg Loss: 0.009
> No improvement. Patience: 5/10
> [34,   100] loss: 0.005
> [34,   200] loss: 0.002
> [34,   300] loss: 0.001
> [34,   400] loss: 0.003
> [34] Avg Loss: 0.010
> No improvement. Patience: 6/10
> [35,   100] loss: 0.009
> [35,   200] loss: 0.014
> [35,   300] loss: 0.001
> [35,   400] loss: 0.021
> [35] Avg Loss: 0.010
> No improvement. Patience: 7/10
> [36,   100] loss: 0.002
> [36,   200] loss: 0.001
> [36,   300] loss: 0.002
> [36,   400] loss: 0.005
> [36] Avg Loss: 0.008
> [37,   100] loss: 0.002
> [37,   200] loss: 0.041
> [37,   300] loss: 0.000
> [37,   400] loss: 0.015
> [37] Avg Loss: 0.009
> No improvement. Patience: 1/10
> [38,   100] loss: 0.013
> [38,   200] loss: 0.004
> [38,   300] loss: 0.001
> [38,   400] loss: 0.005
> [38] Avg Loss: 0.010
> No improvement. Patience: 2/10
> [39,   100] loss: 0.021
> [39,   200] loss: 0.002
> [39,   300] loss: 0.001
> [39,   400] loss: 0.003
> [39] Avg Loss: 0.010
> No improvement. Patience: 3/10
> [40,   100] loss: 0.001
> [40,   200] loss: 0.001
> [40,   300] loss: 0.019
> [40,   400] loss: 0.076
> [40] Avg Loss: 0.010
> No improvement. Patience: 4/10
> [41,   100] loss: 0.001
> [41,   200] loss: 0.012
> [41,   300] loss: 0.014
> [41,   400] loss: 0.009
> [41] Avg Loss: 0.012
> No improvement. Patience: 5/10
> [42,   100] loss: 0.003
> [42,   200] loss: 0.002
> [42,   300] loss: 0.053
> [42,   400] loss: 0.001
> [42] Avg Loss: 0.010
> No improvement. Patience: 6/10
> [43,   100] loss: 0.011
> [43,   200] loss: 0.000
> [43,   300] loss: 0.006
> [43,   400] loss: 0.005
> [43] Avg Loss: 0.008
> No improvement. Patience: 7/10
> [44,   100] loss: 0.013
> [44,   200] loss: 0.030
> [44,   300] loss: 0.021
> [44,   400] loss: 0.004
> [44] Avg Loss: 0.010
> No improvement. Patience: 8/10
> [45,   100] loss: 0.003
> [45,   200] loss: 0.012
> [45,   300] loss: 0.002
> [45,   400] loss: 0.037
> [45] Avg Loss: 0.008
> No improvement. Patience: 9/10
> [46,   100] loss: 0.001
> [46,   200] loss: 0.006
> [46,   300] loss: 0.000
> [46,   400] loss: 0.020
> [46] Avg Loss: 0.010
> No improvement. Patience: 10/10
> Early stopping triggered.
> âœ… Training completed in 91.75 minutes.
> Accuracy on test images: 99.43%
> Accuracy of 0    : 99.80%
> Accuracy of 1    : 99.65%
> Accuracy of 2    : 99.03%
> Accuracy of 3    : 99.70%
> Accuracy of 4    : 99.19%
> Accuracy of 5    : 98.77%
> Accuracy of 6    : 99.37%
> Accuracy of 7    : 99.71%
> Accuracy of 8    : 99.38%
> Accuracy of 9    : 99.60%
> ```
>
> 



## 4.5 å®ä¾‹ï¼šCIFAR-10å›¾åƒåˆ†ç±»

CIFAR-10 æ•°æ®é›†åŒ…å« 60,000 å¼  32Ã—32å½©è‰²å›¾åƒï¼Œå…±10ä¸ªç±»åˆ«ã€‚ç”±äºå›¾åƒæ›´å¤æ‚ï¼Œæˆ‘ä»¬ç»§ç»­ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹ï¼ˆå¦‚ResNet18æˆ–ResNet34ï¼‰è¿›è¡Œè®­ç»ƒã€‚æµç¨‹ç±»ä¼¼MNISTï¼Œä½†è¾“å…¥é€šé“ä¸º3ã€‚è®­ç»ƒåï¼Œç°ä»£æ¶æ„é€šå¸¸èƒ½è¾¾åˆ°70%â€“90%çš„æµ‹è¯•å‡†ç¡®ç‡ï¼ˆå–å†³äºç½‘ç»œæ·±åº¦å’Œè®­ç»ƒç­–ç•¥ï¼‰ã€‚è¯¥å®éªŒå¸®åŠ©å­¦ç”Ÿç†è§£å°å‹å½©è‰²å›¾åƒé›†ä¸Šçš„å·ç§¯ç½‘ç»œè®­ç»ƒè¦ç‚¹ï¼ˆå¦‚æ•°æ®å¢å¼ºã€å­¦ä¹ ç‡è°ƒæ•´ï¼‰ã€‚

```python
import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import matplotlib.pyplot as plt
import numpy as np
import time

def main():
    # 1. æ•°æ®å¢å¼º + é¢„å¤„ç†
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),  # éšæœºæ—‹è½¬
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # è‰²å½©è°ƒæ•´
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)

    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2, pin_memory=True)

    classes = ('plane', 'car', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck')

    # 2. è®¾ç½®è®¾å¤‡å’Œæ¨¡å‹
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print("Using device:", device)

    # åŠ è½½é¢„å®šä¹‰çš„ ResNet18 å¹¶ä¿®æ”¹è¾“å‡ºå±‚
    net = models.resnet18(weights=None)
    net.fc = nn.Linear(net.fc.in_features, 10)  # CIFAR10 10 ç±»
    net.to(device)

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

    # 3. è®­ç»ƒè¿‡ç¨‹
    best_loss = float('inf')
    patience = 10 # æé«˜è€å¿ƒ
    patience_counter = 0

    start_time = time.time()
    print("Starting training with early stopping...")
    for epoch in range(800):  # å¯é€‚å½“å¢å¤§ epoch
        net.train()
        epoch_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            if i % 100 == 99:
                print(f"[{epoch + 1}, {i + 1:5d}] loss: {loss.item():.3f}")

        avg_loss = epoch_loss / len(trainloader)
        print(f"[{epoch+1}] Avg Loss: {avg_loss:.3f}")

        if avg_loss < best_loss - 1e-4:
            best_loss = avg_loss
            patience_counter = 0
        else:
            patience_counter += 1
            print(f"No improvement. Patience: {patience_counter}/{patience}")
            if patience_counter >= patience:
                print("Early stopping triggered.")
                break



    end_time = time.time()
    execution_time_minutes = (end_time - start_time) / 60
    print(f"âœ… Training completed in {execution_time_minutes:.2f} minutes.")


    # ä¿å­˜æ¨¡å‹
    torch.save(net.state_dict(), './resnet18_cifar10_data_augument.pth')

    # 4. æµ‹è¯•å‡†ç¡®ç‡
    correct = 0
    total = 0
    net.eval()
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f"Accuracy on test images: {100 * correct / total:.2f}%")

    # æ¯ç±»å‡†ç¡®ç‡
    class_correct = list(0. for _ in range(10))
    class_total = list(0. for _ in range(10))
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            c = (predicted == labels).squeeze()
            for i in range(len(labels)):
                class_correct[labels[i]] += c[i].item()
                class_total[labels[i]] += 1

    for i in range(10):
        print(f'Accuracy of {classes[i]:5s}: {100 * class_correct[i] / class_total[i]:.2f}%')

    # --- å¯è§†åŒ–é¢„æµ‹ ---

    def imshow_grid(images, labels, preds=None, classes=None, rows=8, cols=8):
        images = images.cpu() / 2 + 0.5  # unnormalize
        npimg = images.numpy()
        fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.5, rows * 1.5))
        for i in range(rows * cols):
            r, c = divmod(i, cols)
            ax = axes[r, c]
            img = np.transpose(npimg[i], (1, 2, 0))
            ax.imshow(img)
            title = f'{classes[labels[i]]}'
            if preds is not None:
                title += f'\nâ†’ {classes[preds[i]]}'
            ax.set_title(title, fontsize=8)
            ax.axis('off')
        plt.tight_layout()
        plt.show()

    # è·å–ä¸€æ‰¹å›¾åƒç”¨äºæ˜¾ç¤º
    dataiter = iter(testloader)
    images, labels = next(dataiter)
    while images.size(0) < 64:
        more_images, more_labels = next(dataiter)
        images = torch.cat([images, more_images], dim=0)
        labels = torch.cat([labels, more_labels], dim=0)
    images = images[:64]
    labels = labels[:64]

    # é¢„æµ‹
    net.eval()
    with torch.no_grad():
        outputs = net(images.to(device))
        _, predicted = torch.max(outputs, 1)

    # æ˜¾ç¤ºå›¾åƒç½‘æ ¼
    imshow_grid(images, labels, predicted.cpu(), classes=classes, rows=8, cols=8)

if __name__ == "__main__":
    import torch.multiprocessing
    torch.multiprocessing.set_start_method('spawn', force=True)
    main()

```



> è¯¦ç»†è®­ç»ƒæ—¥å¿—ï¼š
>
> ```
> /Users/hfyan/miniconda3/bin/python /Users/hfyan/Desktop/LLMs-from-scratch-main/runoob/pytorch-image-classification/image_classification-ResNet18-RandomCropFlipLR_Cosine.py 
> Using device: mps
> Starting training with early stopping...
> [1,   100] loss: 1.752
> [1,   200] loss: 1.675
> [1,   300] loss: 1.654
> [1] Avg Loss: 1.806
> [2,   100] loss: 1.497
> [2,   200] loss: 1.459
> [2,   300] loss: 1.453
> [2] Avg Loss: 1.520
> [3,   100] loss: 1.534
> [3,   200] loss: 1.383
> [3,   300] loss: 1.167
> [3] Avg Loss: 1.372
> [4,   100] loss: 1.390
> [4,   200] loss: 1.221
> [4,   300] loss: 1.238
> [4] Avg Loss: 1.244
> [5,   100] loss: 1.089
> [5,   200] loss: 1.020
> [5,   300] loss: 1.133
> [5] Avg Loss: 1.159
> ......
> No improvement. Patience: 1/10
> [192,   100] loss: 0.187
> [192,   200] loss: 0.293
> [192,   300] loss: 0.356
> [192] Avg Loss: 0.302
> [193,   100] loss: 0.223
> [193,   200] loss: 0.348
> [193,   300] loss: 0.309
> [193] Avg Loss: 0.301
> [194,   100] loss: 0.303
> [194,   200] loss: 0.219
> [194,   300] loss: 0.280
> [194] Avg Loss: 0.304
> No improvement. Patience: 1/10
> [195,   100] loss: 0.279
> [195,   200] loss: 0.296
> [195,   300] loss: 0.313
> [195] Avg Loss: 0.296
> [196,   100] loss: 0.254
> [196,   200] loss: 0.385
> [196,   300] loss: 0.280
> [196] Avg Loss: 0.300
> No improvement. Patience: 1/10
> [197,   100] loss: 0.216
> [197,   200] loss: 0.298
> [197,   300] loss: 0.290
> [197] Avg Loss: 0.298
> No improvement. Patience: 2/10
> [198,   100] loss: 0.267
> [198,   200] loss: 0.218
> [198,   300] loss: 0.367
> [198] Avg Loss: 0.290
> [199,   100] loss: 0.270
> [199,   200] loss: 0.240
> [199,   300] loss: 0.351
> [199] Avg Loss: 0.301
> No improvement. Patience: 1/10
> [200,   100] loss: 0.251
> [200,   200] loss: 0.227
> [200,   300] loss: 0.302
> [200] Avg Loss: 0.299
> No improvement. Patience: 2/10
> [201,   100] loss: 0.348
> [201,   200] loss: 0.301
> [201,   300] loss: 0.193
> [201] Avg Loss: 0.299
> No improvement. Patience: 3/10
> [202,   100] loss: 0.313
> [202,   200] loss: 0.329
> [202,   300] loss: 0.305
> [202] Avg Loss: 0.295
> No improvement. Patience: 4/10
> [203,   100] loss: 0.266
> [203,   200] loss: 0.254
> [203,   300] loss: 0.307
> [203] Avg Loss: 0.294
> No improvement. Patience: 5/10
> [204,   100] loss: 0.372
> [204,   200] loss: 0.295
> [204,   300] loss: 0.348
> [204] Avg Loss: 0.300
> No improvement. Patience: 6/10
> [205,   100] loss: 0.392
> [205,   200] loss: 0.353
> [205,   300] loss: 0.306
> [205] Avg Loss: 0.296
> No improvement. Patience: 7/10
> [206,   100] loss: 0.262
> [206,   200] loss: 0.213
> [206,   300] loss: 0.396
> [206] Avg Loss: 0.293
> No improvement. Patience: 8/10
> [207,   100] loss: 0.293
> [207,   200] loss: 0.204
> [207,   300] loss: 0.337
> [207] Avg Loss: 0.291
> No improvement. Patience: 9/10
> [208,   100] loss: 0.413
> [208,   200] loss: 0.294
> [208,   300] loss: 0.315
> [208] Avg Loss: 0.295
> No improvement. Patience: 10/10
> Early stopping triggered.
> âœ… Training completed in 79.91 minutes.
> Accuracy on test images: 83.57%
> Accuracy of plane: 83.70%
> Accuracy of car  : 92.20%
> Accuracy of bird : 78.70%
> Accuracy of cat  : 60.40%
> Accuracy of deer : 79.30%
> Accuracy of dog  : 77.40%
> Accuracy of frog : 90.30%
> Accuracy of horse: 92.50%
> Accuracy of ship : 88.50%
> Accuracy of truck: 92.70%
> 
> Process finished with exit code 0
> ```
>
> 
>
> <img src="https://raw.githubusercontent.com/GMyhf/img/main/img/202507280020181.jpg" alt="d561be986280572516ac1023e9ff715c" style="zoom:50%;" />
>
> 



## 4.6 å®ä¾‹ï¼šTiny ImageNet å›¾åƒåˆ†ç±»

Tiny ImageNetæ˜¯ä¸€ä¸ªæ›´å¤§è§„æ¨¡çš„å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼ŒåŒ…å«200ä¸ªç±»åˆ«çš„64Ã—64å½©è‰²å›¾åƒï¼Œæ¯ç±»çº¦500å¼ è®­ç»ƒå›¾åƒã€‚æˆ‘ä»¬ä½¿ç”¨æ›´æ·±çš„ç½‘ç»œï¼ˆå¦‚ResNet50æˆ–æ›´å¤§æ¨¡å‹ï¼‰å’Œæ›´å……åˆ†çš„è®­ç»ƒè¿­ä»£æ¥è§£å†³ã€‚è¯¥ä»»åŠ¡éœ€è¦æ›´å¤šç®—åŠ›ï¼ˆGPUæ”¯æŒï¼‰å’ŒæŠ€æœ¯ï¼ˆæ¯”å¦‚å­¦ä¹ ç‡è°ƒåº¦ã€æ­£åˆ™åŒ–ï¼‰ã€‚å®Œæˆåå­¦ç”Ÿå°†æŒæ¡ä»ä»£ç å®ç°åˆ°å®æˆ˜è°ƒä¼˜çš„å®Œæ•´æµç¨‹ï¼Œä½“ä¼šè®­ç»ƒé«˜å¤æ‚åº¦æ¨¡å‹çš„å·¥ç¨‹æŒ‘æˆ˜ã€‚

### 1.å‡†å¤‡Tiny ImageNetæ•°æ®é›†

Tiny ImageNetã€‚å®ƒåŒ…å« 200 ä¸ªç±»åˆ«ï¼Œæ¯ä¸ªç±»åˆ« 500 å¼ è®­ç»ƒå›¾ç‰‡ï¼Œæ€»æ•°æ®é‡å¤§çº¦ 500MBï¼Œéå¸¸é€‚åˆå®éªŒå’Œè°ƒè¯•ã€‚

å¯ä»¥ç”¨ä¸‹é¢æ–¹æ³•ä¸‹è½½åŠé¢„å¤„ç†ï¼Œæˆ–è€…ç›´æ¥ä¸‹è½½é¢„å¤„ç†å¥½çš„æ•°æ®ã€‚

> tiny-imagenet-200.zip, https://disk.pku.edu.cn/link/AA068C93E37D564808A74B8F282DCE0F11
> Name: tiny-imagenet-200.zip
> Expires: Never



ä¸‹è½½ `wget http://cs231n.stanford.edu/tiny-imagenet-200.zip`ï¼Œè®°237MBã€‚

éªŒè¯é›†é€šå¸¸è§£å‹åæ‰€æœ‰å›¾ç‰‡ä¼šåœ¨åŒä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œè€Œ ImageFolder è¦æ±‚æ¯ä¸ªç±»åˆ«æœ‰ç‹¬ç«‹å­æ–‡ä»¶å¤¹ã€‚ä½ éœ€è¦æ ¹æ®å®˜æ–¹æä¾›çš„ éªŒè¯é›†æ ‡ç­¾æ–‡ä»¶ï¼Œå¦‚ val_annotations.txtï¼Œå¯¹å›¾ç‰‡è¿›è¡Œåˆ†ç±»æ•´ç†ã€‚å¸¸è§çš„åšæ³•æ˜¯ç¼–å†™ä¸€ä¸ªè„šæœ¬ï¼Œæ ¹æ®æ–‡ä»¶ä¸­çš„ç±»åˆ«ä¿¡æ¯å°†å›¾ç‰‡ç§»åŠ¨åˆ°å¯¹åº”çš„å­æ–‡ä»¶å¤¹ä¸­ã€‚

è„šæœ¬`tinyimagenet.sh`

```sh
#!/bin/bash

# download and unzip dataset
#wget http://cs231n.stanford.edu/tiny-imagenet-200.zip
unzip tiny-imagenet-200.zip

current="$(pwd)/tiny-imagenet-200"

# training data
cd $current/train
for DIR in $(ls); do
   cd $DIR
   rm *.txt
   mv images/* .
   rm -r images
   cd ..
done

# validation data
cd $current/val
annotate_file="val_annotations.txt"
length=$(cat $annotate_file | wc -l)
for i in $(seq 1 $length); do
    # fetch i th line
    line=$(sed -n ${i}p $annotate_file)
    # get file name and directory name
    file=$(echo $line | cut -f1 -d" " )
    directory=$(echo $line | cut -f2 -d" ")
    mkdir -p $directory
    mv images/$file $directory
done
rm -r images
echo "done"

```



è¿è¡Œ`sh tinyimagenet.sh`ï¼Œæ•°æ®è§£å‹å¹¶åˆ†ç±»å‡†å¤‡å¥½ï¼Œè®°472MBã€‚

```
% ls -l
total 5200
drwxrwxr-x    3 hfyan  staff       96 Dec 12  2014 test
drwxrwxr-x  202 hfyan  staff     6464 Dec 12  2014 train
drwxrwxr-x  203 hfyan  staff     6496 Feb 24 11:09 val
-rw-rw-r--    1 hfyan  staff     2000 Feb  9  2015 wnids.txt
-rw-------    1 hfyan  staff  2655750 Feb  9  2015 words.txt
(base) hfyan@HongfeideMac-Studio tiny-imagenet-200 % pwd
/Users/hfyan/data/tiny-imagenet-200

```





### 2. è®­ç»ƒæ¨¡å‹

åŸºäº PyTorch å’Œ torchvision åº“çš„ç¤ºä¾‹ä»£ç ï¼Œè¯¥ä»£ç æ¼”ç¤ºäº†å¦‚ä½•åŠ è½½ ImageNet æ•°æ®é›†ã€æ„å»ºåŸºäºé¢„è®­ç»ƒ ResNet æ¨¡å‹çš„ç¥ç»ç½‘ç»œï¼Œå¹¶è¿›è¡Œå¾®è°ƒè®­ç»ƒå®ç°å›¾åƒåˆ†ç±»ã€‚

ä»£ç `tiny_imagenet_resnet50_epoch25.py`

```python
import os
import copy
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, models, transforms

# è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
def train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, num_epochs=25, device='cpu'):
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch+1, num_epochs))
        print('-' * 10)

        # æ¯ä¸ª epoch åˆ†ä¸ºè®­ç»ƒå’ŒéªŒè¯é˜¶æ®µ
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
            else:
                model.eval()   # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

            running_loss = 0.0
            running_corrects = 0

            # éå†æ•°æ®
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                optimizer.zero_grad()  # æ¢¯åº¦æ¸…é›¶

                # å‰å‘ä¼ æ’­
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    # ä»…åœ¨è®­ç»ƒé˜¶æ®µåå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            if phase == 'train':
                scheduler.step()

            epoch_loss = running_loss / dataset_sizes[phase]

            # MPS åç«¯ä¸æ”¯æŒ float64 è¿ç®—ã€‚è§£å†³æ–¹æ³•æ˜¯ä½¿ç”¨ float32ï¼Œå³è°ƒç”¨ .float()ã€‚
            #epoch_acc = running_corrects.double() / dataset_sizes[phase]
            epoch_acc = running_corrects.float() / dataset_sizes[phase]

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))

            # ä¿å­˜æœ€ä½³æ¨¡å‹å‚æ•°
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
        print()

    print('Best val Acc: {:.4f}'.format(best_acc))
    model.load_state_dict(best_model_wts)
    return model

def main():
    # 1. æ•°æ®é¢„å¤„ç†ä¸åŠ è½½
    # æ³¨æ„ï¼šæ­¤å¤„å‡å®šImageNetæ•°æ®é›†æŒ‰ç…§train/valæ–‡ä»¶å¤¹åˆ†åˆ«å­˜æ”¾å„ç±»åˆ«å›¾ç‰‡ï¼Œ
    # ä¸”æ¯ä¸ªç±»åˆ«ä½œä¸ºä¸€ä¸ªå­æ–‡ä»¶å¤¹å­˜åœ¨
    data_transforms = {
        'train': transforms.Compose([
            transforms.RandomResizedCrop(224),           # éšæœºè£å‰ªä¸º224Ã—224
            transforms.RandomHorizontalFlip(),           # éšæœºæ°´å¹³ç¿»è½¬
            transforms.ToTensor(),                         # è½¬ä¸ºTensor
            transforms.Normalize([0.485, 0.456, 0.406],    # ImageNetå‡å€¼
                                 [0.229, 0.224, 0.225])      # ImageNetæ ‡å‡†å·®
        ]),
        'val': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),                           # ä¸­å¿ƒè£å‰ª
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],
                                 [0.229, 0.224, 0.225])
        ]),
    }

    # Tiny ImageNet æ•°æ®è·¯å¾„
    data_dir = '/Users/hfyan/data/tiny-imagenet-200'
    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                                data_transforms[x])
                      for x in ['train', 'val']}

    # è®¾ç½® num_workers ä¸º 4 ä»¥åˆ©ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½
    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],
                                                    batch_size=128,    # å¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                                                    shuffle=True,
                                                    num_workers=8)
                   for x in ['train', 'val']}

    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
    class_names = image_datasets['train'].classes

    #device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # ä½¿ç”¨ MPS ä½œä¸º GPU åç«¯ï¼ˆé€‚ç”¨äº Apple Siliconï¼‰
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("Using MPS device for GPU acceleration")
    else:
        device = torch.device("cpu")
        print("MPS device not available, using CPU")

    #2. æ„å»ºæ¨¡å‹ï¼ˆä½¿ç”¨é¢„è®­ç»ƒ ResNet50ï¼‰
    # è¿™é‡Œæˆ‘ä»¬åŠ è½½é¢„è®­ç»ƒçš„ ResNet50 æ¨¡å‹ï¼Œå¹¶ä¿®æ”¹æœ€åçš„å…¨è¿æ¥å±‚ä»¥é€‚åº”Tiny ImageNetçš„ç±»åˆ«æ•°ï¼ˆ200ç±»ï¼‰
    model_ft = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
    num_ftrs = model_ft.fc.in_features
    model_ft.fc = nn.Linear(num_ftrs, len(class_names))
    model_ft = model_ft.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)

    # å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥ï¼Œæ¯7ä¸ªepoché™ä½ä¸€æ¬¡å­¦ä¹ ç‡
    exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

    #3. è®­ç»ƒæ¨¡å‹
    num_epochs = 25  # å¯æ ¹æ®éœ€è¦è°ƒæ•´epochæ•°é‡
    model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,
                           dataloaders, dataset_sizes, num_epochs=num_epochs, device=device)

    #4. ä¿å­˜æ¨¡å‹ï¼Œæ–‡ä»¶åå»ºè®®ä¸º tiny_imagenet_resnet50_epoch25.pth
    torch.save(model_ft.state_dict(), 'tiny_imagenet_resnet50_epoch25.pth')
    print("Model saved as tiny_imagenet_resnet50_epoch25.pth")

if __name__ == '__main__':
    main()

```

> **è¯´æ˜**
>
> - **æ•°æ®é¢„å¤„ç†**
>   ä½¿ç”¨äº† `transforms` å¯¹æ•°æ®è¿›è¡Œäº†æ•°æ®å¢å¼ºï¼ˆå¦‚éšæœºè£å‰ªã€æ°´å¹³ç¿»è½¬ï¼‰ä»¥åŠå½’ä¸€åŒ–ï¼ˆImageNetå¸¸ç”¨çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼‰ã€‚æ•°æ®æ–‡ä»¶å¤¹éœ€è¦ç¬¦åˆ `ImageFolder` çš„è¦æ±‚ï¼Œæ¯ä¸ªç±»åˆ«å­˜æ”¾åœ¨ç‹¬ç«‹çš„å­æ–‡ä»¶å¤¹ä¸­ã€‚
> - **æ¨¡å‹æ„å»º**
>   æœ¬ç¤ºä¾‹ä¸­é‡‡ç”¨é¢„è®­ç»ƒçš„ ResNet50 æ¨¡å‹ï¼Œå¹¶ä¿®æ”¹äº†æœ€åä¸€å±‚å…¨è¿æ¥å±‚ä»¥è¾“å‡ºä¸ç±»åˆ«æ•°åŒ¹é…çš„æ¦‚ç‡åˆ†å¸ƒã€‚
> - **è®­ç»ƒè¿‡ç¨‹**
>   ä»£ç ä¸­å®šä¹‰äº† `train_model` å‡½æ•°ï¼Œå¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ï¼Œå¹¶åœ¨éªŒè¯é›†ä¸Šé€‰å–å‡†ç¡®ç‡æœ€é«˜çš„æ¨¡å‹å‚æ•°ã€‚å­¦ä¹ ç‡è°ƒåº¦å™¨ç”¨äºé€æ­¥é™ä½å­¦ä¹ ç‡ä»¥ä¾¿æ›´å¥½åœ°æ”¶æ•›ã€‚
> - **æ³¨æ„äº‹é¡¹**
>   - ImageNet æ•°æ®é›†è¾ƒå¤§ï¼Œå»ºè®®åœ¨ä½¿ç”¨æ—¶æ³¨æ„æ•°æ®åŠ è½½ã€å†…å­˜ç®¡ç†å’Œè®­ç»ƒæ—¶é•¿ã€‚
>   - å¦‚éœ€æ›´æ·±å…¥çš„æ¨¡å‹è°ƒä¼˜æˆ–ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·å‚è€ƒ PyTorch å®˜æ–¹æ–‡æ¡£å’Œç›¸å…³èµ„æ–™ã€‚
>
> è¯¥ç¤ºä¾‹ä»£ç ä¸ºå…¥é—¨çº§ç¤ºä¾‹ï¼Œå®é™…é¡¹ç›®ä¸­å¯èƒ½éœ€è¦æ›´å¤šçš„ä¼˜åŒ–å’Œé…ç½®ã€‚
>
> 
>
> **ä¸»å…¥å£ä¿æŠ¤**ï¼šæ‰€æœ‰æ¶‰åŠå¤šè¿›ç¨‹æˆ–å¤šçº¿ç¨‹çš„ä»£ç éƒ½å°è£…åœ¨ `if __name__ == '__main__':` ä¸‹ï¼Œé¿å… macOS ä¸‹çš„å¯åŠ¨é—®é¢˜ã€‚



> **2025/2/24 11:30å¼€å§‹è¿è¡Œï¼Œ16:00ç»“æŸ**
>
> ```
> (base) hfyan@HongfeideMac-Studio data % python tiny_imagenet_resnet50_epoch25.py 
> Using MPS device for GPU acceleration
> Epoch 1/25
> ----------
> train Loss: 5.0366 Acc: 0.0720
> val Loss: 4.1348 Acc: 0.2819
> 
> Epoch 2/25
> ----------
> train Loss: 3.2563 Acc: 0.3406
> val Loss: 1.7006 Acc: 0.6197
> 
> Epoch 3/25
> ----------
> train Loss: 2.1834 Acc: 0.5065
> val Loss: 1.2068 Acc: 0.7062
> 
> Epoch 4/25
> ----------
> train Loss: 1.8635 Acc: 0.5663
> val Loss: 1.0010 Acc: 0.7498
> 
> Epoch 5/25
> ----------
> train Loss: 1.6788 Acc: 0.6029
> val Loss: 0.8927 Acc: 0.7702
> 
> Epoch 6/25
> ----------
> train Loss: 1.5723 Acc: 0.6268
> val Loss: 0.8407 Acc: 0.7808
> 
> Epoch 7/25
> ----------
> train Loss: 1.5044 Acc: 0.6390
> val Loss: 0.7990 Acc: 0.7907
> 
> Epoch 8/25
> ----------
> train Loss: 1.4324 Acc: 0.6567
> val Loss: 0.7788 Acc: 0.7939
> 
> Epoch 9/25
> ----------
> train Loss: 1.4212 Acc: 0.6571
> val Loss: 0.7701 Acc: 0.7981
> 
> Epoch 10/25
> ----------
> train Loss: 1.4054 Acc: 0.6614
> val Loss: 0.7669 Acc: 0.7966
> 
> Epoch 11/25
> ----------
> train Loss: 1.4035 Acc: 0.6615
> val Loss: 0.7634 Acc: 0.7980
> 
> Epoch 12/25
> ----------
> train Loss: 1.3995 Acc: 0.6626
> val Loss: 0.7595 Acc: 0.7990
> 
> Epoch 13/25
> ----------
> train Loss: 1.3882 Acc: 0.6647
> val Loss: 0.7558 Acc: 0.7988
> 
> Epoch 14/25
> ----------
> train Loss: 1.3747 Acc: 0.6680
> val Loss: 0.7517 Acc: 0.7997
> 
> Epoch 15/25
> ----------
> train Loss: 1.3754 Acc: 0.6683
> val Loss: 0.7490 Acc: 0.8006
> 
> Epoch 16/25
> ----------
> train Loss: 1.3685 Acc: 0.6689
> val Loss: 0.7592 Acc: 0.7970
> 
> Epoch 17/25
> ----------
> train Loss: 1.3771 Acc: 0.6681
> val Loss: 0.7567 Acc: 0.8009
> 
> Epoch 18/25
> ----------
> train Loss: 1.3690 Acc: 0.6688
> val Loss: 0.7508 Acc: 0.8011
> 
> Epoch 19/25
> ----------
> train Loss: 1.3716 Acc: 0.6694
> val Loss: 0.7521 Acc: 0.8008
> 
> Epoch 20/25
> ----------
> train Loss: 1.3729 Acc: 0.6687
> val Loss: 0.7527 Acc: 0.8002
> 
> Epoch 21/25
> ----------
> train Loss: 1.3709 Acc: 0.6689
> val Loss: 0.7501 Acc: 0.8014
> 
> Epoch 22/25
> ----------
> train Loss: 1.3706 Acc: 0.6708
> val Loss: 0.7516 Acc: 0.8008
> 
> Epoch 23/25
> ----------
> train Loss: 1.3681 Acc: 0.6696
> val Loss: 0.7502 Acc: 0.8002
> 
> Epoch 24/25
> ----------
> train Loss: 1.3725 Acc: 0.6698
> val Loss: 0.7508 Acc: 0.8003
> 
> Epoch 25/25
> ----------
> train Loss: 1.3708 Acc: 0.6696
> val Loss: 0.7480 Acc: 0.8004
> 
> Best val Acc: 0.8014
> Model saved as tiny_imagenet_resnet50_epoch25.pth
> 
> ```
>
> è·‘äº†4å°æ—¶30åˆ†é’Ÿã€‚
>
> <img src="https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250224171542842.png" alt="image-20250224171542842" style="zoom:50%;" />
>
> 
>
> ```
> % ls -lh *.pth
> -rw-r--r--  1 hfyan  staff    92M Feb 24 16:02 tiny_imagenet_resnet50_epoch25.pth
> ```
>



### 3.åŠ è½½è®­ç»ƒå¥½çš„çš„æ¨¡å‹å¹¶è¿›è¡ŒéªŒè¯

å‰é¢å·²ç»ä¿å­˜äº†æ¨¡å‹æƒé‡ï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹æ­¥éª¤åŠ è½½æ¨¡å‹å¹¶åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼š

1. **åŠ è½½æ¨¡å‹ç»“æ„å’Œæƒé‡**  
   è¯·ç¡®ä¿ä½ å®šä¹‰çš„æ¨¡å‹ç»“æ„ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ã€‚ä½¿ç”¨ `torch.load` åŠ è½½æƒé‡ï¼Œå¹¶ç”¨ `model.load_state_dict` å¯¼å…¥ã€‚

2. **åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼**  
   è°ƒç”¨ `model.eval()` ç¡®ä¿æ¨¡å‹å…³é—­ BatchNormã€Dropout ç­‰è®­ç»ƒæ—¶ç‰¹æœ‰çš„è¡Œä¸ºã€‚

3. **éå†éªŒè¯æ•°æ®å¹¶è®¡ç®—å‡†ç¡®ç‡**  
   ä½¿ç”¨ `torch.no_grad()` å…³é—­æ¢¯åº¦è®¡ç®—ï¼ŒåŠ å¿«éªŒè¯é€Ÿåº¦ï¼Œå¹¶é˜²æ­¢å†…å­˜æµªè´¹ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ä»£ç ï¼Œ`eval_tiny_imagenet_resnet50_epoch25_pth.py `ï¼š

```python
import os
import torch
import torch.nn as nn
from torchvision import datasets, models, transforms

# æ•°æ®é¢„å¤„ç†
data_transforms = {
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])
    ]),
}

# ç¡®ä¿å­è¿›ç¨‹å®‰å…¨å¯åŠ¨
if __name__ == '__main__':
    data_dir = '/Users/hfyan/data/tiny-imagenet-200'
    val_dir = os.path.join(data_dir, 'val')
    val_dataset = datasets.ImageFolder(val_dir, data_transforms['val'])
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64,
                                             shuffle=False, num_workers=4)

    # é€‰æ‹©è®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("Using MPS device for GPU acceleration")
    else:
        device = torch.device("cpu")
        print("MPS device not available, using CPU")

    # åŠ è½½æ¨¡å‹
    model_ft = models.resnet50(pretrained=False)
    num_ftrs = model_ft.fc.in_features
    model_ft.fc = nn.Linear(num_ftrs, len(val_dataset.classes))
    model_ft = model_ft.to(device)

    # åŠ è½½æ¨¡å‹æƒé‡
    model_path = 'tiny_imagenet_resnet50_epoch25.pth'
    model_ft.load_state_dict(torch.load(model_path, map_location=device))

    # è¯„ä¼°æ¨¡å¼
    model_ft.eval()

    # æ¨¡å‹è¯„ä¼°
    running_corrects = 0
    total_samples = 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model_ft(inputs)
            _, preds = torch.max(outputs, 1)
            running_corrects += torch.sum(preds == labels.data)
            total_samples += inputs.size(0)

    val_acc = running_corrects.float() / total_samples
    print('Validation Accuracy: {:.4f}'.format(val_acc))

```

> **éªŒè¯æ­¥éª¤**ï¼š  
>
> - åŠ è½½ä¸ä½ è®­ç»ƒæ—¶ä¸€è‡´çš„æ¨¡å‹ç»“æ„ã€‚  
> - ä½¿ç”¨ `model.load_state_dict()` åŠ è½½æƒé‡ã€‚  
> - è°ƒç”¨ `model.eval()` è¿›å…¥éªŒè¯æ¨¡å¼ã€‚  
> - éå†éªŒè¯æ•°æ®é›†ï¼Œè®¡ç®—å‡†ç¡®ç‡æˆ–å…¶ä»–æŒ‡æ ‡ã€‚
>
> è¿™æ ·ï¼Œä½ å°±å¯ä»¥åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹å¹¶å¯¹éªŒè¯é›†æ•°æ®è¿›è¡Œæµ‹è¯•ã€‚
>
> ![image-20250224171857564](https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250224171857564.png)
>
> 
>
> ```python
> (base) hfyan@HongfeideMac-Studio data % python eval_tiny_imagenet_resnet50_epoch25_pth.py 
> Using MPS device for GPU acceleration
> /Users/hfyan/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
>   warnings.warn(
> /Users/hfyan/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
>   warnings.warn(msg)
> Validation Accuracy: 0.8014
> (base) hfyan@HongfeideMac-Studio data % 
> ```
>



# 5. è¿è¡Œã€Šä»é›¶æ„å»ºå¤§æ¨¡å‹ã€‹ä»£ç 

ã€Šä»é›¶æ„å»ºå¤§æ¨¡å‹ã€‹ä»£ç ï¼Œhttps://github.com/rasbt/LLMs-from-scratch

Build a Large Language Model (From Scratch)

å¯ä»¥åœ¨æœ¬åœ° mac æˆ–è€… window è¿è¡Œï¼Œé…ç½®æ–¹æ³•æ–¹æ³•å¦‚ï¼š ...Setup_....mdã€‚

https://github.com/GMyhf/2025fall-cs201/tree/main/LLM



# é™„å½•

# A. PyTorchæ•™ç¨‹@runoob

https://www.runoob.com/pytorch/pytorch-tutorial.html

PyTorch æ˜¯ä¸€ä¸ªå¼€æºçš„æœºå™¨å­¦ä¹ åº“ï¼Œä¸»è¦ç”¨äºè¿›è¡Œè®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸçš„ç ”ç©¶å’Œå¼€å‘ã€‚

PyTorchç”± Facebook çš„äººå·¥æ™ºèƒ½ç ”ç©¶å›¢é˜Ÿå¼€å‘ï¼Œå¹¶åœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ç¤¾åŒºä¸­å¹¿æ³›ä½¿ç”¨ã€‚

PyTorch ä»¥å…¶çµæ´»æ€§å’Œæ˜“ç”¨æ€§è€Œé—»åï¼Œç‰¹åˆ«é€‚åˆäºæ·±åº¦å­¦ä¹ ç ”ç©¶å’Œå¼€å‘ã€‚



## 1.PyTorchå®‰è£…

åœ¨å·²ç»å®‰è£…å¥½çš„pythonç¯å¢ƒä¸­ï¼Œåœ¨terminalçª—å£å‘½ä»¤è¡Œä¸­ï¼Œæ¿€æ´»ç¯å¢ƒï¼Œå¹¶å®‰è£…torchåŒ…

> Pythonå¼€å‘ç¯å¢ƒé…ç½®æŒ‡å—ï¼Œhttps://github.com/GMyhf/2025fall-cs101/blob/main/Python_Development_Setup_Mac_Windows.md

```
[hfyan@HongfeideMac-Studio MyPythonApp % source .venv/bin/activate
(.venv) hfyan@HongfeideMac-Studio MyPythonApp % uv pip install torch torchvision torchaudio

```



**éªŒè¯å®‰è£…**

ä¸ºäº†ç¡®ä¿ PyTorch å·²æ­£ç¡®å®‰è£…ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ‰§è¡Œä»¥ä¸‹ PyTorch ä»£ç æ¥éªŒè¯æ˜¯å¦å®‰è£…æˆåŠŸï¼š

**å®ä¾‹**

```python
import torch

# å½“å‰å®‰è£…çš„ PyTorch åº“çš„ç‰ˆæœ¬
print(torch.__version__)

# æ£€æµ‹ MPS ä½œä¸º GPU åç«¯æ˜¯å¦å¯ç”¨ï¼ˆé€‚ç”¨äº Apple Siliconï¼‰
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("Using MPS device for GPU acceleration")
else:   # æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨ï¼Œå³ä½ çš„ç³»ç»Ÿæœ‰ NVIDIA çš„ GPU
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print("MPS device not available, using CPU")

"""
2.9.1
Using MPS device for GPU acceleration
"""
```



ä¸€ä¸ªç®€å•çš„å®ä¾‹ï¼Œæ„å»ºä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„å¼ é‡ï¼š

```python
import torch
x = torch.rand(5, 3)
print(x)
```

å¦‚æœå®‰è£…æˆåŠŸï¼Œè¾“å‡ºç»“æœç±»ä¼¼å¦‚ä¸‹ï¼š

```
tensor([[0.8622, 0.5916, 0.8073],
        [0.8955, 0.7416, 0.3482],
        [0.8059, 0.1414, 0.2828],
        [0.0923, 0.0113, 0.4226],
        [0.4438, 0.9789, 0.9048]])
```



**å®ä¾‹**

ä¸‹é¢çš„æ˜¯ PyTorch ä¸­ä¸€äº›åŸºæœ¬çš„å¼ é‡æ“ä½œï¼šå¦‚ä½•åˆ›å»ºéšæœºå¼ é‡ã€è¿›è¡Œé€å…ƒç´ è¿ç®—ã€è®¿é—®ç‰¹å®šå…ƒç´ ä»¥åŠè®¡ç®—æ€»å’Œå’Œæœ€å¤§å€¼ã€‚

```python
import torch

# è®¾ç½®æ•°æ®ç±»å‹å’Œè®¾å¤‡
dtype = torch.float  # å¼ é‡æ•°æ®ç±»å‹ä¸ºæµ®ç‚¹å‹
device = torch.device("cpu")  # æœ¬æ¬¡è®¡ç®—åœ¨ CPU ä¸Šè¿›è¡Œ

# åˆ›å»ºå¹¶æ‰“å°ä¸¤ä¸ªéšæœºå¼ é‡ a å’Œ b
a = torch.randn(2, 3, device=device, dtype=dtype)  # åˆ›å»ºä¸€ä¸ª 2x3 çš„éšæœºå¼ é‡
b = torch.randn(2, 3, device=device, dtype=dtype)  # åˆ›å»ºå¦ä¸€ä¸ª 2x3 çš„éšæœºå¼ é‡

print("å¼ é‡ a:")
print(a)

print("å¼ é‡ b:")
print(b)

# é€å…ƒç´ ç›¸ä¹˜å¹¶è¾“å‡ºç»“æœ
print("a å’Œ b çš„é€å…ƒç´ ä¹˜ç§¯:")
print(a * b)

# è¾“å‡ºå¼ é‡ a æ‰€æœ‰å…ƒç´ çš„æ€»å’Œ
print("å¼ é‡ a æ‰€æœ‰å…ƒç´ çš„æ€»å’Œ:")
print(a.sum())

# è¾“å‡ºå¼ é‡ a ä¸­ç¬¬ 2 è¡Œç¬¬ 3 åˆ—çš„å…ƒç´ ï¼ˆæ³¨æ„ç´¢å¼•ä» 0 å¼€å§‹ï¼‰
print("å¼ é‡ a ç¬¬ 2 è¡Œç¬¬ 3 åˆ—çš„å…ƒç´ :")
print(a[1, 2])

# è¾“å‡ºå¼ é‡ a ä¸­çš„æœ€å¤§å€¼
print("å¼ é‡ a ä¸­çš„æœ€å¤§å€¼:")
print(a.max())

"""
å¼ é‡ a:
tensor([[ 0.8182,  0.4718,  0.8318],
        [ 0.8146,  0.1508, -1.7619]])
å¼ é‡ b:
tensor([[-1.7713,  0.8521,  0.4598],
        [ 0.3082,  0.0558,  0.1254]])
a å’Œ b çš„é€å…ƒç´ ä¹˜ç§¯:
tensor([[-1.4493,  0.4020,  0.3824],
        [ 0.2510,  0.0084, -0.2209]])
å¼ é‡ a æ‰€æœ‰å…ƒç´ çš„æ€»å’Œ:
tensor(1.3252)
å¼ é‡ a ç¬¬ 2 è¡Œç¬¬ 3 åˆ—çš„å…ƒç´ :
tensor(-1.7619)
å¼ é‡ a ä¸­çš„æœ€å¤§å€¼:
tensor(0.8318)
"""
```



## 2.PyTorch ç®€ä»‹

PyTorch æ˜¯ä¸€ä¸ªå¼€æºçš„ Python æœºå™¨å­¦ä¹ åº“ï¼ŒåŸºäº Torch åº“ï¼Œåº•å±‚ç”± C++ å®ç°ï¼Œåº”ç”¨äºäººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¦‚è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚

PyTorch æœ€åˆç”± Meta Platforms çš„äººå·¥æ™ºèƒ½ç ”ç©¶å›¢é˜Ÿå¼€å‘ï¼Œç°åœ¨å± äºLinux åŸºé‡‘ä¼šçš„ä¸€éƒ¨åˆ†ã€‚

è®¸å¤šæ·±åº¦å­¦ä¹ è½¯ä»¶éƒ½æ˜¯åŸºäº PyTorch æ„å»ºçš„ï¼ŒåŒ…æ‹¬ç‰¹æ–¯æ‹‰è‡ªåŠ¨é©¾é©¶ã€Uber çš„ Pyroã€Hugging Face çš„ Transformersã€ PyTorch Lightning å’Œ Catalystã€‚

**PyTorch ä¸»è¦æœ‰ä¸¤å¤§ç‰¹å¾ï¼š**

- ç±»ä¼¼äº NumPy çš„å¼ é‡è®¡ç®—ï¼Œèƒ½åœ¨ GPU æˆ– MPS ç­‰ç¡¬ä»¶åŠ é€Ÿå™¨ä¸ŠåŠ é€Ÿã€‚
- åŸºäºå¸¦è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿçš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚

PyTorch åŒ…æ‹¬ torch.autogradã€torch.nnã€torch.optim ç­‰å­æ¨¡å—ã€‚

PyTorch åŒ…å«å¤šç§æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬ MSEï¼ˆå‡æ–¹è¯¯å·® = L2 èŒƒæ•°ï¼‰ã€äº¤å‰ç†µæŸå¤±å’Œè´Ÿç†µä¼¼ç„¶æŸå¤±ï¼ˆå¯¹åˆ†ç±»å™¨æœ‰ç”¨ï¼‰ç­‰ã€‚

### PyTorch ç‰¹æ€§

- **åŠ¨æ€è®¡ç®—å›¾ï¼ˆDynamic Computation Graphsï¼‰**ï¼š PyTorch çš„è®¡ç®—å›¾æ˜¯åŠ¨æ€çš„ï¼Œè¿™æ„å‘³ç€å®ƒä»¬åœ¨è¿è¡Œæ—¶æ„å»ºï¼Œå¹¶ä¸”å¯ä»¥éšæ—¶æ”¹å˜ã€‚è¿™ä¸ºå®éªŒå’Œè°ƒè¯•æä¾›äº†æå¤§çš„çµæ´»æ€§ï¼Œå› ä¸ºå¼€å‘è€…å¯ä»¥é€è¡Œæ‰§è¡Œä»£ç ï¼ŒæŸ¥çœ‹ä¸­é—´ç»“æœã€‚
- **è‡ªåŠ¨å¾®åˆ†ï¼ˆAutomatic Differentiationï¼‰**ï¼š PyTorch çš„è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿå…è®¸å¼€å‘è€…è½»æ¾åœ°è®¡ç®—æ¢¯åº¦ï¼Œè¿™å¯¹äºè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹è‡³å…³é‡è¦ã€‚å®ƒé€šè¿‡åå‘ä¼ æ’­ç®—æ³•è‡ªåŠ¨è®¡ç®—å‡ºæŸå¤±å‡½æ•°å¯¹æ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚
- **å¼ é‡è®¡ç®—ï¼ˆTensor Computationï¼‰**ï¼š PyTorch æä¾›äº†ç±»ä¼¼äº NumPy çš„å¼ é‡æ“ä½œï¼Œè¿™äº›æ“ä½œå¯ä»¥åœ¨ CPU å’Œ GPU ä¸Šæ‰§è¡Œï¼Œä»è€ŒåŠ é€Ÿè®¡ç®—è¿‡ç¨‹ã€‚å¼ é‡æ˜¯ PyTorch ä¸­çš„åŸºæœ¬æ•°æ®ç»“æ„ï¼Œç”¨äºå­˜å‚¨å’Œæ“ä½œæ•°æ®ã€‚
- **ä¸°å¯Œçš„ API**ï¼š PyTorch æä¾›äº†å¤§é‡çš„é¢„å®šä¹‰å±‚ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç®—æ³•ï¼Œè¿™äº›éƒ½æ˜¯æ„å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¸¸ç”¨ç»„ä»¶ã€‚
- **å¤šè¯­è¨€æ”¯æŒ**ï¼š PyTorch è™½ç„¶ä»¥ Python ä¸ºä¸»è¦æ¥å£ï¼Œä½†ä¹Ÿæä¾›äº† C++ æ¥å£ï¼Œå…è®¸æ›´åº•å±‚çš„é›†æˆå’Œæ§åˆ¶ã€‚

#### åŠ¨æ€è®¡ç®—å›¾ï¼ˆDynamic Computation Graphï¼‰

PyTorch æœ€æ˜¾è‘—çš„ç‰¹ç‚¹ä¹‹ä¸€æ˜¯å…¶åŠ¨æ€è®¡ç®—å›¾çš„æœºåˆ¶ã€‚

ä¸ TensorFlow çš„é™æ€è®¡ç®—å›¾ï¼ˆgraphï¼‰ä¸åŒï¼ŒPyTorch åœ¨æ‰§è¡Œæ—¶æ„å»ºè®¡ç®—å›¾ï¼Œè¿™æ„å‘³ç€åœ¨æ¯æ¬¡è®¡ç®—æ—¶ï¼Œå›¾éƒ½ä¼šæ ¹æ®è¾“å…¥æ•°æ®çš„å½¢çŠ¶è‡ªåŠ¨å˜åŒ–ã€‚

**åŠ¨æ€è®¡ç®—å›¾çš„ä¼˜ç‚¹ï¼š**

- æ›´åŠ çµæ´»ï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦æ¡ä»¶åˆ¤æ–­æˆ–é€’å½’çš„åœºæ™¯ã€‚
- æ–¹ä¾¿è°ƒè¯•å’Œä¿®æ”¹ï¼Œèƒ½å¤Ÿç›´æ¥æŸ¥çœ‹ä¸­é—´ç»“æœã€‚
- æ›´æ¥è¿‘ Python ç¼–ç¨‹çš„é£æ ¼ï¼Œæ˜“äºä¸Šæ‰‹ã€‚

#### å¼ é‡ï¼ˆTensorï¼‰ä¸è‡ªåŠ¨æ±‚å¯¼ï¼ˆAutogradï¼‰

PyTorch ä¸­çš„æ ¸å¿ƒæ•°æ®ç»“æ„æ˜¯ **å¼ é‡ï¼ˆTensorï¼‰**ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¤šç»´çŸ©é˜µï¼Œå¯ä»¥åœ¨ CPU æˆ– GPU ä¸Šé«˜æ•ˆåœ°è¿›è¡Œè®¡ç®—ã€‚å¼ é‡çš„æ“ä½œæ”¯æŒè‡ªåŠ¨æ±‚å¯¼ï¼ˆAutogradï¼‰æœºåˆ¶ï¼Œä½¿å¾—åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼Œè¿™å¯¹äºæ·±åº¦å­¦ä¹ ä¸­çš„æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•è‡³å…³é‡è¦ã€‚

**å¼ é‡ï¼ˆTensorï¼‰ï¼š**

- æ”¯æŒåœ¨ CPU å’Œ GPU ä¹‹é—´è¿›è¡Œåˆ‡æ¢ã€‚
- æä¾›äº†ç±»ä¼¼ NumPy çš„æ¥å£ï¼Œæ”¯æŒå…ƒç´ çº§è¿ç®—ã€‚
- æ”¯æŒè‡ªåŠ¨æ±‚å¯¼ï¼Œå¯ä»¥æ–¹ä¾¿åœ°è¿›è¡Œæ¢¯åº¦è®¡ç®—ã€‚

**è‡ªåŠ¨æ±‚å¯¼ï¼ˆAutogradï¼‰ï¼š**

- PyTorch å†…ç½®çš„è‡ªåŠ¨æ±‚å¯¼å¼•æ“ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¿½è¸ªæ‰€æœ‰å¼ é‡çš„æ“ä½œï¼Œå¹¶åœ¨åå‘ä¼ æ’­æ—¶è®¡ç®—æ¢¯åº¦ã€‚
- é€šè¿‡ `requires_grad` å±æ€§ï¼Œå¯ä»¥æŒ‡å®šå¼ é‡éœ€è¦è®¡ç®—æ¢¯åº¦ã€‚
- æ”¯æŒé«˜æ•ˆçš„åå‘ä¼ æ’­ï¼Œé€‚ç”¨äºç¥ç»ç½‘ç»œçš„è®­ç»ƒã€‚

#### æ¨¡å‹å®šä¹‰ä¸è®­ç»ƒ

PyTorch æä¾›äº† `torch.nn` æ¨¡å—ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡ç»§æ‰¿ `nn.Module` ç±»æ¥å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚ä½¿ç”¨ `forward` å‡½æ•°æŒ‡å®šå‰å‘ä¼ æ’­ï¼Œè‡ªåŠ¨åå‘ä¼ æ’­ï¼ˆé€šè¿‡ `autograd`ï¼‰å’Œæ¢¯åº¦è®¡ç®—ä¹Ÿç”± PyTorch å†…éƒ¨å¤„ç†ã€‚

**ç¥ç»ç½‘ç»œæ¨¡å—ï¼ˆtorch.nnï¼‰ï¼š**

- æä¾›äº†å¸¸ç”¨çš„å±‚ï¼ˆå¦‚çº¿æ€§å±‚ã€å·ç§¯å±‚ã€æ± åŒ–å±‚ç­‰ï¼‰ã€‚
- æ”¯æŒå®šä¹‰å¤æ‚çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼ˆåŒ…æ‹¬å¤šè¾“å…¥ã€å¤šè¾“å‡ºçš„ç½‘ç»œï¼‰ã€‚
- å…¼å®¹ä¸ä¼˜åŒ–å™¨ï¼ˆå¦‚ `torch.optim`ï¼‰ä¸€èµ·ä½¿ç”¨ã€‚

#### GPU åŠ é€Ÿ

PyTorch å®Œå…¨æ”¯æŒåœ¨ GPU ä¸Šè¿è¡Œï¼Œä»¥åŠ é€Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒã€‚é€šè¿‡ç®€å•çš„ `.to(device)` æ–¹æ³•ï¼Œç”¨æˆ·å¯ä»¥å°†æ¨¡å‹å’Œå¼ é‡è½¬ç§»åˆ° GPU ä¸Šè¿›è¡Œè®¡ç®—ã€‚PyTorch æ”¯æŒå¤š GPU è®­ç»ƒï¼Œèƒ½å¤Ÿåˆ©ç”¨ NVIDIA CUDA æŠ€æœ¯æ˜¾è‘—æé«˜è®¡ç®—æ•ˆç‡ã€‚

**GPU æ”¯æŒï¼š**

- è‡ªåŠ¨é€‰æ‹© GPU æˆ– CPUã€‚
- æ”¯æŒé€šè¿‡ CUDA åŠ é€Ÿè¿ç®—ã€‚
- æ”¯æŒå¤š GPU å¹¶è¡Œè®¡ç®—ï¼ˆ`DataParallel` æˆ– `torch.distributed`ï¼‰ã€‚

#### ç”Ÿæ€ç³»ç»Ÿä¸ç¤¾åŒºæ”¯æŒ

PyTorch ä½œä¸ºä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæ‹¥æœ‰ä¸€ä¸ªåºå¤§çš„ç¤¾åŒºå’Œç”Ÿæ€ç³»ç»Ÿã€‚å®ƒä¸ä»…åœ¨å­¦æœ¯ç•Œå¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œä¹Ÿåœ¨å·¥ä¸šç•Œï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸä¸­å¾—åˆ°äº†å¹¿æ³›éƒ¨ç½²ã€‚PyTorch è¿˜æä¾›äº†è®¸å¤šä¸æ·±åº¦å­¦ä¹ ç›¸å…³çš„å·¥å…·å’Œåº“ï¼Œå¦‚ï¼š

- **torchvision**ï¼šç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„æ•°æ®é›†å’Œæ¨¡å‹ã€‚
- **torchtext**ï¼šç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ•°æ®é›†å’Œé¢„å¤„ç†å·¥å…·ã€‚
- **torchaudio**ï¼šç”¨äºéŸ³é¢‘å¤„ç†çš„å·¥å…·åŒ…ã€‚
- **PyTorch Lightning**ï¼šä¸€ç§ç®€åŒ– PyTorch ä»£ç çš„é«˜å±‚åº“ï¼Œä¸“æ³¨äºç ”ç©¶å’Œå®éªŒçš„å¿«é€Ÿè¿­ä»£ã€‚

------

### ä¸å…¶ä»–æ¡†æ¶çš„å¯¹æ¯”

PyTorch ç”±äºå…¶çµæ´»æ€§ã€æ˜“ç”¨æ€§å’Œç¤¾åŒºæ”¯æŒï¼Œå·²ç»æˆä¸ºå¾ˆå¤šæ·±åº¦å­¦ä¹ ç ”ç©¶è€…å’Œå¼€å‘è€…çš„é¦–é€‰æ¡†æ¶ã€‚

#### TensorFlow vs PyTorch

- PyTorch çš„åŠ¨æ€è®¡ç®—å›¾ä½¿å¾—å®ƒæ›´åŠ çµæ´»ï¼Œé€‚åˆå¿«é€Ÿå®éªŒå’Œç ”ç©¶ï¼›è€Œ TensorFlow çš„é™æ€è®¡ç®—å›¾åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ›´å…·ä¼˜åŒ–ç©ºé—´ã€‚
- PyTorch åœ¨è°ƒè¯•æ—¶æ›´åŠ æ–¹ä¾¿ï¼ŒTensorFlow åˆ™åœ¨éƒ¨ç½²ä¸Šæ›´åŠ æˆç†Ÿï¼Œæ”¯æŒæ›´å¹¿æ³›çš„ç¡¬ä»¶å’Œå¹³å°ã€‚
- è¿‘å¹´æ¥ï¼ŒTensorFlow ä¹Ÿå¼•å…¥äº†åŠ¨æ€å›¾ï¼ˆå¦‚ TensorFlow 2.xï¼‰ï¼Œä½¿å¾—ä¸¤è€…åœ¨åŠŸèƒ½ä¸Šè¶‹äºæ¥è¿‘ã€‚
- å…¶ä»–æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¦‚ Kerasã€Caffe ç­‰ä¹Ÿæœ‰ä¸€å®šåº”ç”¨ï¼Œä½† PyTorch ç”±äºå…¶çµæ´»æ€§ã€æ˜“ç”¨æ€§å’Œç¤¾åŒºæ”¯æŒï¼Œå·²ç»æˆä¸ºå¾ˆå¤šæ·±åº¦å­¦ä¹ ç ”ç©¶è€…å’Œå¼€å‘è€…çš„é¦–é€‰æ¡†æ¶ã€‚

| ç‰¹æ€§               | **TensorFlow**                                          | **PyTorch**                                                  |
| :----------------- | :------------------------------------------------------ | :----------------------------------------------------------- |
| **å¼€å‘å…¬å¸**       | Google                                                  | Facebook (FAIR)                                              |
| **è®¡ç®—å›¾ç±»å‹**     | é™æ€è®¡ç®—å›¾ï¼ˆå®šä¹‰åå†æ‰§è¡Œï¼‰                              | åŠ¨æ€è®¡ç®—å›¾ï¼ˆå®šä¹‰å³æ‰§è¡Œï¼‰                                     |
| **çµæ´»æ€§**         | ä½ï¼ˆè®¡ç®—å›¾åœ¨ç¼–è¯‘æ—¶æ„å»ºï¼Œä¸æ˜“ä¿®æ”¹ï¼‰                      | é«˜ï¼ˆè®¡ç®—å›¾åœ¨æ‰§è¡Œæ—¶åŠ¨æ€åˆ›å»ºï¼Œæ˜“äºä¿®æ”¹å’Œè°ƒè¯•ï¼‰                 |
| **è°ƒè¯•**           | è¾ƒéš¾ï¼ˆéœ€è¦ä½¿ç”¨ `tf.debugging` æˆ–å¤–éƒ¨å·¥å…·è°ƒè¯•ï¼‰          | å®¹æ˜“ï¼ˆå¯ä»¥ç›´æ¥åœ¨ Python ä¸­è¿›è¡Œè°ƒè¯•ï¼‰                         |
| **æ˜“ç”¨æ€§**         | ä½ï¼ˆè¾ƒå¤æ‚ï¼ŒAPI è¾ƒå¤šï¼Œå­¦ä¹ æ›²çº¿è¾ƒé™¡å³­ï¼‰                  | é«˜ï¼ˆAPI ç®€æ´ï¼Œè¯­æ³•æ›´åŠ æ¥è¿‘ Pythonï¼Œå®¹æ˜“ä¸Šæ‰‹ï¼‰                |
| **éƒ¨ç½²**           | å¼ºï¼ˆæ”¯æŒå¹¿æ³›çš„ç¡¬ä»¶ï¼Œå¦‚ TensorFlow Liteã€TensorFlow.jsï¼‰ | è¾ƒå¼±ï¼ˆéƒ¨ç½²å·¥å…·å’Œå¹³å°ç›¸å¯¹è¾ƒå°‘ï¼Œè™½ç„¶æœ‰ TensorFlow æ”¯æŒï¼‰       |
| **ç¤¾åŒºæ”¯æŒ**       | å¾ˆå¼ºï¼ˆæˆç†Ÿä¸”åºå¤§çš„ç¤¾åŒºï¼Œå¹¿æ³›çš„æ•™ç¨‹å’Œæ–‡æ¡£ï¼‰              | å¾ˆå¼ºï¼ˆç¤¾åŒºæ´»è·ƒï¼Œç‰¹åˆ«æ˜¯åœ¨å­¦æœ¯ç•Œï¼Œå¿«é€Ÿå‘å±•çš„ç”Ÿæ€ï¼‰             |
| **æ¨¡å‹è®­ç»ƒ**       | æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒï¼Œæ”¯æŒå¤šç§è®¾å¤‡ï¼ˆå¦‚ CPUã€GPUã€TPUï¼‰        | æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒï¼Œæ”¯æŒå¤š GPUã€CPU å’Œ TPU                       |
| **API å±‚çº§**       | é«˜çº§APIï¼šKerasï¼›ä½çº§APIï¼šTensorFlow Core                | é«˜çº§APIï¼šTorchVisionã€TorchText ç­‰ï¼›ä½çº§APIï¼šTorch           |
| **æ€§èƒ½**           | é«˜ï¼ˆä¼˜åŒ–æ–¹é¢æˆç†Ÿï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒï¼‰                        | é«˜ï¼ˆé€‚åˆç ”ç©¶å’ŒåŸå‹å¼€å‘ï¼Œç”Ÿäº§æ€§èƒ½ä¹Ÿåœ¨æå‡ï¼‰                   |
| **è‡ªåŠ¨æ±‚å¯¼**       | é€šè¿‡ `tf.GradientTape` å®ç°åŠ¨æ€æ±‚å¯¼ï¼ˆè¾ƒå¤æ‚ï¼‰           | é€šè¿‡ `autograd` åŠ¨æ€æ±‚å¯¼ï¼ˆæ›´ç®€æ´å’Œç›´è§‚ï¼‰                     |
| **è°ƒä¼˜ä¸å¯æ‰©å±•æ€§** | å¼ºï¼ˆæ”¯æŒåœ¨å¤šå¹³å°ä¸Šè¿è¡Œï¼Œå¦‚ TensorFlow Serving ç­‰ï¼‰      | è¾ƒå¼±ï¼ˆè™½ç„¶åœ¨å­¦æœ¯å’Œå®éªŒç¯å¢ƒä¸­è¡¨ç°ä¼˜è¶Šï¼Œä½†ç”Ÿäº§ç¯å¢ƒæ”¯æŒç›¸å¯¹è¾ƒå°‘ï¼‰ |
| **æ¡†æ¶çµæ´»æ€§**     | è¾ƒä½ï¼ˆTensorFlow 2.x å¼•å…¥äº†åŠ¨æ€å›¾ç‰¹æ€§ï¼Œä½†ä»ä¸å®Œå…¨çµæ´»ï¼‰ | é«˜ï¼ˆåŠ¨æ€å›¾å¸¦æ¥æ›´é«˜çš„çµæ´»æ€§ï¼‰                                 |
| **æ”¯æŒå¤šç§è¯­è¨€**   | æ”¯æŒå¤šç§è¯­è¨€ï¼ˆPython, C++, Java, JavaScript, etc.ï¼‰     | ä¸»è¦æ”¯æŒ Pythonï¼ˆä½†ä¹Ÿæœ‰ C++ APIï¼‰                            |
| **å…¼å®¹æ€§ä¸è¿ç§»**   | TensorFlow 2.x ä¸æ—§ç‰ˆæœ¬å…¼å®¹æ€§è¾ƒå¥½                       | ä¸ TensorFlow å…¼å®¹æ€§å·®ï¼Œè¿ç§»è¾ƒéš¾                             |

#### PyTorch vs NumPy

| ç‰¹æ€§         | PyTorch            | NumPy            |
| :----------- | :----------------- | :--------------- |
| **ç›®æ ‡**     | æ·±åº¦å­¦ä¹ ä¸“ç”¨       | é€šç”¨ç§‘å­¦è®¡ç®—     |
| **GPU æ”¯æŒ** | åŸç”Ÿæ”¯æŒ CUDA      | ä¸ç›´æ¥æ”¯æŒ       |
| **è‡ªåŠ¨å¾®åˆ†** | å†…ç½®è‡ªåŠ¨æ±‚å¯¼       | éœ€è¦æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦ |
| **ç¥ç»ç½‘ç»œ** | ä¸°å¯Œçš„ç¥ç»ç½‘ç»œæ¨¡å— | éœ€è¦ä»é›¶å®ç°     |
| **å­¦ä¹ æˆæœ¬** | ç›¸å¯¹è¾ƒé«˜           | ç›¸å¯¹è¾ƒä½         |

------

### PyTorch çš„å†å²ä¸å‘å±•

PyTorch çš„å‰èº«æ˜¯ Torchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº Lua è¯­è¨€çš„ç§‘å­¦è®¡ç®—æ¡†æ¶ã€‚éšç€ Python åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸçš„å…´èµ·ï¼ŒFacebook å›¢é˜Ÿå†³å®šå°† Torch çš„æ ¸å¿ƒæ€æƒ³ç§»æ¤åˆ° Python ä¸Šï¼Œä»è€Œè¯ç”Ÿäº† PyTorchã€‚

- **2016å¹´**ï¼šFacebook å‘å¸ƒ PyTorch 0.1 ç‰ˆæœ¬
- **2017å¹´**ï¼šPyTorch 0.2 å¼•å…¥åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
- **2018å¹´**ï¼šPyTorch 1.0 å‘å¸ƒï¼Œå¢åŠ äº†ç”Ÿäº§éƒ¨ç½²èƒ½åŠ›
- **2019å¹´**ï¼šPyTorch 1.3 å¼•å…¥ç§»åŠ¨ç«¯æ”¯æŒ
- **2020å¹´**ï¼šPyTorch 1.6 å¢åŠ äº†è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ
- **2021å¹´**ï¼šPyTorch 1.9 å¼•å…¥ TorchScript å’Œ C++ å‰ç«¯
- **2022å¹´**ï¼šPyTorch 1.12 ä¼˜åŒ–äº†æ€§èƒ½å’Œç¨³å®šæ€§
- **2023å¹´**ï¼šPyTorch 2.0 å‘å¸ƒï¼Œå¼•å…¥ç¼–è¯‘æ¨¡å¼å¤§å¹…æå‡æ€§èƒ½



## 3.PyTorch åŸºç¡€

PyTorch æ˜¯ä¸€ä¸ªå¼€æºçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä»¥å…¶çµæ´»æ€§å’ŒåŠ¨æ€è®¡ç®—å›¾è€Œå¹¿å—æ¬¢è¿ã€‚

PyTorch ä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªåŸºç¡€æ¦‚å¿µï¼šå¼ é‡ï¼ˆTensorï¼‰ã€è‡ªåŠ¨æ±‚å¯¼ï¼ˆAutogradï¼‰ã€ç¥ç»ç½‘ç»œæ¨¡å—ï¼ˆnn.Moduleï¼‰ã€ä¼˜åŒ–å™¨ï¼ˆoptimï¼‰ç­‰ã€‚

- **å¼ é‡ï¼ˆTensorï¼‰**ï¼šPyTorch çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œæ”¯æŒå¤šç»´æ•°ç»„ï¼Œå¹¶å¯ä»¥åœ¨ CPU æˆ– GPU ä¸Šè¿›è¡ŒåŠ é€Ÿè®¡ç®—ã€‚
- **è‡ªåŠ¨æ±‚å¯¼ï¼ˆAutogradï¼‰**ï¼šPyTorch æä¾›äº†è‡ªåŠ¨æ±‚å¯¼åŠŸèƒ½ï¼Œå¯ä»¥è½»æ¾è®¡ç®—æ¨¡å‹çš„æ¢¯åº¦ï¼Œä¾¿äºè¿›è¡Œåå‘ä¼ æ’­å’Œä¼˜åŒ–ã€‚
- **ç¥ç»ç½‘ç»œï¼ˆnn.Moduleï¼‰**ï¼šPyTorch æä¾›äº†ç®€å•ä¸”å¼ºå¤§çš„ API æ¥æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå¯ä»¥æ–¹ä¾¿åœ°è¿›è¡Œå‰å‘ä¼ æ’­å’Œæ¨¡å‹å®šä¹‰ã€‚
- **ä¼˜åŒ–å™¨ï¼ˆOptimizersï¼‰**ï¼šä½¿ç”¨ä¼˜åŒ–å™¨ï¼ˆå¦‚ Adamã€SGD ç­‰ï¼‰æ¥æ›´æ–°æ¨¡å‹çš„å‚æ•°ï¼Œä½¿å¾—æŸå¤±æœ€å°åŒ–ã€‚
- **è®¾å¤‡ï¼ˆDeviceï¼‰**ï¼šå¯ä»¥å°†æ¨¡å‹å’Œå¼ é‡ç§»åŠ¨åˆ° GPU ä¸Šä»¥åŠ é€Ÿè®¡ç®—ã€‚

------

### PyTorch æ¶æ„æ€»è§ˆ

PyTorch é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œç”±å¤šä¸ªç›¸äº’åä½œçš„æ ¸å¿ƒç»„ä»¶æ„æˆã€‚ç†è§£è¿™äº›ç»„ä»¶çš„ä½œç”¨å’Œç›¸äº’å…³ç³»ï¼Œæ˜¯æŒæ¡ PyTorch çš„å…³é”®ã€‚

**PyTorch æ¶æ„å›¾**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PyTorch ç”Ÿæ€ç³»ç»Ÿ                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  torchvision  â”‚  torchtext  â”‚  torchaudio  â”‚  å…¶ä»–ä¸“ä¸šåº“     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     PyTorch æ ¸å¿ƒ                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   torch.nn    â”‚   torch.optim   â”‚      torch.utils          â”‚
â”‚   (ç¥ç»ç½‘ç»œ)   â”‚   (ä¼˜åŒ–å™¨)      â”‚      (å·¥å…·å‡½æ•°)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚               â”‚                 â”‚   torch.utils.data        â”‚
â”‚  torch æ ¸å¿ƒ   â”‚  autograd       â”‚   (æ•°æ®åŠ è½½)              â”‚
â”‚  (å¼ é‡è®¡ç®—)   â”‚  (è‡ªåŠ¨å¾®åˆ†)     â”‚                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

PyTorch é‡‡ç”¨**åˆ†å±‚æ¶æ„**è®¾è®¡ï¼Œä»ä¸Šå±‚åˆ°åº•å±‚ä¾æ¬¡ä¸ºï¼š 

**1ã€Python APIï¼ˆé¡¶å±‚ï¼‰**

- `torch`ï¼šæ ¸å¿ƒå¼ é‡è®¡ç®—ï¼ˆç±»ä¼¼NumPyï¼Œæ”¯æŒGPUï¼‰ã€‚ 
- `torch.nn`ï¼šç¥ç»ç½‘ç»œå±‚ã€æŸå¤±å‡½æ•°ç­‰ã€‚ 
- `torch.autograd`ï¼šè‡ªåŠ¨å¾®åˆ†ï¼ˆåå‘ä¼ æ’­ï¼‰ã€‚ 
- å¼€å‘è€…ç›´æ¥è°ƒç”¨çš„æ¥å£ï¼Œç®€å•æ˜“ç”¨ã€‚ 

**2ã€C++æ ¸å¿ƒï¼ˆä¸­å±‚ï¼‰**

- **ATen**ï¼šå¼ é‡è¿ç®—æ ¸å¿ƒåº“ï¼ˆ400+æ“ä½œï¼‰ã€‚ 
- **JIT**ï¼šå³æ—¶ç¼–è¯‘ä¼˜åŒ–æ¨¡å‹ã€‚ 
- **Autogradå¼•æ“**ï¼šè‡ªåŠ¨å¾®åˆ†çš„åº•å±‚å®ç°ã€‚ 
- é«˜æ€§èƒ½è®¡ç®—ï¼Œè¿æ¥Pythonä¸åº•å±‚ç¡¬ä»¶ã€‚ 

**3ã€åŸºç¡€åº“ï¼ˆåº•å±‚ï¼‰**

- **TH/THNN**ï¼šCè¯­è¨€å®ç°çš„åŸºç¡€å¼ é‡å’Œç¥ç»ç½‘ç»œæ“ä½œã€‚ 
- **THC/THCUNN**ï¼šå¯¹åº”çš„CUDAï¼ˆGPUï¼‰ç‰ˆæœ¬ã€‚ 
- ç›´æ¥æ“ä½œç¡¬ä»¶ï¼ˆCPU/GPUï¼‰ï¼Œæè‡´ä¼˜åŒ–é€Ÿåº¦ã€‚ 

**æ‰§è¡Œæµç¨‹**ï¼š
Pythonä»£ç  â†’ C++æ ¸å¿ƒè®¡ç®— â†’ åº•å±‚CUDA/Cåº“åŠ é€Ÿ â†’ è¿”å›ç»“æœã€‚
æ—¢ä¿æŒæ˜“ç”¨æ€§ï¼Œåˆç¡®ä¿é«˜æ€§èƒ½ã€‚

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/iGWbOXL.png" alt="img" style="zoom:67%;" />

**å¼ é‡ï¼ˆTensorï¼‰**

å¼ é‡ï¼ˆTensorï¼‰æ˜¯ PyTorch ä¸­çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œç”¨äºå­˜å‚¨å’Œæ“ä½œå¤šç»´æ•°ç»„ã€‚

å¼ é‡å¯ä»¥è§†ä¸ºä¸€ä¸ªå¤šç»´æ•°ç»„ï¼Œæ”¯æŒåŠ é€Ÿè®¡ç®—çš„æ“ä½œã€‚

åœ¨ PyTorch ä¸­ï¼Œå¼ é‡çš„æ¦‚å¿µç±»ä¼¼äº NumPy ä¸­çš„æ•°ç»„ï¼Œä½†æ˜¯ PyTorch çš„å¼ é‡å¯ä»¥è¿è¡Œåœ¨ä¸åŒçš„è®¾å¤‡ä¸Šï¼Œæ¯”å¦‚ CPU å’Œ GPUï¼Œè¿™ä½¿å¾—å®ƒä»¬éå¸¸é€‚åˆäºè¿›è¡Œå¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—ï¼Œç‰¹åˆ«æ˜¯åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸã€‚

- **ç»´åº¦ï¼ˆDimensionalityï¼‰**ï¼šå¼ é‡çš„ç»´åº¦æŒ‡çš„æ˜¯æ•°æ®çš„å¤šç»´æ•°ç»„ç»“æ„ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªæ ‡é‡ï¼ˆ0ç»´å¼ é‡ï¼‰æ˜¯ä¸€ä¸ªå•ç‹¬çš„æ•°å­—ï¼Œä¸€ä¸ªå‘é‡ï¼ˆ1ç»´å¼ é‡ï¼‰æ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼Œä¸€ä¸ªçŸ©é˜µï¼ˆ2ç»´å¼ é‡ï¼‰æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œä»¥æ­¤ç±»æ¨ã€‚
- **å½¢çŠ¶ï¼ˆShapeï¼‰**ï¼šå¼ é‡çš„å½¢çŠ¶æ˜¯æŒ‡æ¯ä¸ªç»´åº¦ä¸Šçš„å¤§å°ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå½¢çŠ¶ä¸º`(3, 4)`çš„å¼ é‡æ„å‘³ç€å®ƒæœ‰3è¡Œ4åˆ—ã€‚
- **æ•°æ®ç±»å‹ï¼ˆDtypeï¼‰**ï¼šå¼ é‡ä¸­çš„æ•°æ®ç±»å‹å®šä¹‰äº†å­˜å‚¨æ¯ä¸ªå…ƒç´ æ‰€éœ€çš„å†…å­˜å¤§å°å’Œè§£é‡Šæ–¹å¼ã€‚PyTorchæ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼ŒåŒ…æ‹¬æ•´æ•°å‹ï¼ˆå¦‚`torch.int8`ã€`torch.int32`ï¼‰ã€æµ®ç‚¹å‹ï¼ˆå¦‚`torch.float32`ã€`torch.float64`ï¼‰å’Œå¸ƒå°”å‹ï¼ˆ`torch.bool`ï¼‰ã€‚

**å¼ é‡åˆ›å»ºï¼š**

**å®ä¾‹**

```python
import torch

# åˆ›å»ºä¸€ä¸ª 2x3 çš„å…¨ 0 å¼ é‡
a = torch.zeros(2, 3)
print(a)

# åˆ›å»ºä¸€ä¸ª 2x3 çš„å…¨ 1 å¼ é‡
b = torch.ones(2, 3)
print(b)

# åˆ›å»ºä¸€ä¸ª 2x3 çš„éšæœºæ•°å¼ é‡
c = torch.randn(2, 3)
print(c)

# ä» NumPy æ•°ç»„åˆ›å»ºå¼ é‡
import numpy as np
numpy_array = np.array([[1, 2], [3, 4]])
tensor_from_numpy = torch.from_numpy(numpy_array)
print(tensor_from_numpy)

# åœ¨æŒ‡å®šè®¾å¤‡ï¼ˆCPU/GPUï¼‰ä¸Šåˆ›å»ºå¼ é‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
d = torch.randn(2, 3, device=device)
print(d)
```

è¾“å‡ºç»“æœç±»ä¼¼å¦‚ä¸‹ï¼š

```
tensor([[0., 0., 0.],
        [0., 0., 0.]])
tensor([[1., 1., 1.],
        [1., 1., 1.]])
tensor([[ 1.0189, -0.5718, -1.2814],
        [-0.5865,  1.0855,  1.1727]])
tensor([[1, 2],
        [3, 4]])
tensor([[-0.3360,  0.2203,  1.3463],
        [-0.5982, -0.2704,  0.5429]])
```

**å¸¸ç”¨å¼ é‡æ“ä½œï¼š**

**å®ä¾‹**

```py
# å¼ é‡ç›¸åŠ 
e = torch.randn(2, 3)
f = torch.randn(2, 3)
print(e + f)

# é€å…ƒç´ ä¹˜æ³•
print(e * f)

# å¼ é‡çš„è½¬ç½®
g = torch.randn(3, 2)
print(g.t()) # æˆ–è€… g.transpose(0, 1)

# å¼ é‡çš„å½¢çŠ¶
print(g.shape) # è¿”å›å½¢çŠ¶
```



**å¼ é‡ä¸è®¾å¤‡**

PyTorch å¼ é‡å¯ä»¥å­˜åœ¨äºä¸åŒçš„è®¾å¤‡ä¸Šï¼ŒåŒ…æ‹¬CPUå’ŒGPUï¼Œä½ å¯ä»¥å°†å¼ é‡ç§»åŠ¨åˆ° GPU ä¸Šä»¥åŠ é€Ÿè®¡ç®—ï¼š

```
if torch.cuda.is_available():
    tensor_gpu = tensor_from_list.to('cuda')  # å°†å¼ é‡ç§»åŠ¨åˆ°GPU
```

**æ¢¯åº¦å’Œè‡ªåŠ¨å¾®åˆ†**

PyTorchçš„å¼ é‡æ”¯æŒè‡ªåŠ¨å¾®åˆ†ï¼Œè¿™æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„å…³é”®ç‰¹æ€§ã€‚å½“ä½ åˆ›å»ºä¸€ä¸ªéœ€è¦æ¢¯åº¦çš„å¼ é‡æ—¶ï¼ŒPyTorchå¯ä»¥è‡ªåŠ¨è®¡ç®—å…¶æ¢¯åº¦ï¼š

**å®ä¾‹**

```python
# åˆ›å»ºä¸€ä¸ªéœ€è¦æ¢¯åº¦çš„å¼ é‡
tensor_requires_grad = torch.tensor([1.0], requires_grad=True)

# è¿›è¡Œä¸€äº›æ“ä½œ
tensor_result = tensor_requires_grad * 2

# è®¡ç®—æ¢¯åº¦
tensor_result.backward()
print(tensor_requires_grad.grad) # è¾“å‡ºæ¢¯åº¦
```



**å†…å­˜å’Œæ€§èƒ½**

PyTorch å¼ é‡è¿˜æä¾›äº†ä¸€äº›å†…å­˜ç®¡ç†åŠŸèƒ½ï¼Œæ¯”å¦‚.clone()ã€.detach() å’Œ .to() æ–¹æ³•ï¼Œå®ƒä»¬å¯ä»¥å¸®åŠ©ä½ ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œæé«˜æ€§èƒ½ã€‚

------

### è‡ªåŠ¨æ±‚å¯¼ï¼ˆAutogradï¼‰

è‡ªåŠ¨æ±‚å¯¼ï¼ˆAutomatic Differentiationï¼Œç®€ç§°Autogradï¼‰æ˜¯æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒç‰¹æ€§ï¼Œå®ƒå…è®¸è®¡ç®—æœºè‡ªåŠ¨è®¡ç®—æ•°å­¦å‡½æ•°çš„å¯¼æ•°ã€‚

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œè‡ªåŠ¨æ±‚å¯¼ä¸»è¦ç”¨äºä¸¤ä¸ªæ–¹é¢ï¼š**ä¸€æ˜¯åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶è®¡ç®—æ¢¯åº¦**ï¼Œ**äºŒæ˜¯è¿›è¡Œåå‘ä¼ æ’­ç®—æ³•çš„å®ç°**ã€‚

è‡ªåŠ¨æ±‚å¯¼åŸºäºé“¾å¼æ³•åˆ™ï¼ˆChain Ruleï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè®¡ç®—å¤æ‚å‡½æ•°å¯¼æ•°çš„æ•°å­¦æ³•åˆ™ã€‚é“¾å¼æ³•åˆ™è¡¨æ˜ï¼Œå¤åˆå‡½æ•°çš„å¯¼æ•°æ˜¯å…¶å„ä¸ªç»„æˆéƒ¨åˆ†å¯¼æ•°çš„ä¹˜ç§¯ã€‚åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ¨¡å‹é€šå¸¸æ˜¯ç”±è®¸å¤šå±‚ç»„æˆçš„å¤æ‚å‡½æ•°ï¼Œè‡ªåŠ¨æ±‚å¯¼èƒ½å¤Ÿé«˜æ•ˆåœ°è®¡ç®—è¿™äº›å±‚çš„æ¢¯åº¦ã€‚

**åŠ¨æ€å›¾ä¸é™æ€å›¾ï¼š**

- **åŠ¨æ€å›¾ï¼ˆDynamic Graphï¼‰**ï¼šåœ¨åŠ¨æ€å›¾ä¸­ï¼Œè®¡ç®—å›¾åœ¨è¿è¡Œæ—¶åŠ¨æ€æ„å»ºã€‚æ¯æ¬¡æ‰§è¡Œæ“ä½œæ—¶ï¼Œè®¡ç®—å›¾éƒ½ä¼šæ›´æ–°ï¼Œè¿™ä½¿å¾—è°ƒè¯•å’Œä¿®æ”¹æ¨¡å‹å˜å¾—æ›´åŠ å®¹æ˜“ã€‚PyTorchä½¿ç”¨çš„æ˜¯åŠ¨æ€å›¾ã€‚
- **é™æ€å›¾ï¼ˆStatic Graphï¼‰**ï¼šåœ¨é™æ€å›¾ä¸­ï¼Œè®¡ç®—å›¾åœ¨å¼€å§‹æ‰§è¡Œä¹‹å‰æ„å»ºå®Œæˆï¼Œå¹¶ä¸”ä¸ä¼šæ”¹å˜ã€‚TensorFlowæœ€åˆä½¿ç”¨çš„æ˜¯é™æ€å›¾ï¼Œä½†åæ¥ä¹Ÿæ”¯æŒåŠ¨æ€å›¾ã€‚

PyTorch æä¾›äº†è‡ªåŠ¨æ±‚å¯¼åŠŸèƒ½ï¼Œé€šè¿‡ autograd æ¨¡å—æ¥è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ã€‚

torch.Tensor å¯¹è±¡æœ‰ä¸€ä¸ª requires_grad å±æ€§ï¼Œç”¨äºæŒ‡ç¤ºæ˜¯å¦éœ€è¦è®¡ç®—è¯¥å¼ é‡çš„æ¢¯åº¦ã€‚

å½“ä½ åˆ›å»ºä¸€ä¸ª requires_grad=True çš„å¼ é‡æ—¶ï¼ŒPyTorch ä¼šè‡ªåŠ¨è·Ÿè¸ªæ‰€æœ‰å¯¹å®ƒçš„æ“ä½œï¼Œä»¥ä¾¿åœ¨ä¹‹åè®¡ç®—æ¢¯åº¦ã€‚

åˆ›å»ºéœ€è¦æ¢¯åº¦çš„å¼ é‡:

**å®ä¾‹**

```python
# åˆ›å»ºä¸€ä¸ªéœ€è¦è®¡ç®—æ¢¯åº¦çš„å¼ é‡
x = torch.randn(2, 2, requires_grad=True)
print(x)

# æ‰§è¡ŒæŸäº›æ“ä½œ
y = x + 2
z = y * y * 3
out = z.mean()

print(out)


```

è¾“å‡ºç»“æœç±»ä¼¼å¦‚ä¸‹ï¼š

```
tensor([[0., 0., 0.],
        [0., 0., 0.]])
tensor([[1., 1., 1.],
        [1., 1., 1.]])
tensor([[ 1.0189, -0.5718, -1.2814],
        [-0.5865,  1.0855,  1.1727]])
tensor([[1, 2],
        [3, 4]])
tensor([[-0.3360,  0.2203,  1.3463],
        [-0.5982, -0.2704,  0.5429]])
tianqixin@Mac-mini runoob-test % python3 test.py
tensor([[-0.1908,  0.2811],
        [ 0.8068,  0.8002]], requires_grad=True)
tensor(18.1469, grad_fn=<MeanBackward0>)
```

### åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰

ä¸€æ—¦å®šä¹‰äº†è®¡ç®—å›¾ï¼Œå¯ä»¥é€šè¿‡ **.backward()** æ–¹æ³•æ¥è®¡ç®—æ¢¯åº¦ã€‚

**å®ä¾‹**

```python
# åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦
out.backward()

# æŸ¥çœ‹ x çš„æ¢¯åº¦
print(x.grad)
```

åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¸­ï¼Œè‡ªåŠ¨æ±‚å¯¼ä¸»è¦ç”¨äºå®ç°åå‘ä¼ æ’­ç®—æ³•ã€‚

åå‘ä¼ æ’­æ˜¯ä¸€ç§é€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°å…³äºç½‘ç»œå‚æ•°çš„æ¢¯åº¦æ¥è®­ç»ƒç¥ç»ç½‘ç»œçš„æ–¹æ³•ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œç½‘ç»œçš„å‰å‘ä¼ æ’­ä¼šè®¡ç®—è¾“å‡ºå’ŒæŸå¤±ï¼Œç„¶ååå‘ä¼ æ’­ä¼šè®¡ç®—æŸå¤±å…³äºæ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¢¯åº¦æ¥æ›´æ–°å‚æ•°ã€‚

**åœæ­¢æ¢¯åº¦è®¡ç®—**

å¦‚æœä½ ä¸å¸Œæœ›æŸäº›å¼ é‡çš„æ¢¯åº¦è¢«è®¡ç®—ï¼ˆä¾‹å¦‚ï¼Œå½“ä½ ä¸éœ€è¦åå‘ä¼ æ’­æ—¶ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ **torch.no_grad()** æˆ–è®¾ç½® **requires_grad=False**ã€‚

**å®ä¾‹**

```python
# ä½¿ç”¨ torch.no_grad() ç¦ç”¨æ¢¯åº¦è®¡ç®—
with torch.no_grad():
  y = x * 2
```



------

### ç¥ç»ç½‘ç»œï¼ˆnn.Moduleï¼‰

ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§æ¨¡ä»¿äººè„‘ç¥ç»å…ƒè¿æ¥çš„è®¡ç®—æ¨¡å‹ï¼Œç”±å¤šå±‚èŠ‚ç‚¹ï¼ˆç¥ç»å…ƒï¼‰ç»„æˆï¼Œç”¨äºå­¦ä¹ æ•°æ®ä¹‹é—´çš„å¤æ‚æ¨¡å¼å’Œå…³ç³»ã€‚

ç¥ç»ç½‘ç»œé€šè¿‡è°ƒæ•´ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥æƒé‡æ¥ä¼˜åŒ–é¢„æµ‹ç»“æœï¼Œè¿™ä¸€è¿‡ç¨‹æ¶‰åŠå‰å‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°ã€‚

ç¥ç»ç½‘ç»œçš„ç±»å‹åŒ…æ‹¬å‰é¦ˆç¥ç»ç½‘ç»œã€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ï¼Œå®ƒä»¬åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³å¤„ç†ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚

PyTorch æä¾›äº†ä¸€ä¸ªéå¸¸æ–¹ä¾¿çš„æ¥å£æ¥æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå³ **torch.nn.Module**ã€‚

æˆ‘ä»¬å¯ä»¥ç»§æ‰¿ nn.Module ç±»å¹¶å®šä¹‰è‡ªå·±çš„ç½‘ç»œå±‚ã€‚

åˆ›å»ºä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œï¼š

**å®ä¾‹**

```python
import torch.nn as nn
import torch.optim as optim

# å®šä¹‰ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œ
class SimpleNN(nn.Module):
  def init(self):
    super(SimpleNN, self).__init__()
    self.fc1 = nn.Linear(2, 2) # è¾“å…¥å±‚åˆ°éšè—å±‚
    self.fc2 = nn.Linear(2, 1) # éšè—å±‚åˆ°è¾“å‡ºå±‚

  def forward(self, x):
    x = torch.relu(self.fc1(x)) # ReLU æ¿€æ´»å‡½æ•°
    x = self.fc2(x)
    return x

# åˆ›å»ºç½‘ç»œå®ä¾‹
model = SimpleNN()

# æ‰“å°æ¨¡å‹ç»“æ„
print(model)

```

è¾“å‡ºç»“æœï¼š

```
SimpleNN(
  (fc1): Linear(in_features=2, out_features=2, bias=True)
  (fc2): Linear(in_features=2, out_features=1, bias=True)
)
```

**è®­ç»ƒè¿‡ç¨‹ï¼š**

1. **å‰å‘ä¼ æ’­ï¼ˆForward Propagationï¼‰**ï¼š åœ¨å‰å‘ä¼ æ’­é˜¶æ®µï¼Œè¾“å…¥æ•°æ®é€šè¿‡ç½‘ç»œå±‚ä¼ é€’ï¼Œæ¯å±‚åº”ç”¨æƒé‡å’Œæ¿€æ´»å‡½æ•°ï¼Œç›´åˆ°äº§ç”Ÿè¾“å‡ºã€‚
2. **è®¡ç®—æŸå¤±ï¼ˆCalculate Lossï¼‰**ï¼š æ ¹æ®ç½‘ç»œçš„è¾“å‡ºå’ŒçœŸå®æ ‡ç­¾ï¼Œè®¡ç®—æŸå¤±å‡½æ•°çš„å€¼ã€‚
3. **åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰**ï¼š åå‘ä¼ æ’­åˆ©ç”¨è‡ªåŠ¨æ±‚å¯¼æŠ€æœ¯è®¡ç®—æŸå¤±å‡½æ•°å…³äºæ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚
4. **å‚æ•°æ›´æ–°ï¼ˆParameter Updateï¼‰**ï¼š ä½¿ç”¨ä¼˜åŒ–å™¨æ ¹æ®æ¢¯åº¦æ›´æ–°ç½‘ç»œçš„æƒé‡å’Œåç½®ã€‚
5. **è¿­ä»£ï¼ˆIterationï¼‰**ï¼š é‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼Œç›´åˆ°æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„æ€§èƒ½è¾¾åˆ°æ»¡æ„çš„æ°´å¹³ã€‚

### å‰å‘ä¼ æ’­ä¸æŸå¤±è®¡ç®—

**å®ä¾‹**

```python
# éšæœºè¾“å…¥
x = torch.randn(1, 2)

# å‰å‘ä¼ æ’­
output = model(x)
print(output)

# å®šä¹‰æŸå¤±å‡½æ•°ï¼ˆä¾‹å¦‚å‡æ–¹è¯¯å·® MSEï¼‰
criterion = nn.MSELoss()

# å‡è®¾ç›®æ ‡å€¼ä¸º 1
target = torch.randn(1, 1)

# è®¡ç®—æŸå¤±
loss = criterion(output, target)
print(loss)
```



### ä¼˜åŒ–å™¨ï¼ˆOptimizersï¼‰

ä¼˜åŒ–å™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°ç¥ç»ç½‘ç»œçš„å‚æ•°ï¼Œä»¥å‡å°‘æŸå¤±å‡½æ•°çš„å€¼ã€‚

PyTorch æä¾›äº†å¤šç§ä¼˜åŒ–å™¨ï¼Œä¾‹å¦‚ SGDã€Adam ç­‰ã€‚

ä½¿ç”¨ä¼˜åŒ–å™¨è¿›è¡Œå‚æ•°æ›´æ–°ï¼š

**å®ä¾‹**

```python
# å®šä¹‰ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨ Adam ä¼˜åŒ–å™¨ï¼‰
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒæ­¥éª¤
optimizer.zero_grad() # æ¸…ç©ºæ¢¯åº¦
loss.backward() # åå‘ä¼ æ’­
optimizer.step() # æ›´æ–°å‚æ•°
```



------

### è®­ç»ƒæ¨¡å‹

è®­ç»ƒæ¨¡å‹æ˜¯æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä¸­çš„æ ¸å¿ƒè¿‡ç¨‹ï¼Œæ—¨åœ¨é€šè¿‡å¤§é‡æ•°æ®å­¦ä¹ æ¨¡å‹å‚æ•°ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿå¯¹æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®åšå‡ºå‡†ç¡®çš„é¢„æµ‹ã€‚

è®­ç»ƒæ¨¡å‹é€šå¸¸åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1. **æ•°æ®å‡†å¤‡**ï¼š
   - æ”¶é›†å’Œå¤„ç†æ•°æ®ï¼ŒåŒ…æ‹¬æ¸…æ´—ã€æ ‡å‡†åŒ–å’Œå½’ä¸€åŒ–ã€‚
   - å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚
2. **å®šä¹‰æ¨¡å‹**ï¼š
   - é€‰æ‹©æ¨¡å‹æ¶æ„ï¼Œä¾‹å¦‚å†³ç­–æ ‘ã€ç¥ç»ç½‘ç»œç­‰ã€‚
   - åˆå§‹åŒ–æ¨¡å‹å‚æ•°ï¼ˆæƒé‡å’Œåç½®ï¼‰ã€‚
3. **é€‰æ‹©æŸå¤±å‡½æ•°**ï¼š
   - æ ¹æ®ä»»åŠ¡ç±»å‹ï¼ˆå¦‚åˆ†ç±»ã€å›å½’ï¼‰é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°ã€‚
4. **é€‰æ‹©ä¼˜åŒ–å™¨**ï¼š
   - é€‰æ‹©ä¸€ä¸ªä¼˜åŒ–ç®—æ³•ï¼Œå¦‚SGDã€Adamç­‰ï¼Œæ¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
5. **å‰å‘ä¼ æ’­**ï¼š
   - åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå°†è¾“å…¥æ•°æ®é€šè¿‡æ¨¡å‹ä¼ é€’ï¼Œè®¡ç®—é¢„æµ‹è¾“å‡ºã€‚
6. **è®¡ç®—æŸå¤±**ï¼š
   - ä½¿ç”¨æŸå¤±å‡½æ•°è¯„ä¼°é¢„æµ‹è¾“å‡ºä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ã€‚
7. **åå‘ä¼ æ’­**ï¼š
   - åˆ©ç”¨è‡ªåŠ¨æ±‚å¯¼è®¡ç®—æŸå¤±ç›¸å¯¹äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚
8. **å‚æ•°æ›´æ–°**ï¼š
   - æ ¹æ®è®¡ç®—å‡ºçš„æ¢¯åº¦å’Œä¼˜åŒ–å™¨çš„ç­–ç•¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
9. **è¿­ä»£ä¼˜åŒ–**ï¼š
   - é‡å¤æ­¥éª¤5-8ï¼Œç›´åˆ°æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½ä¸å†æå‡æˆ–è¾¾åˆ°é¢„å®šçš„è¿­ä»£æ¬¡æ•°ã€‚
10. **è¯„ä¼°å’Œæµ‹è¯•**ï¼š
    - ä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°æ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½ï¼Œç¡®ä¿æ¨¡å‹æ²¡æœ‰è¿‡æ‹Ÿåˆã€‚
11. **æ¨¡å‹è°ƒä¼˜**ï¼š
    - æ ¹æ®æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°è¿›è¡Œè°ƒå‚ï¼Œå¦‚æ”¹å˜å­¦ä¹ ç‡ã€å¢åŠ æ­£åˆ™åŒ–ç­‰ã€‚
12. **éƒ¨ç½²æ¨¡å‹**ï¼š
    - å°†è®­ç»ƒå¥½çš„æ¨¡å‹éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œç”¨äºå®é™…çš„é¢„æµ‹ä»»åŠ¡ã€‚

**å®ä¾‹**

```py
import torch
import torch.nn as nn
import torch.optim as optim

# 1. å®šä¹‰ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œæ¨¡å‹
class SimpleNN(nn.Module):
  def init(self):
    super(SimpleNN, self).init()
    self.fc1 = nn.Linear(2, 2) # è¾“å…¥å±‚åˆ°éšè—å±‚
    self.fc2 = nn.Linear(2, 1) # éšè—å±‚åˆ°è¾“å‡ºå±‚

  def forward(self, x):
    x = torch.relu(self.fc1(x)) # ReLU æ¿€æ´»å‡½æ•°
    x = self.fc2(x)
    return x

# 2. åˆ›å»ºæ¨¡å‹å®ä¾‹
model = SimpleNN()

# 3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.MSELoss() # å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°
optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam ä¼˜åŒ–å™¨

# 4. å‡è®¾æˆ‘ä»¬æœ‰è®­ç»ƒæ•°æ® X å’Œ Y
X = torch.randn(10, 2) # 10 ä¸ªæ ·æœ¬ï¼Œ2 ä¸ªç‰¹å¾
Y = torch.randn(10, 1) # 10 ä¸ªç›®æ ‡å€¼

# 5. è®­ç»ƒå¾ªç¯
for epoch in range(100):  # è®­ç»ƒ 100 è½®
  optimizer.zero_grad() # æ¸…ç©ºä¹‹å‰çš„æ¢¯åº¦
  output = model(X) # å‰å‘ä¼ æ’­
  loss = criterion(output, Y) # è®¡ç®—æŸå¤±
  loss.backward() # åå‘ä¼ æ’­
  optimizer.step() # æ›´æ–°å‚æ•°

  # æ¯ 10 è½®è¾“å‡ºä¸€æ¬¡æŸå¤±
  if (epoch+1) % 10 == 0:
    print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```
Epoch [10/100], Loss: 1.7180
Epoch [20/100], Loss: 1.6352
Epoch [30/100], Loss: 1.5590
Epoch [40/100], Loss: 1.4896
Epoch [50/100], Loss: 1.4268
Epoch [60/100], Loss: 1.3704
Epoch [70/100], Loss: 1.3198
Epoch [80/100], Loss: 1.2747
Epoch [90/100], Loss: 1.2346
Epoch [100/100], Loss: 1.1991
```

åœ¨æ¯ 10 è½®ï¼Œç¨‹åºä¼šè¾“å‡ºå½“å‰çš„æŸå¤±å€¼ï¼Œå¸®åŠ©æˆ‘ä»¬è·Ÿè¸ªæ¨¡å‹çš„è®­ç»ƒè¿›åº¦ã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼ŒæŸå¤±å€¼åº”è¯¥ä¼šé€æ¸é™ä½ï¼Œè¡¨ç¤ºæ¨¡å‹åœ¨ä¸æ–­å­¦ä¹ å¹¶ä¼˜åŒ–å…¶å‚æ•°ã€‚

è®­ç»ƒæ¨¡å‹æ˜¯ä¸€ä¸ªè¿­ä»£çš„è¿‡ç¨‹ï¼Œéœ€è¦ä¸æ–­åœ°è°ƒæ•´å’Œä¼˜åŒ–ï¼Œç›´åˆ°è¾¾åˆ°æ»¡æ„çš„æ€§èƒ½ã€‚è¿™ä¸ªè¿‡ç¨‹æ¶‰åŠåˆ°å¤§é‡çš„å®éªŒå’Œè°ƒä¼˜ï¼Œç›®çš„æ˜¯ä½¿æ¨¡å‹åœ¨æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ä¸Šä¹Ÿèƒ½æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

------

### è®¾å¤‡ï¼ˆDeviceï¼‰

PyTorch å…è®¸ä½ å°†æ¨¡å‹å’Œæ•°æ®ç§»åŠ¨åˆ° GPU ä¸Šè¿›è¡ŒåŠ é€Ÿã€‚

ä½¿ç”¨ **torch.device** æ¥æŒ‡å®šè®¡ç®—è®¾å¤‡ã€‚

å°†æ¨¡å‹å’Œæ•°æ®ç§»è‡³ GPU:

**å®ä¾‹**

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
model.to(device)

# å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
X = X.to(device)
Y = Y.to(device)
```

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ‰€æœ‰å¼ é‡å’Œæ¨¡å‹éƒ½åº”è¯¥ç§»åˆ°åŒä¸€ä¸ªè®¾å¤‡ä¸Šï¼ˆè¦ä¹ˆéƒ½åœ¨ CPU ä¸Šï¼Œè¦ä¹ˆéƒ½åœ¨ GPU ä¸Šï¼‰ã€‚



## 4.PyTorch å¼ é‡ï¼ˆTensorï¼‰

å¼ é‡æ˜¯ä¸€ä¸ªå¤šç»´æ•°ç»„ï¼Œå¯ä»¥æ˜¯æ ‡é‡ã€å‘é‡ã€çŸ©é˜µæˆ–æ›´é«˜ç»´åº¦çš„æ•°æ®ç»“æ„ã€‚

åœ¨ PyTorch ä¸­ï¼Œå¼ é‡ï¼ˆTensorï¼‰æ˜¯æ•°æ®çš„æ ¸å¿ƒè¡¨ç¤ºå½¢å¼ï¼Œç±»ä¼¼äº NumPy çš„å¤šç»´æ•°ç»„ï¼Œä½†å…·æœ‰æ›´å¼ºå¤§çš„åŠŸèƒ½ï¼Œä¾‹å¦‚æ”¯æŒ GPU åŠ é€Ÿå’Œè‡ªåŠ¨æ¢¯åº¦è®¡ç®—ã€‚

å¼ é‡æ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼ˆæ•´å‹ã€æµ®ç‚¹å‹ã€å¸ƒå°”å‹ç­‰ï¼‰ã€‚

å¼ é‡å¯ä»¥å­˜å‚¨åœ¨ CPU æˆ– GPU ä¸­ï¼ŒGPU å¼ é‡å¯æ˜¾è‘—åŠ é€Ÿè®¡ç®—ã€‚

ä¸‹å›¾å±•ç¤ºäº†ä¸åŒç»´åº¦çš„å¼ é‡ï¼ˆTensorï¼‰åœ¨ PyTorch ä¸­çš„è¡¨ç¤ºæ–¹æ³•ï¼š

![img](https://raw.githubusercontent.com/GMyhf/img/main/img/1__D5ZvufDS38WkhK9rK32hQ.jpg)

**è¯´æ˜ï¼š**

- **1D Tensor / Vectorï¼ˆä¸€ç»´å¼ é‡/å‘é‡ï¼‰:** æœ€åŸºæœ¬çš„å¼ é‡å½¢å¼ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå›¾ä¸­çš„ä¾‹å­æ˜¯ä¸€ä¸ªåŒ…å« 10 ä¸ªå…ƒç´ çš„å‘é‡ã€‚
- **2D Tensor / Matrixï¼ˆäºŒç»´å¼ é‡/çŸ©é˜µï¼‰:** äºŒç»´æ•°ç»„ï¼Œé€šå¸¸ç”¨äºè¡¨ç¤ºçŸ©é˜µï¼Œå›¾ä¸­çš„ä¾‹å­æ˜¯ä¸€ä¸ª 4x5 çš„çŸ©é˜µï¼ŒåŒ…å«äº† 20 ä¸ªå…ƒç´ ã€‚
- **3D Tensor / Cubeï¼ˆä¸‰ç»´å¼ é‡/ç«‹æ–¹ä½“ï¼‰:** ä¸‰ç»´æ•°ç»„ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ç”±å¤šä¸ªçŸ©é˜µå †å è€Œæˆçš„ç«‹æ–¹ä½“ï¼Œå›¾ä¸­çš„ä¾‹å­å±•ç¤ºäº†ä¸€ä¸ª 3x4x5 çš„ç«‹æ–¹ä½“ï¼Œå…¶ä¸­æ¯ä¸ª 5x5 çš„çŸ©é˜µä»£è¡¨ç«‹æ–¹ä½“çš„ä¸€ä¸ª"å±‚"ã€‚
- **4D Tensor / Vector of Cubesï¼ˆå››ç»´å¼ é‡/ç«‹æ–¹ä½“å‘é‡ï¼‰:** å››ç»´æ•°ç»„ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ç”±å¤šä¸ªç«‹æ–¹ä½“ç»„æˆçš„å‘é‡ï¼Œå›¾ä¸­çš„ä¾‹å­æ²¡æœ‰å…·ä½“æ•°å€¼ï¼Œä½†å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªåŒ…å«å¤šä¸ª 3D å¼ é‡çš„é›†åˆã€‚
- **5D Tensor / Matrix of Cubesï¼ˆäº”ç»´å¼ é‡/ç«‹æ–¹ä½“çŸ©é˜µï¼‰:** äº”ç»´æ•°ç»„ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ç”±å¤šä¸ª4Då¼ é‡ç»„æˆçš„çŸ©é˜µï¼Œå›¾ä¸­çš„ä¾‹å­åŒæ ·æ²¡æœ‰å…·ä½“æ•°å€¼ï¼Œä½†å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªåŒ…å«å¤šä¸ª 4D å¼ é‡çš„é›†åˆã€‚

------

**åˆ›å»ºå¼ é‡**

å¼ é‡åˆ›å»ºçš„æ–¹å¼æœ‰ï¼š

| **æ–¹æ³•**                            | **è¯´æ˜**                                               | **ç¤ºä¾‹ä»£ç **                                |
| :---------------------------------- | :----------------------------------------------------- | :------------------------------------------ |
| `torch.tensor(data)`                | ä» Python åˆ—è¡¨æˆ– NumPy æ•°ç»„åˆ›å»ºå¼ é‡ã€‚                  | `x = torch.tensor([[1, 2], [3, 4]])`        |
| `torch.zeros(size)`                 | åˆ›å»ºä¸€ä¸ªå…¨ä¸ºé›¶çš„å¼ é‡ã€‚                                 | `x = torch.zeros((2, 3))`                   |
| `torch.ones(size)`                  | åˆ›å»ºä¸€ä¸ªå…¨ä¸º 1 çš„å¼ é‡ã€‚                                | `x = torch.ones((2, 3))`                    |
| `torch.empty(size)`                 | åˆ›å»ºä¸€ä¸ªæœªåˆå§‹åŒ–çš„å¼ é‡ã€‚                               | `x = torch.empty((2, 3))`                   |
| `torch.rand(size)`                  | åˆ›å»ºä¸€ä¸ªæœä»å‡åŒ€åˆ†å¸ƒçš„éšæœºå¼ é‡ï¼Œå€¼åœ¨ `[0, 1)`ã€‚        | `x = torch.rand((2, 3))`                    |
| `torch.randn(size)`                 | åˆ›å»ºä¸€ä¸ªæœä»æ­£æ€åˆ†å¸ƒçš„éšæœºå¼ é‡ï¼Œå‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1ã€‚ | `x = torch.randn((2, 3))`                   |
| `torch.arange(start, end, step)`    | åˆ›å»ºä¸€ä¸ªä¸€ç»´åºåˆ—å¼ é‡ï¼Œç±»ä¼¼äº Python çš„ `range`ã€‚       | `x = torch.arange(0, 10, 2)`                |
| `torch.linspace(start, end, steps)` | åˆ›å»ºä¸€ä¸ªåœ¨æŒ‡å®šèŒƒå›´å†…ç­‰é—´éš”çš„åºåˆ—å¼ é‡ã€‚                 | `x = torch.linspace(0, 1, 5)`               |
| `torch.eye(size)`                   | åˆ›å»ºä¸€ä¸ªå•ä½çŸ©é˜µï¼ˆå¯¹è§’çº¿ä¸º 1ï¼Œå…¶ä»–ä¸º 0ï¼‰ã€‚             | `x = torch.eye(3)`                          |
| `torch.from_numpy(ndarray)`         | å°† NumPy æ•°ç»„è½¬æ¢ä¸ºå¼ é‡ã€‚                              | `x = torch.from_numpy(np.array([1, 2, 3]))` |

ä½¿ç”¨ **torch.tensor()** å‡½æ•°ï¼Œä½ å¯ä»¥å°†ä¸€ä¸ªåˆ—è¡¨æˆ–æ•°ç»„è½¬æ¢ä¸ºå¼ é‡ï¼š

**å®ä¾‹**

```python
import torch

tensor = torch.tensor([1, 2, 3])
print(tensor)
```

è¾“å‡ºå¦‚ä¸‹ï¼š

```
tensor([1, 2, 3])
```

å¦‚æœä½ æœ‰ä¸€ä¸ª NumPy æ•°ç»„ï¼Œå¯ä»¥ä½¿ç”¨ torch.from_numpy() å°†å…¶è½¬æ¢ä¸ºå¼ é‡ï¼š

å®ä¾‹

```python
import numpy as np

np_array = np.array([1, 2, 3])
tensor = torch.from_numpy(np_array)
print(tensor)
```

è¾“å‡ºå¦‚ä¸‹ï¼š

```
tensor([1, 2, 3])
```

åˆ›å»º 2D å¼ é‡ï¼ˆçŸ©é˜µï¼‰ï¼š

**å®ä¾‹**

```python
import torch

tensor_2d = torch.tensor([
  [-9, 4, 2, 5, 7],
  [3, 0, 12, 8, 6],
  [1, 23, -6, 45, 2],
  [22, 3, -1, 72, 6]
])
print("2D Tensor (Matrix):\n", tensor_2d)
print("Shape:", tensor_2d.shape) # å½¢çŠ¶
```

è¾“å‡ºå¦‚ä¸‹ï¼š

```
2D Tensor (Matrix):
 tensor([[-9,  4,  2,  5,  7],
        [ 3,  0, 12,  8,  6],
        [ 1, 23, -6, 45,  2],
        [22,  3, -1, 72,  6]])
Shape: torch.Size([4, 5])
```

å…¶ä»–ç»´åº¦çš„åˆ›å»ºï¼š

```python
# åˆ›å»º 3D å¼ é‡ï¼ˆç«‹æ–¹ä½“ï¼‰
tensor_3d = torch.stack([tensor_2d, tensor_2d + 10, tensor_2d - 5])  # å †å  3 ä¸ª 2D å¼ é‡
print("3D Tensor (Cube):\n", tensor_3d)
print("Shape:", tensor_3d.shape)  # å½¢çŠ¶

# åˆ›å»º 4D å¼ é‡ï¼ˆå‘é‡çš„ç«‹æ–¹ä½“ï¼‰
tensor_4d = torch.stack([tensor_3d, tensor_3d + 100])  # å †å  2 ä¸ª 3D å¼ é‡
print("4D Tensor (Vector of Cubes):\n", tensor_4d)
print("Shape:", tensor_4d.shape)  # å½¢çŠ¶

# åˆ›å»º 5D å¼ é‡ï¼ˆçŸ©é˜µçš„ç«‹æ–¹ä½“ï¼‰
tensor_5d = torch.stack([tensor_4d, tensor_4d + 1000])  # å †å  2 ä¸ª 4D å¼ é‡
print("5D Tensor (Matrix of Cubes):\n", tensor_5d)
print("Shape:", tensor_5d.shape)  # å½¢çŠ¶
```

------

**å¼ é‡çš„å±æ€§**

å¼ é‡çš„å±æ€§å¦‚ä¸‹è¡¨ï¼š

| **å±æ€§**           | **è¯´æ˜**                         | **ç¤ºä¾‹**                 |
| :----------------- | :------------------------------- | :----------------------- |
| `.shape`           | è·å–å¼ é‡çš„å½¢çŠ¶                   | `tensor.shape`           |
| `.size()`          | è·å–å¼ é‡çš„å½¢çŠ¶                   | `tensor.size()`          |
| `.dtype`           | è·å–å¼ é‡çš„æ•°æ®ç±»å‹               | `tensor.dtype`           |
| `.device`          | æŸ¥çœ‹å¼ é‡æ‰€åœ¨çš„è®¾å¤‡ (CPU/GPU)     | `tensor.device`          |
| `.dim()`           | è·å–å¼ é‡çš„ç»´åº¦æ•°                 | `tensor.dim()`           |
| `.requires_grad`   | æ˜¯å¦å¯ç”¨æ¢¯åº¦è®¡ç®—                 | `tensor.requires_grad`   |
| `.numel()`         | è·å–å¼ é‡ä¸­çš„å…ƒç´ æ€»æ•°             | `tensor.numel()`         |
| `.is_cuda`         | æ£€æŸ¥å¼ é‡æ˜¯å¦åœ¨ GPU ä¸Š            | `tensor.is_cuda`         |
| `.T`               | è·å–å¼ é‡çš„è½¬ç½®ï¼ˆé€‚ç”¨äº 2D å¼ é‡ï¼‰ | `tensor.T`               |
| `.item()`          | è·å–å•å…ƒç´ å¼ é‡çš„å€¼               | `tensor.item()`          |
| `.is_contiguous()` | æ£€æŸ¥å¼ é‡æ˜¯å¦è¿ç»­å­˜å‚¨             | `tensor.is_contiguous()` |

**å®ä¾‹**

```python
import torch

# åˆ›å»ºä¸€ä¸ª 2D å¼ é‡
tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)

# å¼ é‡çš„å±æ€§
print("Tensor:\n", tensor)
print("Shape:", tensor.shape) # è·å–å½¢çŠ¶
print("Size:", tensor.size()) # è·å–å½¢çŠ¶ï¼ˆå¦ä¸€ç§æ–¹æ³•ï¼‰
print("Data Type:", tensor.dtype) # æ•°æ®ç±»å‹
print("Device:", tensor.device) # è®¾å¤‡
print("Dimensions:", tensor.dim()) # ç»´åº¦æ•°
print("Total Elements:", tensor.numel()) # å…ƒç´ æ€»æ•°
print("Requires Grad:", tensor.requires_grad) # æ˜¯å¦å¯ç”¨æ¢¯åº¦
print("Is CUDA:", tensor.is_cuda) # æ˜¯å¦åœ¨ GPU ä¸Š
print("Is Contiguous:", tensor.is_contiguous()) # æ˜¯å¦è¿ç»­å­˜å‚¨

\# è·å–å•å…ƒç´ å€¼
single_value = torch.tensor(42)
print("Single Element Value:", single_value.item())

\# è½¬ç½®å¼ é‡
tensor_T = tensor.T
print("Transposed Tensor:\n", tensor_T)
```

è¾“å‡ºç»“æœï¼š

```
Tensor:
 tensor([[1., 2., 3.],
         [4., 5., 6.]])
Shape: torch.Size([2, 3])
Size: torch.Size([2, 3])
Data Type: torch.float32
Device: cpu
Dimensions: 2
Total Elements: 6
Requires Grad: False
Is CUDA: False
Is Contiguous: True
Single Element Value: 42
Transposed Tensor:
 tensor([[1., 4.],
         [2., 5.],
         [3., 6.]])
```

------

**å¼ é‡çš„æ“ä½œ**

å¼ é‡æ“ä½œæ–¹æ³•è¯´æ˜å¦‚ä¸‹ã€‚

**åŸºç¡€æ“ä½œï¼š**

| **æ“ä½œ**                | **è¯´æ˜**                       | **ç¤ºä¾‹ä»£ç **                  |
| :---------------------- | :----------------------------- | :---------------------------- |
| `+`, `-`, `*`, `/`      | å…ƒç´ çº§åŠ æ³•ã€å‡æ³•ã€ä¹˜æ³•ã€é™¤æ³•ã€‚ | `z = x + y`                   |
| `torch.matmul(x, y)`    | çŸ©é˜µä¹˜æ³•ã€‚                     | `z = torch.matmul(x, y)`      |
| `torch.dot(x, y)`       | å‘é‡ç‚¹ç§¯ï¼ˆä»…é€‚ç”¨äº 1D å¼ é‡ï¼‰ã€‚ | `z = torch.dot(x, y)`         |
| `torch.sum(x)`          | æ±‚å’Œã€‚                         | `z = torch.sum(x)`            |
| `torch.mean(x)`         | æ±‚å‡å€¼ã€‚                       | `z = torch.mean(x)`           |
| `torch.max(x)`          | æ±‚æœ€å¤§å€¼ã€‚                     | `z = torch.max(x)`            |
| `torch.min(x)`          | æ±‚æœ€å°å€¼ã€‚                     | `z = torch.min(x)`            |
| `torch.argmax(x, dim)`  | è¿”å›æœ€å¤§å€¼çš„ç´¢å¼•ï¼ˆæŒ‡å®šç»´åº¦ï¼‰ã€‚ | `z = torch.argmax(x, dim=1)`  |
| `torch.softmax(x, dim)` | è®¡ç®— softmaxï¼ˆæŒ‡å®šç»´åº¦ï¼‰ã€‚     | `z = torch.softmax(x, dim=1)` |

å½¢çŠ¶æ“ä½œ

| **æ“ä½œ**                 | **è¯´æ˜**                       | **ç¤ºä¾‹ä»£ç **                   |
| :----------------------- | :----------------------------- | :----------------------------- |
| `x.view(shape)`          | æ”¹å˜å¼ é‡çš„å½¢çŠ¶ï¼ˆä¸æ”¹å˜æ•°æ®ï¼‰ã€‚ | `z = x.view(3, 4)`             |
| `x.reshape(shape)`       | ç±»ä¼¼äº `view`ï¼Œä½†æ›´çµæ´»ã€‚      | `z = x.reshape(3, 4)`          |
| `x.t()`                  | è½¬ç½®çŸ©é˜µã€‚                     | `z = x.t()`                    |
| `x.unsqueeze(dim)`       | åœ¨æŒ‡å®šç»´åº¦æ·»åŠ ä¸€ä¸ªç»´åº¦ã€‚       | `z = x.unsqueeze(0)`           |
| `x.squeeze(dim)`         | å»æ‰æŒ‡å®šç»´åº¦ä¸º 1 çš„ç»´åº¦ã€‚      | `z = x.squeeze(0)`             |
| `torch.cat((x, y), dim)` | æŒ‰æŒ‡å®šç»´åº¦è¿æ¥å¤šä¸ªå¼ é‡ã€‚       | `z = torch.cat((x, y), dim=1)` |

**å®ä¾‹**

```python
import torch

# åˆ›å»ºä¸€ä¸ª 2D å¼ é‡
tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)
print("åŸå§‹å¼ é‡:\n", tensor)

# 1. ç´¢å¼•å’Œåˆ‡ç‰‡æ“ä½œ
print("\nã€ç´¢å¼•å’Œåˆ‡ç‰‡ã€‘")
print("è·å–ç¬¬ä¸€è¡Œ:", tensor[0]) # è·å–ç¬¬ä¸€è¡Œ
print("è·å–ç¬¬ä¸€è¡Œç¬¬ä¸€åˆ—çš„å…ƒç´ :", tensor[0, 0]) # è·å–ç‰¹å®šå…ƒç´ 
print("è·å–ç¬¬äºŒåˆ—çš„æ‰€æœ‰å…ƒç´ :", tensor[:, 1]) # è·å–ç¬¬äºŒåˆ—æ‰€æœ‰å…ƒç´ 

# 2. å½¢çŠ¶å˜æ¢æ“ä½œ
print("\nã€å½¢çŠ¶å˜æ¢ã€‘")
reshaped = tensor.view(3, 2) # æ”¹å˜å¼ é‡å½¢çŠ¶ä¸º 3x2
print("æ”¹å˜å½¢çŠ¶åçš„å¼ é‡:\n", reshaped)
flattened = tensor.flatten() # å°†å¼ é‡å±•å¹³æˆä¸€ç»´
print("å±•å¹³åçš„å¼ é‡:\n", flattened)

# 3. æ•°å­¦è¿ç®—æ“ä½œ
print("\nã€æ•°å­¦è¿ç®—ã€‘")
tensor_add = tensor + 10 # å¼ é‡åŠ æ³•
print("å¼ é‡åŠ  10:\n", tensor_add)
tensor_mul = tensor * 2 # å¼ é‡ä¹˜æ³•
print("å¼ é‡ä¹˜ 2:\n", tensor_mul)
tensor_sum = tensor.sum() # è®¡ç®—æ‰€æœ‰å…ƒç´ çš„å’Œ
print("å¼ é‡å…ƒç´ çš„å’Œ:", tensor_sum.item())

# 4. ä¸å…¶ä»–å¼ é‡çš„æ“ä½œ
print("\nã€ä¸å…¶ä»–å¼ é‡æ“ä½œã€‘")
tensor2 = torch.tensor([[1, 1, 1], [1, 1, 1]], dtype=torch.float32)
print("å¦ä¸€ä¸ªå¼ é‡:\n", tensor2)
tensor_dot = torch.matmul(tensor, tensor2.T) # å¼ é‡çŸ©é˜µä¹˜æ³•
print("çŸ©é˜µä¹˜æ³•ç»“æœ:\n", tensor_dot)

# 5. æ¡ä»¶åˆ¤æ–­å’Œç­›é€‰
print("\nã€æ¡ä»¶åˆ¤æ–­å’Œç­›é€‰ã€‘")
mask = tensor > 3 # åˆ›å»ºä¸€ä¸ªå¸ƒå°”æ©ç 
print("å¤§äº 3 çš„å…ƒç´ çš„å¸ƒå°”æ©ç :\n", mask)
filtered_tensor = tensor[tensor > 3] # ç­›é€‰å‡ºç¬¦åˆæ¡ä»¶çš„å…ƒç´ 
print("å¤§äº 3 çš„å…ƒç´ :\n", filtered_tensor)
```

è¾“å‡ºç»“æœï¼š

```
åŸå§‹å¼ é‡:
 tensor([[1., 2., 3.],
         [4., 5., 6.]])

ã€ç´¢å¼•å’Œåˆ‡ç‰‡ã€‘
è·å–ç¬¬ä¸€è¡Œ: tensor([1., 2., 3.])
è·å–ç¬¬ä¸€è¡Œç¬¬ä¸€åˆ—çš„å…ƒç´ : tensor(1.)
è·å–ç¬¬äºŒåˆ—çš„æ‰€æœ‰å…ƒç´ : tensor([2., 5.])

ã€å½¢çŠ¶å˜æ¢ã€‘
æ”¹å˜å½¢çŠ¶åçš„å¼ é‡:
 tensor([[1., 2.],
         [3., 4.],
         [5., 6.]])
å±•å¹³åçš„å¼ é‡:
 tensor([1., 2., 3., 4., 5., 6.])

ã€æ•°å­¦è¿ç®—ã€‘
å¼ é‡åŠ  10:
 tensor([[11., 12., 13.],
         [14., 15., 16.]])
å¼ é‡ä¹˜ 2:
 tensor([[ 2.,  4.,  6.],
         [ 8., 10., 12.]])
å¼ é‡å…ƒç´ çš„å’Œ: 21.0

ã€ä¸å…¶ä»–å¼ é‡æ“ä½œã€‘
å¦ä¸€ä¸ªå¼ é‡:
 tensor([[1., 1., 1.],
         [1., 1., 1.]])
çŸ©é˜µä¹˜æ³•ç»“æœ:
 tensor([[ 6.,  6.],
         [15., 15.]])

ã€æ¡ä»¶åˆ¤æ–­å’Œç­›é€‰ã€‘
å¤§äº 3 çš„å…ƒç´ çš„å¸ƒå°”æ©ç :
 tensor([[False, False, False],
         [ True,  True,  True]])
å¤§äº 3 çš„å…ƒç´ :
 tensor([4., 5., 6.])
```

------

**å¼ é‡çš„ GPU åŠ é€Ÿ**

å°†å¼ é‡è½¬ç§»åˆ° GPUï¼š

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
x = torch.tensor([1.0, 2.0, 3.0], device=device)
```

æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨ï¼š

```python
torch.cuda.is_available()  # è¿”å› True æˆ– False
```

------

**å¼ é‡ä¸ NumPy çš„äº’æ“ä½œ**

å¼ é‡ä¸ NumPy çš„äº’æ“ä½œå¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š

| **æ“ä½œ**                    | **è¯´æ˜**                                   | **ç¤ºä¾‹ä»£ç **                     |
| :-------------------------- | :----------------------------------------- | :------------------------------- |
| `torch.from_numpy(ndarray)` | å°† NumPy æ•°ç»„è½¬æ¢ä¸ºå¼ é‡ã€‚                  | `x = torch.from_numpy(np_array)` |
| `x.numpy()`                 | å°†å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„ï¼ˆä»…é™ CPU å¼ é‡ï¼‰ã€‚ | `np_array = x.numpy()`           |

**å®ä¾‹**

```python
import torch
import numpy as np

# 1. NumPy æ•°ç»„è½¬æ¢ä¸º PyTorch å¼ é‡
print("1. NumPy è½¬ä¸º PyTorch å¼ é‡")
numpy_array = np.array([[1, 2, 3], [4, 5, 6]])
print("NumPy æ•°ç»„:\n", numpy_array)

# ä½¿ç”¨ torch.from_numpy() å°† NumPy æ•°ç»„è½¬æ¢ä¸ºå¼ é‡
tensor_from_numpy = torch.from_numpy(numpy_array)
print("è½¬æ¢åçš„ PyTorch å¼ é‡:\n", tensor_from_numpy)

# ä¿®æ”¹ NumPy æ•°ç»„ï¼Œè§‚å¯Ÿå¼ é‡çš„å˜åŒ–ï¼ˆå…±äº«å†…å­˜ï¼‰
numpy_array[0, 0] = 100
print("ä¿®æ”¹åçš„ NumPy æ•°ç»„:\n", numpy_array)
print("PyTorch å¼ é‡ä¹Ÿä¼šåŒæ­¥å˜åŒ–:\n", tensor_from_numpy)

# 2. PyTorch å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„
print("\n2. PyTorch å¼ é‡è½¬ä¸º NumPy æ•°ç»„")
tensor = torch.tensor([[7, 8, 9], [10, 11, 12]], dtype=torch.float32)
print("PyTorch å¼ é‡:\n", tensor)

# ä½¿ç”¨ tensor.numpy() å°†å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„
numpy_from_tensor = tensor.numpy()
print("è½¬æ¢åçš„ NumPy æ•°ç»„:\n", numpy_from_tensor)

# ä¿®æ”¹å¼ é‡ï¼Œè§‚å¯Ÿ NumPy æ•°ç»„çš„å˜åŒ–ï¼ˆå…±äº«å†…å­˜ï¼‰
tensor[0, 0] = 77
print("ä¿®æ”¹åçš„ PyTorch å¼ é‡:\n", tensor)
print("NumPy æ•°ç»„ä¹Ÿä¼šåŒæ­¥å˜åŒ–:\n", numpy_from_tensor)

# 3. æ³¨æ„ï¼šä¸å…±äº«å†…å­˜çš„æƒ…å†µï¼ˆéœ€è¦å¤åˆ¶æ•°æ®ï¼‰
print("\n3. ä½¿ç”¨ clone() ä¿è¯ç‹¬ç«‹æ•°æ®")
tensor_independent = torch.tensor([[13, 14, 15], [16, 17, 18]], dtype=torch.float32)
numpy_independent = tensor_independent.clone().numpy() # ä½¿ç”¨ clone å¤åˆ¶æ•°æ®
print("åŸå§‹å¼ é‡:\n", tensor_independent)
tensor_independent[0, 0] = 0 # ä¿®æ”¹å¼ é‡æ•°æ®
print("ä¿®æ”¹åçš„å¼ é‡:\n", tensor_independent)
print("NumPy æ•°ç»„ï¼ˆä¸ä¼šåŒæ­¥å˜åŒ–ï¼‰:\n", numpy_independent)
```

è¾“å‡ºç»“æœï¼š

```
1. NumPy è½¬ä¸º PyTorch å¼ é‡
NumPy æ•°ç»„:
 [[1 2 3]
 [4 5 6]]
è½¬æ¢åçš„ PyTorch å¼ é‡:
 tensor([[1, 2, 3],
         [4, 5, 6]])

ä¿®æ”¹åçš„ NumPy æ•°ç»„:
 [[100   2   3]
 [  4   5   6]]
PyTorch å¼ é‡ä¹Ÿä¼šåŒæ­¥å˜åŒ–:
 tensor([[100,   2,   3],
         [  4,   5,   6]])

2. PyTorch å¼ é‡è½¬ä¸º NumPy æ•°ç»„
PyTorch å¼ é‡:
 tensor([[ 7.,  8.,  9.],
         [10., 11., 12.]])
è½¬æ¢åçš„ NumPy æ•°ç»„:
 [[ 7.  8.  9.]
 [10. 11. 12.]]

ä¿®æ”¹åçš„ PyTorch å¼ é‡:
 tensor([[77.,  8.,  9.],
         [10., 11., 12.]])
NumPy æ•°ç»„ä¹Ÿä¼šåŒæ­¥å˜åŒ–:
 [[77.  8.  9.]
 [10. 11. 12.]]

3. ä½¿ç”¨ clone() ä¿è¯ç‹¬ç«‹æ•°æ®
åŸå§‹å¼ é‡:
 tensor([[13., 14., 15.],
         [16., 17., 18.]])
ä¿®æ”¹åçš„å¼ é‡:
 tensor([[ 0., 14., 15.],
         [16., 17., 18.]])
NumPy æ•°ç»„ï¼ˆä¸ä¼šåŒæ­¥å˜åŒ–ï¼‰:
 [[13. 14. 15.]
 [16. 17. 18.]]
```



## 5.PyTorch ç¥ç»ç½‘ç»œåŸºç¡€

ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§æ¨¡ä»¿äººè„‘å¤„ç†ä¿¡æ¯æ–¹å¼çš„è®¡ç®—æ¨¡å‹ï¼Œå®ƒç”±è®¸å¤šç›¸äº’è¿æ¥çš„èŠ‚ç‚¹ï¼ˆç¥ç»å…ƒï¼‰ç»„æˆï¼Œè¿™äº›èŠ‚ç‚¹æŒ‰å±‚æ¬¡æ’åˆ—ã€‚

ç¥ç»ç½‘ç»œçš„å¼ºå¤§ä¹‹å¤„åœ¨äºå…¶èƒ½å¤Ÿè‡ªåŠ¨ä»å¤§é‡æ•°æ®ä¸­å­¦ä¹ å¤æ‚çš„æ¨¡å¼å’Œç‰¹å¾ï¼Œæ— éœ€äººå·¥è®¾è®¡ç‰¹å¾æå–å™¨ã€‚

éšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼Œç¥ç»ç½‘ç»œå·²ç»æˆä¸ºè§£å†³è®¸å¤šå¤æ‚é—®é¢˜çš„å…³é”®æŠ€æœ¯ã€‚

**ç¥ç»å…ƒï¼ˆNeuronï¼‰**

ç¥ç»å…ƒæ˜¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬å•å…ƒï¼Œå®ƒæ¥æ”¶è¾“å…¥ä¿¡å·ï¼Œé€šè¿‡åŠ æƒæ±‚å’Œåä¸åç½®ï¼ˆbiasï¼‰ç›¸åŠ ï¼Œç„¶åé€šè¿‡æ¿€æ´»å‡½æ•°å¤„ç†ä»¥äº§ç”Ÿè¾“å‡ºã€‚

ç¥ç»å…ƒçš„æƒé‡å’Œåç½®æ˜¯ç½‘ç»œå­¦ä¹ è¿‡ç¨‹ä¸­éœ€è¦è°ƒæ•´çš„å‚æ•°ã€‚

**è¾“å…¥å’Œè¾“å‡º:**

- **è¾“å…¥ï¼ˆInputï¼‰**ï¼šè¾“å…¥æ˜¯ç½‘ç»œçš„èµ·å§‹ç‚¹ï¼Œå¯ä»¥æ˜¯ç‰¹å¾æ•°æ®ï¼Œå¦‚å›¾åƒçš„åƒç´ å€¼æˆ–æ–‡æœ¬çš„è¯å‘é‡ã€‚
- **è¾“å‡ºï¼ˆOutputï¼‰**ï¼šè¾“å‡ºæ˜¯ç½‘ç»œçš„ç»ˆç‚¹ï¼Œè¡¨ç¤ºæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œå¦‚åˆ†ç±»ä»»åŠ¡ä¸­çš„ç±»åˆ«æ ‡ç­¾ã€‚

ç¥ç»å…ƒæ¥æ”¶å¤šä¸ªè¾“å…¥ï¼ˆä¾‹å¦‚x1, x2, ..., xnï¼‰ï¼Œå¦‚æœè¾“å…¥çš„åŠ æƒå’Œå¤§äºæ¿€æ´»é˜ˆå€¼ï¼ˆactivation potentialï¼‰ï¼Œåˆ™äº§ç”ŸäºŒè¿›åˆ¶è¾“å‡ºã€‚

![img](https://raw.githubusercontent.com/GMyhf/img/main/img/1_upfpVueoUuKPkyX3PR3KBg.png)

ç¥ç»å…ƒçš„è¾“å‡ºå¯ä»¥çœ‹ä½œæ˜¯è¾“å…¥çš„åŠ æƒå’ŒåŠ ä¸Šåç½®ï¼ˆbiasï¼‰ï¼Œç¥ç»å…ƒçš„æ•°å­¦è¡¨ç¤ºï¼š

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/f0b929045ae6eef23514bd7024be62f0.png" alt="img" style="zoom:50%;" />

è¿™é‡Œï¼Œ**wj** æ˜¯æƒé‡ï¼Œ**xj** æ˜¯è¾“å…¥ï¼Œè€Œ **Bias** æ˜¯åç½®é¡¹ã€‚

**å±‚ï¼ˆLayerï¼‰**

è¾“å…¥å±‚å’Œè¾“å‡ºå±‚ä¹‹é—´çš„å±‚è¢«ç§°ä¸ºéšè—å±‚ï¼Œå±‚ä¸å±‚ä¹‹é—´çš„è¿æ¥å¯†åº¦å’Œç±»å‹æ„æˆäº†ç½‘ç»œçš„é…ç½®ã€‚

ç¥ç»ç½‘ç»œç”±å¤šä¸ªå±‚ç»„æˆï¼ŒåŒ…æ‹¬ï¼š

- **è¾“å…¥å±‚ï¼ˆInput Layerï¼‰**ï¼šæ¥æ”¶åŸå§‹è¾“å…¥æ•°æ®ã€‚
- **éšè—å±‚ï¼ˆHidden Layerï¼‰**ï¼šå¯¹è¾“å…¥æ•°æ®è¿›è¡Œå¤„ç†ï¼Œå¯ä»¥æœ‰å¤šä¸ªéšè—å±‚ã€‚
- **è¾“å‡ºå±‚ï¼ˆOutput Layerï¼‰**ï¼šäº§ç”Ÿæœ€ç»ˆçš„è¾“å‡ºç»“æœã€‚

å…¸å‹çš„ç¥ç»ç½‘ç»œæ¶æ„:

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/1_3fA77_mLNiJTSgZFhYnU0Q3K5DV4.webp" alt="img" style="zoom: 50%;" />



**å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeedforward Neural Networkï¼ŒFNNï¼‰**

å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeedforward Neural Networkï¼ŒFNNï¼‰æ˜¯ç¥ç»ç½‘ç»œå®¶æ—ä¸­çš„åŸºæœ¬å•å…ƒã€‚

å‰é¦ˆç¥ç»ç½‘ç»œç‰¹ç‚¹æ˜¯æ•°æ®ä»è¾“å…¥å±‚å¼€å§‹ï¼Œç»è¿‡ä¸€ä¸ªæˆ–å¤šä¸ªéšè—å±‚ï¼Œæœ€ååˆ°è¾¾è¾“å‡ºå±‚ï¼Œå…¨è¿‡ç¨‹æ²¡æœ‰å¾ªç¯æˆ–åé¦ˆã€‚

![img](https://raw.githubusercontent.com/GMyhf/img/main/img/neural-net.png)

**å‰é¦ˆç¥ç»ç½‘ç»œçš„åŸºæœ¬ç»“æ„ï¼š**

- **è¾“å…¥å±‚ï¼š** æ•°æ®è¿›å…¥ç½‘ç»œçš„å…¥å£ç‚¹ã€‚è¾“å…¥å±‚çš„æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªè¾“å…¥ç‰¹å¾ã€‚
- **éšè—å±‚ï¼š**ä¸€ä¸ªæˆ–å¤šä¸ªå±‚ï¼Œç”¨äºæ•è·æ•°æ®çš„éçº¿æ€§ç‰¹å¾ã€‚æ¯ä¸ªéšè—å±‚ç”±å¤šä¸ªç¥ç»å…ƒç»„æˆï¼Œæ¯ä¸ªç¥ç»å…ƒé€šè¿‡æ¿€æ´»å‡½æ•°å¢åŠ éçº¿æ€§èƒ½åŠ›ã€‚
- **è¾“å‡ºå±‚ï¼š**è¾“å‡ºç½‘ç»œçš„é¢„æµ‹ç»“æœã€‚èŠ‚ç‚¹æ•°å’Œé—®é¢˜ç±»å‹ç›¸å…³ï¼Œä¾‹å¦‚åˆ†ç±»é—®é¢˜çš„è¾“å‡ºèŠ‚ç‚¹æ•°ç­‰äºç±»åˆ«æ•°ã€‚
- **è¿æ¥æƒé‡ä¸åç½®ï¼š**æ¯ä¸ªç¥ç»å…ƒçš„è¾“å…¥é€šè¿‡æƒé‡è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¹¶åŠ ä¸Šåç½®å€¼ï¼Œç„¶åé€šè¿‡æ¿€æ´»å‡½æ•°ä¼ é€’ã€‚



**å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Network, RNNï¼‰**

å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Network, RNNï¼‰ç»œæ˜¯ä¸€ç±»ä¸“é—¨å¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œèƒ½å¤Ÿæ•è·è¾“å…¥æ•°æ®ä¸­æ—¶é—´æˆ–é¡ºåºä¿¡æ¯çš„ä¾èµ–å…³ç³»ã€‚

RNN çš„ç‰¹åˆ«ä¹‹å¤„åœ¨äºå®ƒå…·æœ‰"è®°å¿†èƒ½åŠ›"ï¼Œå¯ä»¥åœ¨ç½‘ç»œçš„éšè—çŠ¶æ€ä¸­ä¿å­˜ä¹‹å‰æ—¶é—´æ­¥çš„ä¿¡æ¯ã€‚

å¾ªç¯ç¥ç»ç½‘ç»œç”¨äºå¤„ç†éšæ—¶é—´å˜åŒ–çš„æ•°æ®æ¨¡å¼ã€‚

åœ¨ RNN ä¸­ï¼Œç›¸åŒçš„å±‚è¢«ç”¨æ¥æ¥æ”¶è¾“å…¥å‚æ•°ï¼Œå¹¶åœ¨æŒ‡å®šçš„ç¥ç»ç½‘ç»œä¸­æ˜¾ç¤ºè¾“å‡ºå‚æ•°ã€‚

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/0_xs3Dya3qQBx6IU7C.png" alt="img" style="zoom:67%;" />

PyTorch æä¾›äº†å¼ºå¤§çš„å·¥å…·æ¥æ„å»ºå’Œè®­ç»ƒç¥ç»ç½‘ç»œã€‚

ç¥ç»ç½‘ç»œåœ¨ PyTorch ä¸­æ˜¯é€šè¿‡ **torch.nn** æ¨¡å—æ¥å®ç°çš„ã€‚

**torch.nn** æ¨¡å—æä¾›äº†å„ç§ç½‘ç»œå±‚ï¼ˆå¦‚å…¨è¿æ¥å±‚ã€å·ç§¯å±‚ç­‰ï¼‰ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼Œè®©ç¥ç»ç½‘ç»œçš„æ„å»ºå’Œè®­ç»ƒå˜å¾—æ›´åŠ æ–¹ä¾¿ã€‚

![img](https://raw.githubusercontent.com/GMyhf/img/main/img/1_3DUs-90altOgaBcVJ9LTGg.png)

åœ¨ PyTorch ä¸­ï¼Œæ„å»ºç¥ç»ç½‘ç»œé€šå¸¸éœ€è¦ç»§æ‰¿ nn.Module ç±»ã€‚

nn.Module æ˜¯æ‰€æœ‰ç¥ç»ç½‘ç»œæ¨¡å—çš„åŸºç±»ï¼Œä½ éœ€è¦å®šä¹‰ä»¥ä¸‹ä¸¤ä¸ªéƒ¨åˆ†ï¼š

- **`__init__()`**ï¼šå®šä¹‰ç½‘ç»œå±‚ã€‚
- **`forward()`**ï¼šå®šä¹‰æ•°æ®çš„å‰å‘ä¼ æ’­è¿‡ç¨‹ã€‚

ç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼ˆFully Connected Networkï¼‰ï¼š

**å®ä¾‹**

```python
import torch
import torch.nn as nn

# å®šä¹‰ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œæ¨¡å‹
class SimpleNN(nn.Module):
  def init(self):
    super(SimpleNN, self).init()
    # å®šä¹‰ä¸€ä¸ªè¾“å…¥å±‚åˆ°éšè—å±‚çš„å…¨è¿æ¥å±‚
    self.fc1 = nn.Linear(2, 2) # è¾“å…¥ 2 ä¸ªç‰¹å¾ï¼Œè¾“å‡º 2 ä¸ªç‰¹å¾
    # å®šä¹‰ä¸€ä¸ªéšè—å±‚åˆ°è¾“å‡ºå±‚çš„å…¨è¿æ¥å±‚
    self.fc2 = nn.Linear(2, 1) # è¾“å…¥ 2 ä¸ªç‰¹å¾ï¼Œè¾“å‡º 1 ä¸ªé¢„æµ‹å€¼

  def forward(self, x):
    # å‰å‘ä¼ æ’­è¿‡ç¨‹
    x = torch.relu(self.fc1(x)) # ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°
    x = self.fc2(x) # è¾“å‡ºå±‚
    return x

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = SimpleNN()

# æ‰“å°æ¨¡å‹
print(model)
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```
SimpleNN(
  (fc1): Linear(in_features=2, out_features=2, bias=True)
  (fc2): Linear(in_features=2, out_features=1, bias=True)
)
```

PyTorch æä¾›äº†è®¸å¤š<mark>å¸¸è§çš„ç¥ç»ç½‘ç»œå±‚</mark>ï¼Œä»¥ä¸‹æ˜¯å‡ ä¸ªå¸¸è§çš„ï¼š

- **`nn.Linear(in_features, out_features)`**ï¼šå…¨è¿æ¥å±‚ï¼Œè¾“å…¥ `in_features` ä¸ªç‰¹å¾ï¼Œè¾“å‡º `out_features` ä¸ªç‰¹å¾ã€‚
- **`nn.Conv2d(in_channels, out_channels, kernel_size)`**ï¼š2D å·ç§¯å±‚ï¼Œç”¨äºå›¾åƒå¤„ç†ã€‚
- **`nn.MaxPool2d(kernel_size)`**ï¼š2D æœ€å¤§æ± åŒ–å±‚ï¼Œç”¨äºé™ç»´ã€‚
- **`nn.ReLU()`**ï¼šReLU æ¿€æ´»å‡½æ•°ï¼Œå¸¸ç”¨äºéšè—å±‚ã€‚
- **`nn.Softmax(dim)`**ï¼šSoftmax æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸ç”¨äºè¾“å‡ºå±‚ï¼Œé€‚ç”¨äºå¤šç±»åˆ†ç±»é—®é¢˜ã€‚



**æ¿€æ´»å‡½æ•°ï¼ˆActivation Functionï¼‰**

æ¿€æ´»å‡½æ•°å†³å®šäº†ç¥ç»å…ƒæ˜¯å¦åº”è¯¥è¢«æ¿€æ´»ã€‚å®ƒä»¬æ˜¯éçº¿æ€§å‡½æ•°ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å’Œæ‰§è¡Œæ›´å¤æ‚çš„ä»»åŠ¡ã€‚å¸¸è§çš„æ¿€æ´»å‡½æ•°åŒ…æ‹¬ï¼š

- Sigmoidï¼šç”¨äºäºŒåˆ†ç±»é—®é¢˜ï¼Œè¾“å‡ºå€¼åœ¨ 0 å’Œ 1 ä¹‹é—´ã€‚
- Tanhï¼šè¾“å‡ºå€¼åœ¨ -1 å’Œ 1 ä¹‹é—´ï¼Œå¸¸ç”¨äºè¾“å‡ºå±‚ä¹‹å‰ã€‚
- ReLUï¼ˆRectified Linear Unitï¼‰ï¼šç›®å‰æœ€æµè¡Œçš„æ¿€æ´»å‡½æ•°ä¹‹ä¸€ï¼Œå®šä¹‰ä¸º `f(x) = max(0, x)`ï¼Œæœ‰åŠ©äºè§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚
- Softmaxï¼šå¸¸ç”¨äºå¤šåˆ†ç±»é—®é¢˜çš„è¾“å‡ºå±‚ï¼Œå°†è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚

**å®ä¾‹**

```python
import torch.nn.functional as F

# ReLU æ¿€æ´»
output = F.relu(input_tensor)

# Sigmoid æ¿€æ´»
output = torch.sigmoid(input_tensor)

# Tanh æ¿€æ´»
output = torch.tanh(input_tensor)
```



**æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰**

æŸå¤±å‡½æ•°ç”¨äºè¡¡é‡æ¨¡å‹çš„é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ã€‚

å¸¸è§çš„æŸå¤±å‡½æ•°åŒ…æ‹¬ï¼š

- **å‡æ–¹è¯¯å·®ï¼ˆMSELossï¼‰**ï¼šå›å½’é—®é¢˜å¸¸ç”¨ï¼Œè®¡ç®—è¾“å‡ºä¸ç›®æ ‡å€¼çš„å¹³æ–¹å·®ã€‚
- **äº¤å‰ç†µæŸå¤±ï¼ˆCrossEntropyLossï¼‰**ï¼šåˆ†ç±»é—®é¢˜å¸¸ç”¨ï¼Œè®¡ç®—è¾“å‡ºå’ŒçœŸå®æ ‡ç­¾ä¹‹é—´çš„äº¤å‰ç†µã€‚
- **BCEWithLogitsLoss**ï¼šäºŒåˆ†ç±»é—®é¢˜ï¼Œç»“åˆäº† Sigmoid æ¿€æ´»å’ŒäºŒå…ƒäº¤å‰ç†µæŸå¤±ã€‚

**å®ä¾‹**

```python
# å‡æ–¹è¯¯å·®æŸå¤±
criterion = nn.MSELoss()

# äº¤å‰ç†µæŸå¤±
criterion = nn.CrossEntropyLoss()

# äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
criterion = nn.BCEWithLogitsLoss()
```



**ä¼˜åŒ–å™¨ï¼ˆOptimizerï¼‰**

<mark>ä¼˜åŒ–å™¨è´Ÿè´£åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°ç½‘ç»œçš„æƒé‡å’Œåç½®</mark>ã€‚

å¸¸è§çš„ä¼˜åŒ–å™¨åŒ…æ‹¬ï¼š

- SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰
- Adamï¼ˆè‡ªé€‚åº”çŸ©ä¼°è®¡ï¼‰
- RMSpropï¼ˆå‡æ–¹æ ¹ä¼ æ’­ï¼‰

**å®ä¾‹**

```python
import torch.optim as optim

# ä½¿ç”¨ SGD ä¼˜åŒ–å™¨
optimizer = optim.SGD(model.parameters(), lr=0.01)

# ä½¿ç”¨ Adam ä¼˜åŒ–å™¨
optimizer = optim.Adam(model.parameters(), lr=0.001)
```



**è®­ç»ƒè¿‡ç¨‹ï¼ˆTraining Processï¼‰**

è®­ç»ƒç¥ç»ç½‘ç»œæ¶‰åŠä»¥ä¸‹æ­¥éª¤ï¼š

1. **å‡†å¤‡æ•°æ®**ï¼šé€šè¿‡ `DataLoader` åŠ è½½æ•°æ®ã€‚
2. **å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨**ã€‚
3. **å‰å‘ä¼ æ’­**ï¼šè®¡ç®—æ¨¡å‹çš„è¾“å‡ºã€‚
4. **è®¡ç®—æŸå¤±**ï¼šä¸ç›®æ ‡è¿›è¡Œæ¯”è¾ƒï¼Œå¾—åˆ°æŸå¤±å€¼ã€‚
5. **åå‘ä¼ æ’­**ï¼šé€šè¿‡ `loss.backward()` è®¡ç®—æ¢¯åº¦ã€‚
6. **æ›´æ–°å‚æ•°**ï¼šé€šè¿‡ `optimizer.step()` æ›´æ–°æ¨¡å‹çš„å‚æ•°ã€‚
7. **é‡å¤ä¸Šè¿°æ­¥éª¤**ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šçš„è®­ç»ƒè½®æ•°ã€‚

**å®ä¾‹**

```python
\# å‡è®¾å·²ç»å®šä¹‰å¥½äº†æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨

\# è®­ç»ƒæ•°æ®ç¤ºä¾‹
X = torch.randn(10, 2) # 10 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰ 2 ä¸ªç‰¹å¾
Y = torch.randn(10, 1) # 10 ä¸ªç›®æ ‡æ ‡ç­¾

\# è®­ç»ƒè¿‡ç¨‹
for epoch in range(100):  # è®­ç»ƒ 100 è½®
  model.train() # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
  optimizer.zero_grad() # æ¸…é™¤æ¢¯åº¦
  output = model(X) # å‰å‘ä¼ æ’­
  loss = criterion(output, Y) # è®¡ç®—æŸå¤±
  loss.backward() # åå‘ä¼ æ’­
  optimizer.step() # æ›´æ–°æƒé‡
  
  if (epoch + 1) % 10 == 0:  # æ¯ 10 è½®è¾“å‡ºä¸€æ¬¡æŸå¤±
    print(f'Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}')
```



**æµ‹è¯•ä¸è¯„ä¼°**

è®­ç»ƒå®Œæˆåï¼Œéœ€è¦å¯¹æ¨¡å‹è¿›è¡Œæµ‹è¯•å’Œè¯„ä¼°ã€‚

å¸¸è§çš„æ­¥éª¤åŒ…æ‹¬ï¼š

- **è®¡ç®—æµ‹è¯•é›†çš„æŸå¤±**ï¼šæµ‹è¯•æ¨¡å‹åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šçš„è¡¨ç°ã€‚
- **è®¡ç®—å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰**ï¼šå¯¹äºåˆ†ç±»é—®é¢˜ï¼Œè®¡ç®—æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ã€‚

**å®ä¾‹**

```python
# å‡è®¾ä½ æœ‰æµ‹è¯•é›† X_test å’Œ Y_test
model.eval() # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
with torch.no_grad():  # åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ç¦ç”¨æ¢¯åº¦è®¡ç®—
  output = model(X_test)
  loss = criterion(output, Y_test)
  print(f'Test Loss: {loss.item():.4f}')
```



**ç¥ç»ç½‘ç»œç±»å‹**

1. **å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeedforward Neural Networksï¼‰**ï¼šæ•°æ®å•å‘æµåŠ¨ï¼Œä»è¾“å…¥å±‚åˆ°è¾“å‡ºå±‚ï¼Œæ— åé¦ˆè¿æ¥ã€‚
2. **å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvolutional Neural Networks, CNNsï¼‰**ï¼šé€‚ç”¨äºå›¾åƒå¤„ç†ï¼Œä½¿ç”¨å·ç§¯å±‚æå–ç©ºé—´ç‰¹å¾ã€‚
3. **å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Networks, RNNsï¼‰**ï¼šé€‚ç”¨äºåºåˆ—æ•°æ®ï¼Œå¦‚æ—¶é—´åºåˆ—åˆ†æå’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œå…è®¸ä¿¡æ¯åé¦ˆå¾ªç¯ã€‚
4. **é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLong Short-Term Memory, LSTMï¼‰**ï¼šä¸€ç§ç‰¹æ®Šçš„RNNï¼Œèƒ½å¤Ÿå­¦ä¹ é•¿æœŸä¾èµ–å…³ç³»ã€‚



# å‚è€ƒæ–‡çŒ®

[1] Dartmouth workshop - Wikipedia

https://en.wikipedia.org/wiki/Dartmouth_workshop

[2]OpenAI Presents GPT-3, a 175 Billion Parameters Language Model | NVIDIA Technical Blog. July 07, 2020.

https://developer.nvidia.com/blog/openai-presents-gpt-3-a-175-billion-parameters-language-model/

[3] GPT-4 Technical Report. March 27, 2023.

https://cdn.openai.com/papers/gpt-4.pdf

[4] Transformer (deep learning architecture) - Wikipedia

https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)

[5] GPT-4 | OpenAI. March 14, 2023.

https://openai.com/index/gpt-4-research/