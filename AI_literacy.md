# äººå·¥æ™ºèƒ½æ¦‚è§ˆ

*Updated 2025-12-03 19:51 GMT+8*  
*Compiled by Hongfei Yan (2025 Summer)*    

https://github.com/GMyhf/2025fall-cs201/blob/main/AI_literacy.md



äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯ç ”ç©¶å’Œå¼€å‘èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æŠ€æœ¯å’Œæ–¹æ³•çš„å­¦ç§‘ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€å›¾åƒç†è§£ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨äººç­‰[1] [2]ã€‚æ—©åœ¨1950å¹´ï¼Œè‰¾ä¼¦Â·å›¾çµæå‡ºäº†æ£€éªŒæœºå™¨æ™ºèƒ½çš„â€œæ¨¡ä»¿æ¸¸æˆâ€ï¼ˆå³**å›¾çµæµ‹è¯•**ï¼‰ï¼Œæ£€éªŒæœºå™¨æ˜¯å¦èƒ½è®©äººåˆ†ä¸æ¸…å…¶ä¸äººç±»å¯¹è¯çš„åŒºåˆ«ã€‚1956å¹´è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ï¼ˆ**Dartmouth AI Workshop**ï¼‰å¬å¼€ï¼Œè¢«è®¤ä¸ºæ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„åˆ›å§‹æ—¶åˆ»ï¼Œçº¦ç¿°Â·éº¦å¡é”¡ç­‰äººé¦–æ¬¡æ­£å¼æå‡ºâ€œäººå·¥æ™ºèƒ½â€è¿™ä¸€æœ¯è¯­[1]ã€‚æ­¤åï¼ŒAIå‘å±•ç»å†äº†å¤šæ¬¡é«˜æ½®ä¸ä½è°·ï¼Œåˆ°21ä¸–çºªä¾èµ–äºå¼ºå¤§çš„è®¡ç®—èµ„æºã€æµ·é‡æ•°æ®å’Œæ–°ç®—æ³•çš„**æ·±åº¦å­¦ä¹ **æŠ€æœ¯å®ç°çªç ´ï¼Œæ¨åŠ¨AIè¿›å…¥å¹¿æ³›åº”ç”¨é˜¶æ®µã€‚



# 1. AIä¸‰å¤§æµæ´¾

äººå·¥æ™ºèƒ½çš„å‘å±•æ€æƒ³åˆ†ä¸ºä¸‰ä¸ªä¸»è¦æµæ´¾ï¼š

- **ç¬¦å·ä¸»ä¹‰ï¼ˆSymbolic AIï¼‰**ï¼šæ ¸å¿ƒè§‚ç‚¹æ˜¯é€šè¿‡ç¬¦å·è¡¨ç¤ºå’Œé€»è¾‘æ¨ç†æ¥å®ç°æ™ºèƒ½ã€‚å…¸å‹æ–¹æ³•åŒ…æ‹¬ä¸“å®¶ç³»ç»Ÿã€æœç´¢ç®—æ³•ã€é€»è¾‘æ¨ç†ï¼ˆå¦‚ä¸€é˜¶é€»è¾‘ï¼‰ã€è§„åˆ’ç³»ç»Ÿç­‰ã€‚ä»£è¡¨äººç‰©æœ‰è‰¾ä¼¦Â·çº½å„å°”ã€èµ«ä¼¯ç‰¹Â·è¥¿è’™ã€çº¦ç¿°Â·éº¦å¡é”¡ç­‰ã€‚ç¬¦å·ä¸»ä¹‰å¼ºè°ƒå¯è§£é‡Šæ€§å¼ºï¼Œé€‚ç”¨äºæœ‰æ˜ç¡®è§„åˆ™çš„ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦æ¨ç†ã€æ£‹ç±»æ¸¸æˆï¼‰ã€‚å…¶ä»£è¡¨æˆæœåŒ…æ‹¬20ä¸–çºª80å¹´ä»£çš„ä¸“å®¶ç³»ç»Ÿï¼ˆå¦‚DECå…¬å¸çš„XCONç³»ç»Ÿï¼Œæ˜¾è‘—æé«˜äº†é…ç½®æ•ˆç‡ï¼‰ä»¥åŠIBMçš„æ£‹ç±»ç¨‹åº**æ·±è“ï¼ˆDeep Blueï¼‰**ï¼Œ1997å¹´å‡»è´¥å›½é™…è±¡æ£‹å† å†›å¡æ–¯å¸•ç½—å¤«ã€‚ä½†ç¬¦å·ä¸»ä¹‰çš„ç¼ºç‚¹æ˜¯å­¦ä¹ èƒ½åŠ›å¼±ï¼Œéš¾ä»¥å¤„ç†æ¨¡ç³Šä¿¡æ¯ï¼Œéœ€è¦å¤§é‡æ‰‹å·¥ç¼–ç è§„åˆ™ã€‚

- **è¿æ¥ä¸»ä¹‰ï¼ˆConnectionismï¼‰**ï¼šæ ¸å¿ƒè§‚ç‚¹æ˜¯é€šè¿‡æ¨¡æ‹Ÿäººè„‘ç¥ç»å…ƒç½‘ç»œç»“æ„æ¥å®ç°æ™ºèƒ½ï¼Œä¾èµ–æ•°æ®é©±åŠ¨å­¦ä¹ ã€‚ä¸»è¦æ–¹æ³•æ˜¯å„ç§äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼ˆå¦‚å·ç§¯ç¥ç»ç½‘ç»œCNNã€å¾ªç¯ç¥ç»ç½‘ç»œRNNã€Transformerç­‰ï¼‰ã€‚ä»£è¡¨äººç‰©åŒ…æ‹¬Geoffrey Hintonã€Yann LeCunã€Ilya Sutskeverã€David Rumelhartç­‰ã€‚çº¦ç¿°Â·éœæ™®è²å°”å¾·ï¼ˆJohn Hopfieldï¼‰ä¸Geoffrey Hintonäº2024å¹´è·å¾—è¯ºè´å°”ç‰©ç†å­¦å¥–ï¼Œä»¥è¡¨å½°ä»–ä»¬åœ¨ç¥ç»ç½‘ç»œé¢†åŸŸçš„å¥ åŸºæ€§è´¡çŒ®ã€‚è¿æ¥ä¸»ä¹‰æµæ´¾å¼•é¢†äº†è¿‘å¹´æ¥AIçš„ä¸»è¦çªç ´ï¼ˆâ€œå¼ºæ•°æ®ã€å¼±è§„åˆ™â€ï¼‰ã€‚å…¶ä»£è¡¨æˆæœåŒ…æ‹¬æœ€æ—©çš„å•å±‚æ„ŸçŸ¥æœºï¼ˆPerceptronï¼Œ1958å¹´ï¼‰ä»¥åŠ1986å¹´Rumelhartç­‰äººæå‡ºçš„**è¯¯å·®åå‘ä¼ æ’­ç®—æ³•**ï¼ˆBackpropagationï¼‰ï¼Œä½¿å¤šå±‚ç¥ç»ç½‘ç»œå¾—ä»¥é«˜æ•ˆè®­ç»ƒã€‚ç°ä»£è¿æ¥ä¸»ä¹‰ç³»ç»Ÿåœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå‡å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä¾‹å¦‚Googleçš„AlphaGoå’Œå„ç§è§†è§‰æ¨¡å‹ã€OpenAIçš„GPTç³»åˆ—è¯­è¨€æ¨¡å‹ç­‰[3]ã€‚
- **è¡Œä¸ºä¸»ä¹‰ï¼ˆBehaviorismï¼‰**ï¼šåœ¨AIé¢†åŸŸå¸¸æŒ‡â€œæœºå™¨äººè¡ŒåŠ¨æ´¾â€æˆ–å¼ºåŒ–å­¦ä¹ æ€æƒ³ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å¹¶æ ¹æ®åé¦ˆï¼ˆå¥–æƒ©ï¼‰è‡ªä¸»å­¦ä¹ ï¼Œæ— éœ€äº‹å…ˆå‡è®¾å†…éƒ¨çŸ¥è¯†ç»“æ„ã€‚ä»£è¡¨äººç‰©æœ‰ç½—å¾·å°¼Â·å¸ƒé²å…‹æ–¯ï¼ˆRodney Brooksï¼‰ã€æå¼€å¤ç­‰ã€‚è¡Œä¸ºä¸»ä¹‰AIå¼ºè°ƒâ€œè¡ŒåŠ¨ä¼˜å…ˆâ€ï¼Œå¯¹ç°ä»£æœºå™¨äººå­¦å’Œå¼ºåŒ–å­¦ä¹ å½±å“æ·±è¿œã€‚å…¸å‹åº”ç”¨æ˜¯åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç³»ç»Ÿï¼Œå¦‚**AlphaGo/AlphaZero**ï¼ˆé€šè¿‡è‡ªæˆ‘åšå¼ˆå­¦ä¹ å›´æ£‹ç­–ç•¥ï¼‰å’Œè‡ªåŠ¨é©¾é©¶ç­‰ã€‚è¡Œä¸ºä¸»ä¹‰æµæ´¾ä¸‹çš„æ™ºèƒ½ä½“å¯è§†ä¸ºé€šè¿‡è¯•é”™å’Œç¯å¢ƒåé¦ˆæ¥ä¼˜åŒ–å†³ç­–ã€‚



# 2. AIçš„â€œä¸‰è¦ç´ â€ï¼šç®—æ³•ã€ç®—åŠ›ã€æ•°æ®

äººå·¥æ™ºèƒ½çš„å‘å±•ä¾èµ–äºä¸‰å¤§è¦ç´ ï¼š**ç®—æ³•**ï¼ˆAlgorithmï¼‰ã€**ç®—åŠ›**ï¼ˆComputeï¼‰å’Œ**æ•°æ®**ï¼ˆDataï¼‰ã€‚

- **ç®—æ³•ï¼ˆçµé­‚ï¼‰**ï¼šä¸åŒä»»åŠ¡ç±»å‹å¯¹åº”ä¸åŒç®—æ³•èŒƒå¼ã€‚å¸¸è§åˆ†ç±»åŒ…æ‹¬ç›‘ç£å­¦ä¹ ï¼ˆä½¿ç”¨æ ‡æ³¨æ•°æ®è®­ç»ƒæ¨¡å‹è¿›è¡Œåˆ†ç±»æˆ–å›å½’ï¼‰ã€æ— ç›‘ç£å­¦ä¹ ï¼ˆä»æ— æ ‡ç­¾æ•°æ®ä¸­æŒ–æ˜ç»“æ„ï¼Œå¦‚èšç±»ï¼‰ã€å¼ºåŒ–å­¦ä¹ ï¼ˆä½¿ç”¨å¥–æƒ©æœºåˆ¶ä¼˜åŒ–ç­–ç•¥ï¼‰ã€‚ä¾‹å¦‚ï¼Œé€»è¾‘å›å½’ç”¨äºåˆ†ç±»ï¼ˆç›‘ç£å­¦ä¹ ï¼‰ï¼ŒKMeansç”¨äºèšç±»ï¼ˆæ— ç›‘ç£å­¦ä¹ ï¼‰ã€‚æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„ç®—æ³•ä¸æ–­æ¼”è¿›ï¼Œå¼•å…¥äº†å¤šå±‚ç¥ç»ç½‘ç»œã€æ³¨æ„åŠ›æœºåˆ¶ç­‰åˆ›æ–°æ¶æ„ã€‚
- **ç®—åŠ›ï¼ˆå¼•æ“ï¼‰**ï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹å¾€å¾€éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºã€‚GPU/TPUç­‰é«˜æ€§èƒ½ç¡¬ä»¶ä½¿å¾—è®­ç»ƒå¤§è§„æ¨¡ç¥ç»ç½‘ç»œæˆä¸ºå¯èƒ½ã€‚ä»¥PyTorchä¸ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•æ£€æµ‹å½“å‰ç¡¬ä»¶ç¯å¢ƒä¸­GPUæˆ–Apple MPSçš„å¯ç”¨æ€§ã€‚
- **æ•°æ®ï¼ˆç‡ƒæ–™ï¼‰**ï¼šè®­ç»ƒæ¨¡å‹éœ€è¦å¤§é‡é«˜è´¨é‡çš„æ•°æ®ã€‚ç›‘ç£å­¦ä¹ å°¤å…¶ä¾èµ–æ ‡æ³¨æ•°æ®ã€‚ä¾‹å¦‚ï¼Œæé£é£ç­‰äººåœ¨2006å¹´å‘èµ·çš„ImageNetè®¡åˆ’æ”¶é›†äº†æ•°åƒä¸‡å¼ å›¾åƒï¼Œå¹¶ä¾æ‰˜ä¼—åŒ…æ ‡æ³¨åˆ›é€ äº†åŒ…å«1400ä¸‡å¼ æ ‡æ³¨å›¾ç‰‡çš„å¤§å‹æ•°æ®é›†ï¼Œå¤§å¤§æ¨åŠ¨äº†è®¡ç®—æœºè§†è§‰ç®—æ³•çš„å‘å±•ã€‚ä¸æ­¤åŒæ—¶ï¼Œæ— æ ‡ç­¾æ•°æ®ä¹Ÿé€šè¿‡è‡ªåŠ¨è®°å½•ç­‰æ–¹å¼æä¾›äº†æµ·é‡ä¿¡æ¯ï¼Œå¯ç”¨äºæ— ç›‘ç£å­¦ä¹ ã€‚æ€»ä¹‹ï¼Œç®—æ³•ã€ç®—åŠ›ä¸æ•°æ®ä¸‰è€…å…±åŒæ„æˆAIç³»ç»Ÿçš„åŸºç¡€ã€‚



# 3. å‰æ²¿åº”ç”¨æ¡ˆä¾‹

- **æ™ºèƒ½åšå¼ˆï¼šAlphaGo/AlphaZero**ï¼šDeepMindçš„AlphaGoç»“åˆäº†æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€å¼ºåŒ–å­¦ä¹ å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œæˆä¸ºé¦–ä¸ªæˆ˜èƒœå›´æ£‹äººç±»å† å†›çš„AIç³»ç»Ÿã€‚2016å¹´AlphaGoä»¥4:1å‡»è´¥æä¸–çŸ³ï¼Œ2017å¹´ä»¥3:0æˆ˜èƒœæŸ¯æ´ã€‚å…¶å¼ºåŒ–å­¦ä¹ ç‰ˆæœ¬AlphaGo Zeroæ— éœ€äººç±»æ£‹è°±ï¼Œä»éšæœºå¯¹å¼ˆä¸­è‡ªå­¦ï¼Œç»è¿‡æ•°å‘¨è®­ç»ƒä¾¿è¶…è¶Šäº†åŸç‰ˆAlphaGoã€‚è¿›ä¸€æ­¥çš„AlphaZeroç”šè‡³èƒ½ä»é›¶å¼€å§‹è‡ªå­¦å¤šç§æ£‹ç±»ï¼ˆå›´æ£‹ã€å›½é™…è±¡æ£‹ã€æ—¥æœ¬å°†æ£‹ç­‰ï¼‰ï¼Œå±•ç°å‡ºè¶…å¼ºçš„ç­–ç•¥å­¦ä¹ èƒ½åŠ›ã€‚å®ƒè¯æ˜äº†æ”¾å¼ƒäººç±»ç»éªŒã€æœ‰æ¡ä»¶çš„è‡ªæˆ‘å¯¹å¼ˆï¼ˆself-playï¼‰å­¦ä¹ åœ¨æŸäº›é¢†åŸŸèƒ½å¸¦æ¥æ›´ä¼˜è§£ã€‚
- **è‡ªç„¶è¯­è¨€å¤„ç†ï¼šTransformerä¸GPT**ï¼šTransformeræ¨¡å‹ç”±Googleç ”ç©¶è€…åœ¨2017å¹´æå‡ºï¼ˆè‘—åè®ºæ–‡*Attention Is All You Need*ï¼‰ï¼Œå…¶æ ¸å¿ƒæ˜¯**è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼Œå…è®¸æ¨¡å‹å¹¶è¡Œå¤„ç†åºåˆ—å¹¶æ•æ‰è¿œè·ç¦»ä¾èµ–[4]ã€‚Transformeræ¶æ„å¹¿æ³›åº”ç”¨äºå¤§è§„æ¨¡è‡ªç„¶è¯­è¨€å¤„ç†å’Œå…¶å®ƒé¢†åŸŸï¼Œå‚¬ç”Ÿäº†ä¼—å¤šé¢„è®­ç»ƒæ¨¡å‹å¦‚GPTç³»åˆ—å’ŒBERT[4]ã€‚GPTï¼ˆGenerative Pre-trained Transformerï¼‰æ˜¯OpenAIæ¨å‡ºçš„ä¸€ç±»è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨å·¨å¤§çš„Transformerè§£ç å™¨ç»“æ„è¿›è¡Œæ— ç›‘ç£é¢„è®­ç»ƒåå†å¾®è°ƒã€‚GPT-3äº2020å¹´é—®ä¸–ï¼Œæ‹¥æœ‰çº¦1750äº¿å‚æ•°[2]ï¼Œèƒ½å¤Ÿç”Ÿæˆè¿è´¯æµç•…çš„æ–‡æœ¬ï¼Œæ”¯æŒé›¶æ ·æœ¬å­¦ä¹ ï¼ˆzero-shotï¼‰å’Œå°‘æ ·æœ¬å­¦ä¹ ï¼ˆfew-shotï¼‰ï¼Œåœ¨ç¿»è¯‘ã€å¯¹è¯ã€å†™ä½œç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚**GPT-4**ï¼ˆ2023å¹´å‘å¸ƒï¼‰åœ¨GPT-3.5åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ‰©å±•è§„æ¨¡å’Œèƒ½åŠ›ï¼Œæ˜¯ä¸€ä¸ªæ”¯æŒæ–‡æœ¬å’Œå›¾åƒè¾“å…¥çš„**å¤šæ¨¡æ€å¤§æ¨¡å‹**[5] [3]ã€‚GPT-4åœ¨åŒ…æ‹¬æ¨¡æ‹Ÿå¾‹å¸ˆèµ„æ ¼è€ƒè¯•ï¼ˆbar examï¼‰åœ¨å†…çš„å¤šé¡¹ä¸“ä¸šæµ‹è¯•ä¸­è¡¨ç°å‡ºç±»äººæ°´å¹³ï¼ˆæˆç»©åœ¨å‰10%ï¼‰[3]ã€‚ä¸å‰ä»£æ¨¡å‹ç›¸æ¯”ï¼ŒGPT-4æ›´åŠ å¯é ã€å¯Œæœ‰åˆ›é€ åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†æ›´å¤æ‚ã€æ›´é•¿çš„æŒ‡ä»¤[5]ã€‚GPTç³»åˆ—æ¨¡å‹è¢«å¹¿æ³›åº”ç”¨äºå¯¹è¯æœºå™¨äººã€å†…å®¹ç”Ÿæˆã€ç¼–ç¨‹è¾…åŠ©ã€æ•™è‚²è¾…å¯¼ç­‰åœºæ™¯ã€‚

**ç¤ºä¾‹ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé—®ç­”**ã€‚ä»¥Hugging Face Transformerä¸ºä¾‹ï¼Œä¸‹è¿° `first_qa.py`ä»£ç è½½å…¥æœ¬åœ°ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé—®ç­”æ¨ç†ï¼š



> | æ¨¡å‹                                     | é€‚ç”¨è¯­è¨€ | ç”¨é€”     |
> | ---------------------------------------- | -------- | -------- |
> | `distilbert-base-cased-distilled-squad`  | è‹±æ–‡     | è‹±æ–‡é—®ç­” |
> | `uer/roberta-base-chinese-extractive-qa` | ä¸­æ–‡     | ä¸­æ–‡é—®ç­” |
>
> Q: å¦‚ä½•ç”¨**æµè§ˆå™¨æ‰‹åŠ¨ä¸‹è½½** `uer/roberta-base-chinese-extractive-qa` æ¨¡å‹ï¼Œåšåˆ°å®Œå…¨ **ç¦»çº¿éƒ¨ç½²** çš„æ­¥éª¤ï¼š
>
> 1. æ‰“å¼€æ¨¡å‹é¡µé¢
>
> æµè§ˆå™¨è®¿é—®ï¼šhttps://huggingface.co/uer/roberta-base-chinese-extractive-qa
>
> 2. è¿›å…¥ â€œFiles and versionsâ€ é¡µé¢ï¼Œæ‰‹åŠ¨ä¸‹è½½ä»¥ä¸‹å‡ ä¸ªå…³é”®æ–‡ä»¶ï¼š
>
> | æ–‡ä»¶å                    | è¯´æ˜                           |
>| ------------------------- | ------------------------------ |
> | `config.json`             | æ¨¡å‹ç»“æ„é…ç½®                   |
> | `pytorch_model.bin`       | æ¨¡å‹æƒé‡ï¼ˆå¾ˆå¤§ï¼Œ400MB å·¦å³ï¼‰   |
> | `tokenizer_config.json`   | tokenizer é…ç½®                 |
> | `vocab.txt`               | ä¸­æ–‡è¯è¡¨ï¼ˆå¿…éœ€ï¼‰               |
> | `special_tokens_map.json` | ç‰¹æ®Šç¬¦å·å®šä¹‰ï¼ˆå¯é€‰ä½†æ¨èï¼‰     |
> | `tokenizer.json`          | tokenizer çš„äºŒè¿›åˆ¶å½¢å¼ï¼ˆå¯é€‰ï¼‰ |
> 
> ä½ å¯ä»¥åœ¨ç½‘é¡µä¸­ä¾æ¬¡ç‚¹å‡»è¿™äº›æ–‡ä»¶ï¼Œç„¶åç‚¹å‡»å³ä¸Šè§’ â€œDownloadâ€ã€‚
>
> 
>
> æˆ–è€…è¿™é‡Œä¸‹è½½ï¼š
>
> https://disk.pku.edu.cn/link/AA5F507BA7BC504334ACA7FCBECFE64995
>Name: model-roberta-chinese-qa.zip
> Expires: Never
> Pickup Code: zXih



æˆ‘çš„clabäº‘è™šæ‹Ÿæœºbeijing.zhengmao.ltdï¼Œ`AI_literacy`æ–‡ä»¶å¤¹

```
[rocky@jensen AI_literacy]$ tree
.
â”œâ”€â”€ first_qa.py
â””â”€â”€ models
    â””â”€â”€ roberta-chinese-qa
        â”œâ”€â”€ config.json
        â”œâ”€â”€ pytorch_model.bin
        â”œâ”€â”€ special_tokens_map.json
        â”œâ”€â”€ tokenizer_config.json
        â””â”€â”€ vocab.txt

```



**åˆ›å»ºç‹¬ç«‹çš„è™šæ‹Ÿç¯å¢ƒ & å®‰è£… æ·±åº¦å­¦ä¹ æ¡†æ¶ PyTorch**

> ä¸ºæ¯ä¸ªé¡¹ç›®åˆ›å»ºç‹¬ç«‹çš„è™šæ‹Ÿç¯å¢ƒï¼Œé¿å…ä¾èµ–å†²çªã€‚
>
> ```
> cd ~/AI_literacy  # æ›¿æ¢ä¸ºä½ çš„é¡¹ç›®è·¯å¾„
> python3 -m venv .venv
> source .venv/bin/activate
> ```
>
> > é€€å‡ºè™šæ‹Ÿç¯å¢ƒå‘½ä»¤ï¼š`deactivate`
>
> 
>
> å®‰è£… æ·±åº¦å­¦ä¹ æ¡†æ¶ PyTorch  
> pip install -U transformers  
> pip install torch



`first_qa.py`

```python
from transformers import pipeline

qa = pipeline(
    "question-answering",
    #model="./models/distilbert-base-cased-distilled-squad",
    model="./models/roberta-chinese-qa",
    tokenizer="./models/roberta-chinese-qa",
    framework="pt"  # æ˜¾å¼è¦æ±‚ä½¿ç”¨ PyTorch
)

result = qa(
    question="è°æ˜¯äººå·¥æ™ºèƒ½ä¹‹çˆ¶ï¼Ÿ",
    context="è‰¾ä¼¦Â·å›¾çµæ˜¯äººå·¥æ™ºèƒ½ä¹‹çˆ¶ï¼Œè¢«èª‰ä¸ºè®¡ç®—æœºç§‘å­¦çš„å¥ åŸºäººã€‚"
)

print(result)       # çœ‹å®Œæ•´ç»“æœ
print(result["answer"])  # è¾“å‡ºåº”ä¸ºï¼šè‰¾ä¼¦Â·å›¾çµ
```

ä¸Šè¿°ä»£ç ä¸­ï¼ŒæŒ‡å®šäº†æœ¬åœ°ä¸‹è½½çš„ä¸­æ–‡é—®ç­”æ¨¡å‹ç›®å½•ï¼Œé€šè¿‡pipelineæ¥å£ç›´æ¥è¿›è¡ŒæŠ½å–å¼é—®ç­”æ¨ç†ã€‚åœ¨çœŸå®åº”ç”¨ä¸­ï¼Œå¯æ›¿æ¢ä¸ºæ›´å¼ºå¤§çš„æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰å¹¶ç»“åˆè¯­è¨€æç¤ºï¼ˆPromptï¼‰å®ç°æ›´å¤æ‚çš„è‡ªç„¶è¯­è¨€ä»»åŠ¡ã€‚

è¿è¡Œç»“æœå±•ç¤º

```
(.venv) [rocky@jensen AI_literacy]$ python first_qa.py 
Device set to use cpu
{'score': 0.31832563877105713, 'start': 0, 'end': 5, 'answer': 'è‰¾ä¼¦Â·å›¾çµ'}
è‰¾ä¼¦Â·å›¾çµ
```



> ç”¨ä¸€ä¸ªæœ¬åœ°çš„ä¸­æ–‡é—®ç­”æ¨¡å‹ï¼Œåœ¨ä¸€æ®µæ–‡æœ¬é‡Œæå–é—®é¢˜çš„ç­”æ¡ˆã€‚
>
> 1. å¼•å…¥ Hugging Face çš„ `pipeline`
>
> ```python
>from transformers import pipeline
> ```
> 
> è¿™æ˜¯ Hugging Face `transformers` æä¾›çš„ç®€æ´ APIï¼Œç”¨äºå¿«é€Ÿæ„å»º NLP ä»»åŠ¡ç®¡é“ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ã€ç¿»è¯‘ç­‰ã€‚
>
> 
>
> 2. æ„é€ é—®ç­”ä»»åŠ¡çš„ pipelineï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰
>
> ```python
>qa = pipeline(
>  "question-answering",
>  #model="./distilbert-base-cased-distilled-squad",
>     model="./roberta-chinese-qa",      # ä½¿ç”¨æœ¬åœ°ä¸­æ–‡æ¨¡å‹ç›®å½•
>     framework="pt"                     # å¼ºåˆ¶ä½¿ç”¨ PyTorch
>    )
>    ```
> 
> å‚æ•°è¯´æ˜ï¼š
>
> | å‚æ•°                           | å«ä¹‰                                                         |
>| ------------------------------ | ------------------------------------------------------------ |
> | `"question-answering"`         | æŒ‡å®šä»»åŠ¡ç±»å‹æ˜¯æŠ½å–å¼é—®ç­”ï¼ˆextractive QAï¼‰                    |
> | `model="./roberta-chinese-qa"` | æŒ‡å‘æœ¬åœ°ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶å¤¹ï¼Œé‡Œé¢åŒ…å« `pytorch_model.bin`ã€`config.json` ç­‰ |
> | `framework="pt"`               | æ˜¾å¼è¦æ±‚ä½¿ç”¨ PyTorchï¼Œè€Œä¸æ˜¯ TensorFlowï¼Œé˜²æ­¢æ„å¤–åŠ è½½ TF æ¨¡å‹å¼•å‘é”™è¯¯ |
> 
> `tokenizer` ä¼šè‡ªåŠ¨ä»æ¨¡å‹ç›®å½•ä¸­åŠ è½½ï¼Œæ— éœ€å•ç‹¬æŒ‡å®šã€‚
>
> ------
>
> 3. è¿è¡Œé—®ç­”æ¨ç†
>
> ```python
>result = qa(
>  question="è°æ˜¯äººå·¥æ™ºèƒ½ä¹‹çˆ¶ï¼Ÿ",
>  context="è‰¾ä¼¦Â·å›¾çµæ˜¯äººå·¥æ™ºèƒ½ä¹‹çˆ¶ï¼Œè¢«èª‰ä¸ºè®¡ç®—æœºç§‘å­¦çš„å¥ åŸºäººã€‚"
>    )
>    ```
> 
> è¯´æ˜ï¼š
>
> - è¾“å…¥çš„ `question` æ˜¯ç”¨æˆ·è¦é—®çš„é—®é¢˜ï¼›
>- `context` æ˜¯åŒ…å«ç­”æ¡ˆçš„ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ˆæ¨¡å‹ä¼šåœ¨é‡Œé¢æŸ¥æ‰¾ç­”æ¡ˆï¼‰ï¼›
> - è¿”å›çš„æ˜¯ä¸€ä¸ªåŒ…å« **é¢„æµ‹ç­”æ¡ˆä½ç½®ã€å†…å®¹ã€ç½®ä¿¡åº¦** çš„å­—å…¸ç»“æ„ã€‚
> 
> 
>
> ğŸ›  å¸¸è§è¡¥å……å»ºè®®ï¼š
>
> - **æ›´å¤æ‚ context**ï¼šä½ å¯ä»¥ç»™å®ƒæ›´å¤šæ®µè½ï¼Œå®ƒä¼šæ‰¾æœ€æœ‰å¯èƒ½çš„ç­”æ¡ˆï¼›
>
> - **ä¸­æ–‡ç²¾åº¦æé«˜**ï¼šä½ å¯ä»¥å°è¯• `hfl/chinese-roberta-wwm-ext-large` ä¹‹ç±»çš„æ¨¡å‹ï¼›
>
> 



# 4. æ·±åº¦å­¦ä¹ ä¸ç¥ç»ç½‘ç»œ

æ·±åº¦å­¦ä¹ æ˜¯è¿æ¥ä¸»ä¹‰æµæ´¾çš„æ ¸å¿ƒæ–¹æ³•ä¹‹ä¸€ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨**å¤šå±‚ç¥ç»ç½‘ç»œ**è‡ªåŠ¨ä»æ•°æ®ä¸­å­¦ä¹ å±‚æ¬¡åŒ–çš„ç‰¹å¾è¡¨ç¤ºã€‚æœ¬èŠ‚å°†ç³»ç»Ÿä»‹ç»ç¥ç»ç½‘ç»œçš„å…³é”®è®­ç»ƒæœºåˆ¶â€”â€”**åå‘ä¼ æ’­ç®—æ³•**ï¼Œå¹¶ç»“åˆ5ä¸ªç”±æµ…å…¥æ·±çš„PyTorchå®æˆ˜ç¤ºä¾‹ï¼Œå¸®åŠ©è¯»è€…æŒæ¡ä»åŸºç¡€å»ºæ¨¡åˆ°å¤æ‚å›¾åƒåˆ†ç±»ä»»åŠ¡çš„å®Œæ•´æµç¨‹ã€‚



**ç¥ç»ç½‘ç»œä¸åå‘ä¼ æ’­**

**åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰** æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„åŸºçŸ³ç®—æ³•ã€‚å®ƒé€šè¿‡ä¸¤ä¸ªé˜¶æ®µååŒå·¥ä½œï¼š 

1. **å‰å‘ä¼ æ’­**ï¼šè¾“å…¥æ•°æ®é€å±‚ç»è¿‡åŠ æƒæ±‚å’Œä¸éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ ReLUã€Softmaxï¼‰ï¼Œæœ€ç»ˆè¾“å‡ºé¢„æµ‹ç»“æœï¼› 
2. **åå‘ä¼ æ’­**ï¼šåŸºäºæŸå¤±å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µã€å‡æ–¹è¯¯å·®ï¼‰è®¡ç®—é¢„æµ‹å€¼ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„è¯¯å·®ï¼Œå¹¶åˆ©ç”¨**é“¾å¼æ³•åˆ™**é«˜æ•ˆåœ°è®¡ç®—æŸå¤±å¯¹æ¯ä¸€å±‚å‚æ•°çš„æ¢¯åº¦ã€‚

éšåï¼Œä¼˜åŒ–å™¨ï¼ˆå¦‚ SGDã€Adamï¼‰ä¾æ®è¿™äº›æ¢¯åº¦æ›´æ–°ç½‘ç»œæƒé‡ï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–æ•´ä½“æŸå¤±ã€‚è¿™ä¸€æœºåˆ¶ä½¿å¾—è®­ç»ƒåŒ…å«æ•°åç”šè‡³ä¸Šç™¾å±‚çš„æ·±åº¦ç½‘ç»œæˆä¸ºå¯èƒ½ï¼Œç›´æ¥æ¨åŠ¨äº†æ·±åº¦å­¦ä¹ çš„çˆ†å‘å¼å‘å±•ã€‚

**åå‘ä¼ æ’­ç®—æ³•æµç¨‹**

1. **å‰å‘ä¼ æ’­**ï¼šè¾“å…¥ â†’ é€å±‚è®¡ç®— â†’ è¾“å‡ºé¢„æµ‹å€¼ $\hat{y}$ 
2. **æŸå¤±è®¡ç®—**ï¼š$L = \text{Loss}(\hat{y}, y)$ 
3. **æ¢¯åº¦åä¼ **ï¼šä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚ï¼Œé€å±‚è®¡ç®— $\frac{\partial L}{\partial w}$ å’Œ $\frac{\partial L}{\partial b}$ 
4. **å‚æ•°æ›´æ–°**ï¼šä¾‹å¦‚ä½¿ç”¨æ¢¯åº¦ä¸‹é™ï¼š
   $$ w \leftarrow w - \eta \cdot \frac{\partial L}{\partial w} $$
   å…¶ä¸­ $\eta$ ä¸ºå­¦ä¹ ç‡ã€‚ 
5. **è¿­ä»£è®­ç»ƒ**ï¼šéå†è®­ç»ƒé›†å¤šä¸ª epochï¼Œç›´è‡³æŸå¤±æ”¶æ•›æˆ–è¾¾åˆ°é¢„è®¾è½®æ¬¡ã€‚

> âš ï¸ **æ³¨æ„**ï¼šæ·±å±‚ç½‘ç»œæ˜“é­é‡**æ¢¯åº¦æ¶ˆå¤±**ï¼ˆvanishing gradientsï¼‰æˆ–**æ¢¯åº¦çˆ†ç‚¸**ï¼ˆexploding gradientsï¼‰é—®é¢˜ï¼Œå°¤å…¶åœ¨ä½¿ç”¨ Sigmoid æˆ– Tanh æ¿€æ´»å‡½æ•°æ—¶ã€‚ç°ä»£å®è·µä¸­å¸¸é‡‡ç”¨ **ReLU æ¿€æ´»å‡½æ•°**ã€**æ‰¹å½’ä¸€åŒ–ï¼ˆBatchNormï¼‰** å’Œ**æ®‹å·®è¿æ¥ï¼ˆResNetï¼‰** ç­‰æŠ€æœ¯åŠ ä»¥ç¼“è§£ã€‚



**äº¤äº’å¯è§†åŒ–ï¼šç†è§£ç¥ç»ç½‘ç»œ**

Google æä¾›äº†ä¸€ä¸ªä¼˜ç§€çš„äº¤äº’å¼ç¥ç»ç½‘ç»œè®­ç»ƒå·¥å…·ï¼Œå¸®åŠ©ç›´è§‚ç†è§£æ¨¡å‹å¦‚ä½•åˆ†ç¦»éçº¿æ€§å¯åˆ†æ•°æ®ï¼š

https://developers.google.com/machine-learning/crash-course/neural-networks/interactive-exercises?hl=zh-cn

**ä»»åŠ¡ç›®æ ‡**ï¼šé…ç½®ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œä½¿å…¶èƒ½å¤Ÿå°†ä¸‹å›¾ä¸­çš„æ©™ç‚¹ä¸è“ç‚¹åˆ†å¼€ï¼Œå¹¶åœ¨è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ä¸Šå®ç°ä½äº 0.2 çš„æŸå¤±ã€‚

**æ“ä½œæŒ‡å—ï¼š**

1. è°ƒæ•´ç½‘ç»œç»“æ„ï¼š
   - ç‚¹å‡» **HIDDEN LAYERS** å·¦ä¾§çš„ **+ / â€“** æ·»åŠ /åˆ é™¤éšè—å±‚ï¼›
   - ç‚¹å‡»æŸéšè—å±‚ä¸Šæ–¹çš„ **+ / â€“** è°ƒæ•´è¯¥å±‚ç¥ç»å…ƒæ•°é‡ã€‚
2. ä¿®æ”¹è¶…å‚æ•°ï¼š
   - ä» **Learning rate** ä¸‹æ‹‰èœå•é€‰æ‹©åˆé€‚çš„å­¦ä¹ ç‡ï¼ˆå»ºè®®å°è¯• 0.01 ~ 0.1ï¼‰ï¼›
   - ä» **Activation** ä¸‹æ‹‰èœå•é€‰æ‹©æ¿€æ´»å‡½æ•°ï¼ˆæ¨è **ReLU**ï¼‰ã€‚
3. ç‚¹å‡» â–¶ï¸ å¼€å§‹è®­ç»ƒï¼Œè§‚å¯ŸæŸå¤±æ›²çº¿ä¸å†³ç­–è¾¹ç•Œæ¼”åŒ–ã€‚
4. è‹¥æœªè¾¾æ ‡ï¼Œç‚¹å‡» **Reset** å¹¶å°è¯•æ–°é…ç½®ã€‚

**æˆåŠŸç¤ºä¾‹æˆªå›¾å¦‚ä¸‹**ï¼ˆæŸå¤±å‡ä½äº 0.2ï¼‰ï¼š

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/6e8ec7f85c470b44edc373985d94337c.png" alt="6e8ec7f85c470b44edc373985d94337c" style="zoom: 50%;" />



**PyTorch å®æˆ˜ï¼š5 ä¸ªç¥ç»ç½‘ç»œåº”ç”¨ç¤ºä¾‹**

ä»¥ä¸‹ç¤ºä¾‹æ¶µç›–ä»æ‰‹åŠ¨å®ç°åˆ°ç°ä»£æ¶æ„çš„å®Œæ•´æ¼”è¿›è·¯å¾„ã€‚åå››ä¸ªç¤ºä¾‹ä½¿ç”¨PyTorchç¼–å†™ï¼Œ æ•™ç¨‹åœ¨ https://www.runoob.com/pytorch/pytorch-tutorial.html æˆ–å‚çœ‹é™„å½• Aã€‚

| ç¼–å· | ç¤ºä¾‹åç§°                     | å†…å®¹æ¦‚è¿°                                          | æŠ€æœ¯è¦ç‚¹                                   |
| ---- | ---------------------------- | ------------------------------------------------- | ------------------------------------------ |
| 1    | `0_xor_bp_neural_net_manual` | æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­ï¼Œè§£å†³ XOR éçº¿æ€§åˆ†ç±»é—®é¢˜         | å¼ é‡è¿ç®—ã€æ¢¯åº¦æ‰‹åŠ¨è®¡ç®—ã€æ— æ¡†æ¶ä¾èµ–         |
| 2    | `1_iris_neural_network`      | ä½¿ç”¨å…¨è¿æ¥ç½‘ç»œå¯¹é¸¢å°¾èŠ±æ•°æ®é›†è¿›è¡Œä¸‰åˆ†ç±»            | æ•°æ®åŠ è½½ã€`nn.Module`ã€äº¤å‰ç†µæŸå¤±          |
| 3    | `2_mnist_resnet18`           | å¾®è°ƒ ResNet18 å¯¹ MNIST æ‰‹å†™æ•°å­—åˆ†ç±»               | è¿ç§»å­¦ä¹ ã€å›¾åƒé¢„å¤„ç†ã€`torchvision.models` |
| 4    | `3_cifar10_resnet18`         | åœ¨ CIFAR-10 ä¸Šè®­ç»ƒ ResNet18                       | æ•°æ®å¢å¼ºã€å­¦ä¹ ç‡è°ƒåº¦ã€GPU åŠ é€Ÿ             |
| 5    | `4_tiny_imagenet_resnet50`   | ä½¿ç”¨ ResNet50 å¤„ç†æ›´å¤æ‚çš„ Tiny ImageNet åˆ†ç±»ä»»åŠ¡ | å¤§è§„æ¨¡æ•°æ®å¤„ç†ã€æ¨¡å‹å¾®è°ƒã€æ€§èƒ½è¯„ä¼°         |

> æ‰€æœ‰ä»£ç å‡å¯åœ¨é…å¥—ä»“åº“ï¼ˆhttps://github.com/GMyhf/2025spring-cs201/tree/main/LLMï¼‰ä¸­æ‰¾åˆ°ï¼Œå»ºè®®æŒ‰é¡ºåºå®è·µï¼Œé€æ­¥æŒæ¡ä»â€œç†è§£åŸç†â€åˆ°â€œå·¥ç¨‹éƒ¨ç½²â€çš„å®Œæ•´æŠ€èƒ½é“¾ã€‚



## 4.1 åœ¨å¼‚æˆ–é—®é¢˜ï¼ˆXORï¼‰ä¸­æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­

å¼‚æˆ–é—®é¢˜æ˜¯ç»å…¸çš„éçº¿æ€§å¯åˆ†é—®é¢˜ï¼Œç”¨æ¥æ¼”ç¤ºç¥ç»ç½‘ç»œçš„å­¦ä¹ èƒ½åŠ›ã€‚ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œå¯æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­æ¥è§£å†³å¼‚æˆ–ã€‚ä»¥ä¸‹å…ˆç»™å‡ºç®€æ´çš„ä¼ªä»£ç ï¼Œå†ç»™å‡ºå¯ä»¥è¿è¡Œçš„Pythonä»£ç ç¤ºä¾‹å±•ç¤ºäº†åå‘ä¼ æ’­æ›´æ–°æƒé‡çš„æ–¹å¼ï¼š

```python
# å‡è®¾ç½‘ç»œç»“æ„ï¼šè¾“å…¥å±‚2ä¸ªèŠ‚ç‚¹ï¼Œéšè—å±‚2ä¸ªèŠ‚ç‚¹ï¼Œè¾“å‡ºå±‚1ä¸ªèŠ‚ç‚¹
# åˆå§‹åŒ–æƒé‡
W1 = random([...])  # è¾“å…¥åˆ°éšè—å±‚
W2 = random([...])  # éšè—å±‚åˆ°è¾“å‡ºå±‚
learning_rate = 0.1

for epoch in range(epochs):
    # å‰å‘è®¡ç®—
    hidden = sigmoid(X @ W1)       # Xä¸ºè¾“å…¥[å››ç»„XORè¾“å…¥]
    output = sigmoid(hidden @ W2)  # é¢„æµ‹
    # è®¡ç®—è¯¯å·®
    error = (y - output)           # yä¸ºçœŸå®æ ‡ç­¾
    # åå‘ä¼ æ’­ï¼ˆé“¾å¼æ³•åˆ™ï¼‰
    dW2 = hidden.T @ (error * output * (1 - output))
    dW1 = X.T @ ((error * output * (1 - output)) @ W2.T * hidden * (1 - hidden))
    # æ›´æ–°æƒé‡
    W2 += learning_rate * dW2
    W1 += learning_rate * dW1

```



Backpropagation in Neural Network

https://www.geeksforgeeks.org/machine-learning/backpropagation-in-neural-network/



åå‘ä¼ æ’­ï¼ˆBack Propagationï¼‰ï¼Œåˆç§°ä¸ºâ€œè¯¯å·®çš„åå‘ä¼ æ’­â€ï¼Œæ˜¯ä¸€ç§ç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œçš„æ–¹æ³•ã€‚å…¶ç›®æ ‡æ˜¯é€šè¿‡è°ƒæ•´ç½‘ç»œä¸­çš„æƒé‡ï¼ˆweightsï¼‰å’Œåç½®ï¼ˆbiasesï¼‰ï¼Œæ¥å‡å°æ¨¡å‹é¢„æµ‹è¾“å‡ºä¸å®é™…è¾“å‡ºä¹‹é—´çš„å·®å¼‚ã€‚

å®ƒé€šè¿‡è¿­ä»£æ–¹å¼æ›´æ–°æƒé‡å’Œåç½®ï¼Œä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼ˆcost functionï¼‰ã€‚åœ¨æ¯ä¸€ä¸ªè®­ç»ƒå‘¨æœŸï¼ˆepochï¼‰ä¸­ï¼Œæ¨¡å‹ä¼šæ ¹æ®è¯¯å·®æ¢¯åº¦ï¼ˆerror gradientï¼‰æ›´æ–°å‚æ•°ï¼Œå¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•åŒ…æ‹¬æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰æˆ–éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ã€‚è¯¥ç®—æ³•ä½¿ç”¨å¾®ç§¯åˆ†ä¸­çš„<mark>é“¾å¼æ³•åˆ™</mark>æ¥è®¡ç®—æ¢¯åº¦ï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ç©¿è¶Šå¤æ‚çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œä¼˜åŒ–æŸå¤±å‡½æ•°ã€‚

> Back Propagation is also known as "Backward Propagation of Errors" is a method used to train neural network . Its goal is to reduce the difference between the modelâ€™s predicted output and the actual output by adjusting the weights and biases in the network.
>
> It works iteratively to adjust weights and bias to minimize the cost function. In each epoch the model adapts these parameters by reducing loss by following the error gradient. It often uses optimization algorithms like **gradient descent** or **stochastic gradient descent**. The algorithm computes the gradient using the chain rule from calculus allowing it to effectively navigate complex layers in the neural network to minimize the cost function.

<img src="https://media.geeksforgeeks.org/wp-content/uploads/20250701163824448467/Backpropagation-in-Neural-Network-1.webp" alt="Backpropagation-in-Neural-Network-1" style="zoom:67%;" />

<center>A simple illustration of how the backpropagation works by adjustments of weights</center>

<center>é€šè¿‡æƒé‡è°ƒæ•´ï¼Œç®€å•å±•ç¤ºåå‘ä¼ æ’­çš„å·¥ä½œæ–¹å¼</center>



**åå‘ä¼ æ’­çš„é‡è¦æ€§ï¼š**

- **é«˜æ•ˆçš„æƒé‡æ›´æ–°**ï¼šåˆ©ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¯ä¸ªæƒé‡çš„æ¢¯åº¦ï¼Œä»è€Œé«˜æ•ˆåœ°æ›´æ–°å‚æ•°ã€‚
- **è‰¯å¥½çš„æ‰©å±•æ€§**ï¼šé€‚ç”¨äºå¤šå±‚ç»“æ„å’Œå¤æ‚æ¶æ„ï¼Œæ˜¯æ·±åº¦å­¦ä¹ å¯è¡Œçš„æ ¸å¿ƒç®—æ³•ã€‚
- **è‡ªåŠ¨å­¦ä¹ èƒ½åŠ›**ï¼šè®­ç»ƒè¿‡ç¨‹è‡ªåŠ¨è¿›è¡Œï¼Œæ¨¡å‹ä¼šä¸æ–­è°ƒæ•´è‡ªèº«æ¥ä¼˜åŒ–æ€§èƒ½ã€‚

> **Back Propagation** plays a critical role in how neural networks improve over time. Here's why:
>
> 1. **Efficient Weight Update**: It computes the gradient of the loss function with respect to each weight using the chain rule making it possible to update weights efficiently.
> 2. **Scalability**: The Back Propagation algorithm scales well to networks with multiple layers and complex architectures making deep learning feasible.
> 3. **Automated Learning**: With Back Propagation the learning process becomes automated and the model can adjust itself to optimize its performance.



### 4.1.1 åå‘ä¼ æ’­ç®—æ³•çš„å·¥ä½œæµç¨‹

åå‘ä¼ æ’­ç®—æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼š**å‰å‘ä¼ æ’­ï¼ˆForward Passï¼‰** å’Œ **åå‘ä¼ æ’­ï¼ˆBackward Passï¼‰**

#### 1. å‰å‘ä¼ æ’­

è¾“å…¥æ•°æ®ä»è¾“å…¥å±‚å¼€å§‹ï¼Œç»è¿‡å¸¦æƒé‡çš„è¿æ¥ä¼ é€’åˆ°éšè—å±‚ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªæœ‰ä¸¤ä¸ªéšè—å±‚ h1 å’Œ h2 çš„ç½‘ç»œä¸­ï¼Œh1 çš„è¾“å‡ºä½œä¸º h2 çš„è¾“å…¥ã€‚åœ¨åº”ç”¨æ¿€æ´»å‡½æ•°å‰ï¼Œè¿˜ä¼šåŠ ä¸Šåç½®é¡¹ã€‚

æ¯ä¸€å±‚éƒ½ä¼šè®¡ç®—è¾“å…¥çš„åŠ æƒå’Œï¼ˆè®°ä½œ `a`ï¼‰ï¼Œå†é€šè¿‡å¦‚ ReLU ç­‰æ¿€æ´»å‡½æ•°å¾—åˆ°è¾“å‡º `o`ã€‚æœ€ç»ˆï¼Œè¾“å‡ºå±‚é€šå¸¸ä¼šä½¿ç”¨ softmax æ¿€æ´»å‡½æ•°å°†ç»“æœè½¬æ¢ä¸ºåˆ†ç±»æ¦‚ç‡ã€‚

> ### Working of Back Propagation Algorithm
>
> The Back Propagation algorithm involves two main steps: the **Forward Pass** and the **Backward Pass**.
>
> ### 1. Forward Pass Work
>
> In **forward pass** the input data is fed into the input layer. These inputs combined with their respective weights are passed to hidden layers. For example in a network with two hidden layers (h1 and h2) the output from h1 serves as the input to h2. Before applying an activation function, a bias is added to the weighted inputs.
>
> Each hidden layer computes the weighted sum (`a`) of the inputs then applies an activation function like [**ReLU (Rectified Linear Unit)**](https://www.geeksforgeeks.org/deep-learning/relu-activation-function-in-deep-learning/) to obtain the output (`o`). The output is passed to the next layer where an activation function such as [**softmax**](https://www.geeksforgeeks.org/deep-learning/the-role-of-softmax-in-neural-networks-detailed-explanation-and-applications/) converts the weighted outputs into probabilities for classification.

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/Backpropagation-in-Neural-Network-2.webp" alt="Backpropagation-in-Neural-Network-2" style="zoom:67%;" />

<center>The forward pass using weights and biases</center>

> h1,h2ï¼Œè¡¨ç¤ºéšè—å±‚çš„ä¸¤ä¸ªç¥ç»å…ƒ



#### 2. Backward Passåå‘ä¼ æ’­

åå‘ä¼ æ’­é˜¶æ®µä¼šå°†é¢„æµ‹è¾“å‡ºä¸å®é™…è¾“å‡ºçš„è¯¯å·®å‘åä¼ é€’ï¼Œå¹¶è°ƒæ•´æ¯ä¸€å±‚çš„æƒé‡å’Œåç½®ã€‚å¸¸è§çš„è¯¯å·®è®¡ç®—æ–¹æ³•æ˜¯**å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰**ï¼š

$MSE = (\text{Predicted Output} âˆ’ \text{Actual Output})^2$

åœ¨è¯¯å·®è®¡ç®—ä¹‹åï¼Œé€šè¿‡é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦ï¼Œè¿™äº›æ¢¯åº¦ç”¨äºæŒ‡å¯¼æƒé‡å’Œåç½®çš„æ›´æ–°æ–¹å‘å’Œå¹…åº¦ã€‚åå‘ä¼ æ’­è¿‡ç¨‹æ˜¯é€å±‚æ‰§è¡Œçš„ï¼Œ<mark>æ¿€æ´»å‡½æ•°çš„å¯¼æ•°åœ¨æ¢¯åº¦è®¡ç®—ä¸­èµ·ç€å…³é”®ä½œç”¨</mark>ã€‚



**åå‘ä¼ æ’­çš„ç¤ºä¾‹ï¼šæœºå™¨å­¦ä¹ ä¸­çš„æ¡ˆä¾‹**

å‡è®¾æˆ‘ä»¬ä½¿ç”¨ sigmoid æ¿€æ´»å‡½æ•°ï¼Œç›®æ ‡è¾“å‡ºä¸º 0.5ï¼Œå­¦ä¹ ç‡ä¸º 1ã€‚

> #### 2. Backward Pass
>
> In the backward pass the error (the difference between the predicted and actual output) is propagated back through the network to adjust the weights and biases. One common method for error calculation is the [**Mean Squared Error (MSE)**](https://www.geeksforgeeks.org/maths/mean-squared-error/) given by:
>
> $MSE = (\text{Predicted Output} âˆ’ \text{Actual Output})^2$
>
> Once the error is calculated the network adjusts weights using **gradients** which are computed with the chain rule. These gradients indicate how much each weight and bias should be adjusted to minimize the error in the next iteration. The backward pass continues layer by layer ensuring that the network learns and improves its performance. The activation function through its derivative plays a crucial role in computing these gradients during Back Propagation.
>
> 
>
> **Example of Back Propagation in Machine Learning**
>
> Letâ€™s walk through an example of Back Propagation in machine learning. Assume the neurons use the sigmoid activation function for the forward and backward pass. The target output is 0.5 and the learning rate is 1.

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/Backpropagation-in-Neural-Network-3.webp" alt="Backpropagation-in-Neural-Network-3" style="zoom:67%;" />

<center>Example (1) of backpropagation sum</center>



### 4.1.2 å‰å‘ä¼ æ’­

#### 1. åˆå§‹è®¡ç®—

The weighted sum at each node is calculated using:

> $a_j=\sum(w_{i,j}âˆ—x_i)$

Where,

- $a_j$ is the weighted sum of all the inputs and weights at each node
- $w_{i,j}$ represents the weights between the $i^{th}$ input and the $j^{th}$ neuron
- $x_i$ represents the value of the $i^{th}$ input

`O (output):`After applying the activation function to `a`, we get the output of the neuron:

> $o_j = \text{activation function}(a_j)$

#### 2. Sigmoidå‡½æ•°

The sigmoid function returns a value between 0 and 1, introducing non-linearity into the model.

> $y_j = \frac{1}{1+e^{âˆ’a_j}}$ 

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/Backpropagation-in-Neural-Network-4.webp" alt="Backpropagation-in-Neural-Network-4" style="zoom:67%;" />

<center>To find the outputs of y3, y4 and y5</center>



#### 3. è¾“å‡ºè®¡ç®—

h1 èŠ‚ç‚¹ï¼š
$$
a_1 = (w_{1,1} \times x_1) + (w_{2,1} \times x_2)
$$

$$
a_1 = (0.2 \times 0.35) + (0.2 \times 0.7) = 0.21
$$

è®¡ç®—å®Œ $a_1$ åï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­è®¡ç®— $y_3$ çš„å€¼ï¼š

$$
y_j = F(a_j) = \frac{1}{1 + e^{-a_1}}
$$

$$
y_3 = F(0.21) = \frac{1}{1 + e^{-0.21}} = 0.56
$$



h2 èŠ‚ç‚¹ï¼š
$$
a_2 = (w_{1,2} \times x_1) + (w_{2,2} \times x_2) = (0.3 \times 0.35) + (0.3 \times 0.7) = 0.315
$$

$$
y_4 = F(0.315) = \frac{1}{1 + e^{-0.315}} = 0.578
$$



è¾“å‡ºèŠ‚ç‚¹ O3ï¼š
$$
a_3 = (w_{1,3} \times y_3) + (w_{2,3} \times y_4) = (0.3 \times 0.56) + (0.9 \times 0.58) = 0.702
$$

$$
y_5 = F(0.702) = \frac{1}{1 + e^{-0.702}} = 0.67
$$



> At h1 node
>
> Once we calculated the a1 value, we can now proceed to find the y3 value:
>
> Similarly find the values of y4 at h2 and y5 at O3



<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/Backpropagation-in-Neural-Network-5.webp" alt="Backpropagation-in-Neural-Network-5" style="zoom:67%;" />

<center>Values of y3, y4 and y5</center>



#### 4. è¯¯å·®è®¡ç®—

Our actual output is 0.5 but we obtained 0.67**.** To calculate the error we can use the below formula:

> $Error_j=y_{target}âˆ’y_5$ 

=> 0.5âˆ’0.67=âˆ’0.17

Using this error value we will be backpropagating.



### 4.1.3 åå‘ä¼ æ’­

#### 1. Calculating Gradientsè®¡ç®—æ¢¯åº¦

The change in each weight is calculated as:

> $Î”w_{ij}=Î·Ã—Î´_jÃ—O_j$

Where:

- $Î´_j$ is the error term for each unit,
- $Î·$ is the learning rate.

#### 2. Output Unit Errorè¾“å‡ºå±‚è¯¯å·®

For O3:

> $Î´_5=y_5(1âˆ’y_5)(y_{target}âˆ’y_5)$

=0.67(1âˆ’0.67)(âˆ’0.17)=âˆ’0.0376

#### 3. Hidden Unit Erroréšè—å±‚è¯¯å·®

For h1:

> $Î´_3=y_3(1âˆ’y_3)(w_{1,3}Ã—Î´_5)$

=0.56(1âˆ’0.56)(0.3Ã—âˆ’0.0376)=âˆ’0.0027



For h2:

> $Î´_4=y_4(1âˆ’y_4)(w_{2,3}Ã—Î´_5)$

=0.59(1âˆ’0.59)(0.9Ã—âˆ’0.0376)=âˆ’0.0819



#### 4. Weight Updatesæƒé‡æ›´æ–°

For the weights from hidden to output layer:

> $Î”w_{2,3}=1Ã—(âˆ’0.0376)Ã—0.59=âˆ’0.022184$

New weight:

> $w_{2,3}(new)=âˆ’0.022184+0.9=0.877816$

For weights from input to hidden layer:

> $Î”w_{1,1}=1Ã—(âˆ’0.0027)Ã—0.35=0.000945$

New weight:

> $w_{1,1}(new)=0.000945+0.2=0.200945$

Similarly other weights are updated:

- $w_{1,2}(new)=0.273225$
- $w_{1,3}(new)=0.086615$
- $w_{2,1}(new)=0.269445$
- $w_{2,2}(new)=0.18534$

The updated weights are illustrated below

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/Backpropagation-in-Neural-Network-5-20251127160556998.webp" alt="Backpropagation-in-Neural-Network-5" style="zoom:67%;" />

<center>Through backward pass the weights are updated</center>

> ä¸Šå›¾æƒé‡æ²¡æœ‰æ›´æ–°ï¼Œä¾‹å¦‚ï¼š$w_{2,2}$åº”è¯¥æ›´æ–°ä¸º0.18534



After updating the weights the forward pass is repeated yielding:

- y3=0.57
- y4=0.56
- y5=0.61

ä»æœªè¾¾åˆ°ç›®æ ‡å€¼ 0.5ï¼Œå› æ­¤ç»§ç»­è¿›è¡Œåå‘ä¼ æ’­ï¼Œç›´åˆ°æ”¶æ•›ã€‚

> Since y5=0.61 is still not the target output the process of calculating the error and backpropagating continues until the desired output is reached.



This process demonstrates how Back Propagation iteratively updates weights by minimizing errors until the network accurately predicts the output.

> $Error=y_{target}âˆ’y_5$

=0.5âˆ’0.61=âˆ’0.11=0.5âˆ’0.61=âˆ’0.11

This process is said to be continued until the actual output is gained by the neural network.



### 4.1.4 ç”¨äº XOR é—®é¢˜çš„åå‘ä¼ æ’­å®ç°

**Q: XOR é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ**

> XORï¼ˆå¼‚æˆ–ï¼‰æ˜¯ä¸€ä¸ªç»å…¸çš„é€»è¾‘é—®é¢˜ï¼Œå®ƒçš„è¾“å…¥è¾“å‡ºå¦‚ä¸‹ï¼š
>
> | è¾“å…¥ A | è¾“å…¥ B | è¾“å‡º |
> | ------ | ------ | ---- |
> | 0      | 0      | 0    |
> | 0      | 1      | 1    |
> | 1      | 0      | 1    |
> | 1      | 1      | 0    |
>
> è¿™ä¸ªé—®é¢˜**ä¸èƒ½ç”¨ä¸€æ¡ç›´çº¿åˆ†å¼€**ï¼ˆä¸æ˜¯çº¿æ€§å¯åˆ†çš„ï¼‰ï¼Œæ‰€ä»¥å•å±‚æ„ŸçŸ¥æœºæ— æ³•è§£å†³ï¼Œå¿…é¡»ç”¨**è‡³å°‘ä¸€ä¸ªéšè—å±‚çš„ç¥ç»ç½‘ç»œ**ã€‚



> â€œ**å•å±‚æ„ŸçŸ¥æœº**â€ï¼ˆSingle-Layer Perceptronï¼‰æ˜¯ç¥ç»ç½‘ç»œæœ€åŸå§‹ã€æœ€ç®€å•çš„å½¢å¼ï¼Œç”± Frank Rosenblatt åœ¨ 1957 å¹´æå‡ºã€‚ç†è§£å®ƒï¼Œæœ‰åŠ©äºæ˜ç™½ä¸ºä»€ä¹ˆåƒ **XOR è¿™æ ·çš„é—®é¢˜æ— æ³•è¢«å®ƒè§£å†³**ï¼Œä»è€Œå¼•å‡ºå¤šå±‚ç¥ç»ç½‘ç»œå’Œåå‘ä¼ æ’­çš„å¿…è¦æ€§ã€‚
>
> å•å±‚æ„ŸçŸ¥æœºç»“æ„ï¼š
>
> - **è¾“å…¥å±‚**ï¼šæ¥æ”¶ç‰¹å¾ï¼ˆæ¯”å¦‚ $x_1, x_2$ï¼‰
> - **è¾“å‡ºå±‚**ï¼š**ç›´æ¥è¾“å‡ºç»“æœ**ï¼ˆæ²¡æœ‰éšè—å±‚ï¼ï¼‰
> - æ¯ä¸ªè¾“å…¥æœ‰ä¸€ä¸ªå¯¹åº”çš„æƒé‡ $w_1, w_2$ï¼Œè¿˜æœ‰ä¸€ä¸ªåç½® $b$
>
> **æ•°å­¦è¡¨è¾¾ï¼š**
> $$
> z = w_1 x_1 + w_2 x_2 + b
> \nonumber
> $$
>
> $$
> \text{output} = \begin{cases} 1 & \text{if } z \geq 0 \\ 0 & \text{if } z < 0 \end{cases}
> \nonumber
> $$
>
> > æ³¨æ„ï¼š**æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼ˆæˆ–åªæœ‰é˜¶è·ƒå‡½æ•°ï¼‰**ï¼Œ**æ²¡æœ‰éšè—å±‚**ï¼Œæ‰€ä»¥å«â€œå•å±‚â€ã€‚
> >
> > é˜¶è·ƒå‡½æ•°æ˜¯â€œç¡¬åˆ¤å†³â€ï¼Œé€‚åˆç†è®ºåˆ†æï¼›ä½†å› ä¸ºä¸å¯å¯¼ï¼Œä¸èƒ½ç”¨äºç°ä»£ç¥ç»ç½‘ç»œçš„è®­ç»ƒã€‚
>
> ------
>
> **å•å±‚æ„ŸçŸ¥æœºèƒ½åšä»€ä¹ˆ**ï¼Ÿ
>
> å®ƒåªèƒ½è§£å†³ **çº¿æ€§å¯åˆ†**ï¼ˆlinearly separableï¼‰çš„é—®é¢˜ã€‚
>
> **ä¾‹å­ï¼šAND é—¨**
>
> | xâ‚   | xâ‚‚   | y    |
> | ---- | ---- | ---- |
> | 0    | 0    | 0    |
> | 0    | 1    | 0    |
> | 1    | 0    | 0    |
> | 1    | 1    | 1    |
>
> å¯ä»¥ç”¨ä¸€æ¡ç›´çº¿åˆ†å¼€ 0 å’Œ 1 â†’ **çº¿æ€§å¯åˆ†** â†’ **å•å±‚æ„ŸçŸ¥æœºå¯ä»¥å­¦ä¼š**
>
> æ¯”å¦‚ï¼š
> å– (w_1 = 1, w_2 = 1, b = -1.5)
> åˆ™ï¼š
>
> - (0+0-1.5 = -1.5 < 0 â†’ 0)
> - (1+1-1.5 = 0.5 â‰¥ 0 â†’ 1)
>
> å®Œç¾ï¼
>
> ------
>
> **å•å±‚æ„ŸçŸ¥æœºä¸èƒ½åšä»€ä¹ˆï¼Ÿ**
>
> **XOR é—®é¢˜ï¼ˆå¼‚æˆ–ï¼‰ï¼š**
>
> | xâ‚   | xâ‚‚   | y    |
> | ---- | ---- | ---- |
> | 0    | 0    | 0    |
> | 0    | 1    | 1    |
> | 1    | 0    | 1    |
> | 1    | 1    | 0    |
>
> åœ¨äºŒç»´å¹³é¢ä¸Šç”»å‡ºæ¥ï¼š
>
> ```
> (0,1) â— (y=1)        (1,1) â—‹ (y=0)
> 
> (0,0) â—‹ (y=0)        (1,0) â— (y=1)
> ```
>
> ä½ ä¼šå‘ç°ï¼š**æ— æ³•ç”¨ä¸€æ¡ç›´çº¿æŠŠ â— å’Œ â—‹ å®Œå…¨åˆ†å¼€**ï¼
>
> â†’ è¿™å°±æ˜¯ **éçº¿æ€§å¯åˆ†é—®é¢˜**ã€‚
>
> **ç»“è®º**ï¼š 
>
> > **å•å±‚æ„ŸçŸ¥æœºæ— æ³•è§£å†³ XOR é—®é¢˜**ï¼Œå› ä¸ºå®ƒç¼ºä¹éçº¿æ€§è¡¨è¾¾èƒ½åŠ›ã€‚
>
> ------
>
> ** é‚£æ€ä¹ˆåŠï¼Ÿâ€”â€”å¼•å…¥éšè—å±‚ï¼**
>
> 1969 å¹´ï¼ŒMinsky å’Œ Papert åœ¨ã€ŠPerceptronsã€‹ä¸€ä¹¦ä¸­æŒ‡å‡ºäº†è¿™ä¸ªå±€é™ï¼Œå¯¼è‡´ç¥ç»ç½‘ç»œç ”ç©¶ä¸€åº¦åœæ»ã€‚
>
> ç›´åˆ°åæ¥äººä»¬å‘ç°ï¼š
>
> > **åªè¦åŠ ä¸€ä¸ªéšè—å±‚ï¼Œå¹¶ä½¿ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ sigmoidã€ReLUï¼‰ï¼Œç¥ç»ç½‘ç»œå°±èƒ½é€¼è¿‘ä»»æ„å‡½æ•°**ï¼ˆä¸‡èƒ½è¿‘ä¼¼å®šç†ï¼‰ã€‚
>
> äºæ˜¯ï¼Œ**å¤šå±‚æ„ŸçŸ¥æœº**ï¼ˆMLPï¼‰ + **åå‘ä¼ æ’­** æˆä¸ºè§£å†³æ–¹æ¡ˆã€‚
>
> ------
>
> **ğŸ”„ å¯¹æ¯”æ€»ç»“**
>
> | ç‰¹æ€§                | å•å±‚æ„ŸçŸ¥æœº         | å¤šå±‚æ„ŸçŸ¥æœºï¼ˆå¸¦åå‘ä¼ æ’­ï¼‰ |
> | ------------------- | ------------------ | ------------------------ |
> | éšè—å±‚              | âŒ æ²¡æœ‰             | âœ… æœ‰ï¼ˆè‡³å°‘1å±‚ï¼‰          |
> | æ¿€æ´»å‡½æ•°            | é˜¶è·ƒå‡½æ•°ï¼ˆä¸å¯å¯¼ï¼‰ | Sigmoid / ReLUï¼ˆå¯å¯¼ï¼‰   |
> | èƒ½å¦è§£å†³ AND/OR/NOT | âœ… å¯ä»¥             | âœ… å¯ä»¥                   |
> | èƒ½å¦è§£å†³ XOR        | âŒ ä¸è¡Œ             | âœ… å¯ä»¥                   |
> | æ˜¯å¦æ”¯æŒåå‘ä¼ æ’­    | âŒ ä¸æ”¯æŒï¼ˆä¸å¯å¯¼ï¼‰ | âœ… æ”¯æŒ                   |
> | å­¦ä¹ èƒ½åŠ›            | ä»…çº¿æ€§åˆ†ç±»         | éçº¿æ€§å»ºæ¨¡               |
>
> ------
>
> å°çŸ¥è¯†
>
> - â€œæ„ŸçŸ¥æœºâ€ï¼ˆPerceptronï¼‰é€šå¸¸ç‰¹æŒ‡**å•å±‚ã€ä½¿ç”¨é˜¶è·ƒæ¿€æ´»ã€ç”¨æ„ŸçŸ¥æœºå­¦ä¹ è§„åˆ™æ›´æ–°æƒé‡**çš„æ¨¡å‹ã€‚
> - è€Œæˆ‘ä»¬ä»Šå¤©è¯´çš„â€œç¥ç»ç½‘ç»œâ€ï¼Œä¸€èˆ¬æŒ‡**å¤šå±‚ã€å¯å¾®æ¿€æ´»ã€ç”¨æ¢¯åº¦ä¸‹é™+åå‘ä¼ æ’­è®­ç»ƒ**çš„æ¨¡å‹ï¼Œä¹Ÿå« **å¤šå±‚æ„ŸçŸ¥æœº**ï¼ˆMLPï¼‰ï¼Œå°½ç®¡åå­—é‡Œæœ‰â€œæ„ŸçŸ¥æœºâ€ï¼Œä½†å·²ç»å®Œå…¨ä¸åŒäº†ã€‚
>
> 



ä»¥ä¸‹ä»£ç æ¼”ç¤ºäº†å¦‚ä½•åœ¨ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨åå‘ä¼ æ’­æ¥è§£å†³ XOR é—®é¢˜ã€‚è¯¥ç¥ç»ç½‘ç»œåŒ…å«ï¼š

> This code demonstrates how Back Propagation is used in a neural network to solve the XOR problem. The neural network consists of:
>

#### 1. å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„

è¾“å…¥å±‚ï¼š2ä¸ªèŠ‚ç‚¹ï¼Œéšè—å±‚ï¼š4ä¸ªç¥ç»å…ƒï¼Œè¾“å‡ºå±‚ï¼š1ä¸ªç¥ç»å…ƒï¼Œæ¿€æ´»å‡½æ•°ï¼šSigmoid

> We define a neural network as Input layer with 2 inputs, Hidden layer with 4 neurons, Output layer with 1 output neuron and use **Sigmoid** function as activation function.

- **self.input_size = input_size**: stores the size of the input layer
- **self.hidden_size = hidden_size:** stores the size of the hidden layer
- **self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)**: initializes weights for input to hidden layer
- **self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)**: initializes weights for hidden to output layer
- **self.bias_hidden = np.zeros((1, self.hidden_size)):** initializes bias for hidden layer
- **self.bias_output = np.zeros((1, self.output_size)):** initializes bias for output layer



```python3
import numpy as np


class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        self.weights_input_hidden = np.random.randn(
            self.input_size, self.hidden_size)
        self.weights_hidden_output = np.random.randn(
            self.hidden_size, self.output_size)

        self.bias_hidden = np.zeros((1, self.hidden_size))
        self.bias_output = np.zeros((1, self.output_size))

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)
```



#### 2. å®šä¹‰å‰å‘ä¼ æ’­

In Forward pass inputs are passed through the network activating the hidden and output layers using the sigmoid function.

- **self.hidden_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden**: calculates activation for hidden layer
- **self.hidden_output= self.sigmoid(self.hidden_activation)**: applies activation function to hidden layer
- **self.output_activation= np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output:** calculates activation for output layer
- **self.predicted_output = self.sigmoid(self.output_activation):** applies activation function to output layer





```python3
def feedforward(self, X):
    self.hidden_activation = np.dot(
        X, self.weights_input_hidden) + self.bias_hidden
    self.hidden_output = self.sigmoid(self.hidden_activation)

    self.output_activation = np.dot(
        self.hidden_output, self.weights_hidden_output) + self.bias_output
    self.predicted_output = self.sigmoid(self.output_activation)

    return self.predicted_output
```



#### 3. å®šä¹‰åå‘ä¼ æ’­

In Backward pass or Back Propagation the errors between the predicted and actual outputs are computed. The gradients are calculated using the derivative of the sigmoid function and weights and biases are updated accordingly.

- **output_error = y - self.predicted_output:** calculates the error at the output layer
- **output_delta = output_error * self.sigmoid_derivative(self.predicted_output):** calculates the delta for the output layer
- **hidden_error = np.dot(output_delta, self.weights_hidden_output.T):** calculates the error at the hidden layer
- **hidden_delta = hidden_error \* self.sigmoid_derivative(self.hidden_output):** calculates the delta for the hidden layer
- **self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * learning_rate:** updates weights between hidden and output layers
- **self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate:** updates weights between input and hidden layers



```python3
def backward(self, X, y, learning_rate):
    output_error = y - self.predicted_output
    output_delta = output_error * \
        self.sigmoid_derivative(self.predicted_output)

    hidden_error = np.dot(output_delta, self.weights_hidden_output.T)
    hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)

    self.weights_hidden_output += np.dot(self.hidden_output.T,
                                         output_delta) * learning_rate
    self.bias_output += np.sum(output_delta, axis=0,
                               keepdims=True) * learning_rate
    self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate
    self.bias_hidden += np.sum(hidden_delta, axis=0,
                               keepdims=True) * learning_rate
```



#### 4. è®­ç»ƒç½‘ç»œ

The network is trained over 10,000 epochs using the Back Propagation algorithm with a learning rate of 0.1 progressively reducing the error.

- **output = self.feedforward(X):** computes the output for the current inputs
- **self.backward(X, y, learning_rate):** updates weights and biases using Back Propagation
- **loss = np.mean(np.square(y - output)):** calculates the mean squared error (MSE) loss



```python3
def train(self, X, y, epochs, learning_rate):
    for epoch in range(epochs):
        output = self.feedforward(X)
        self.backward(X, y, learning_rate)
        if epoch % 4000 == 0:
            loss = np.mean(np.square(y - output))
            print(f"Epoch {epoch}, Loss:{loss}")
```

#### 5. æµ‹è¯•ç¥ç»ç½‘ç»œ

- **X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]):** defines the input data
- **y = np.array([[0], [1], [1], [0]]):** defines the target values
- **nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1):** initializes the neural network
- **nn.train(X, y, epochs=10000, learning_rate=0.1):** trains the network
- **output = nn.feedforward(X):** gets the final predictions after training





```python3
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)
nn.train(X, y, epochs=10000, learning_rate=0.1)

output = nn.feedforward(X)
print("Predictions after training:")
print(output)
```

**Output:**

![Screenshot-2025-03-07-130223](https://raw.githubusercontent.com/GMyhf/img/main/img/Screenshot-2025-03-07-130223.png)

<center>Trained Model</center>



è®­ç»ƒåˆæœŸæŸå¤±ä¸º 0.2713ï¼Œé€æ­¥ä¸‹é™åˆ° 0.0066ï¼ˆç¬¬8000è½®ï¼‰ã€‚æœ€ç»ˆæ¨¡å‹å¯ä»¥å¾ˆå¥½åœ°é€¼è¿‘ XOR å‡½æ•°çš„è¾“å‡ºï¼Œå³ï¼š

- å¯¹äºè¾“å…¥ [0,0] å’Œ [1,1]ï¼Œè¾“å‡ºæ¥è¿‘ 0

- å¯¹äºè¾“å…¥ [0,1] å’Œ [1,0]ï¼Œè¾“å‡ºæ¥è¿‘ 1

  

> - The output shows the training progress of a neural network over 10,000 epochs. Initially the loss was high (0.2713) but it gradually decreased as the network learned reaching a low value of 0.0066 by epoch 8000.
> - The final predictions are close to the expected XOR outputs: approximately 0 for [0, 0] and [1, 1] and approximately 1 for [0, 1] and [1, 0] indicating that the network successfully learned to approximate the XOR function.



### 4.1.5 åå‘ä¼ æ’­çš„ä¼˜ç‚¹

**æ˜“äºå®ç°**ï¼šé€‚åˆåˆå­¦è€…ï¼Œæ— éœ€å¤ªå¤šç¥ç»ç½‘ç»œèƒŒæ™¯

**ç»“æ„ç®€å•ï¼Œçµæ´»åº”ç”¨**ï¼šä»ç®€å•å‰é¦ˆåˆ°å¤æ‚å·ç§¯/å¾ªç¯ç½‘ç»œéƒ½å¯ä½¿ç”¨

**é«˜æ•ˆ**ï¼šç›´æ¥æ ¹æ®è¯¯å·®æ›´æ–°æƒé‡ï¼Œå­¦ä¹ é€Ÿåº¦å¿«

**è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›**ï¼šæœ‰åŠ©äºæ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°æ›´å¥½

**å¯æ‰©å±•æ€§å¥½**ï¼šé€‚ç”¨äºå¤§å‹æ•°æ®é›†å’Œæ·±å±‚æ¨¡å‹

> **Advantages of Back Propagation for Neural Network Training**
>
> The key benefits of using the Back Propagation algorithm are:
>
> 1. **Ease of Implementation:** Back Propagation is beginner-friendly requiring no prior neural network knowledge and simplifies programming by adjusting weights with error derivatives.
> 2. **Simplicity and Flexibility:** Its straightforward design suits a range of tasks from basic feedforward to complex convolutional or recurrent networks.
> 3. **Efficiency**: Back Propagation accelerates learning by directly updating weights based on error especially in deep networks.
> 4. **Generalization:** It helps models generalize well to new data improving prediction accuracy on unseen examples.
> 5. **Scalability:** The algorithm scales efficiently with larger datasets and more complex networks making it ideal for large-scale tasks.



### 4.1.6 åå‘ä¼ æ’­é¢ä¸´çš„æŒ‘æˆ˜

**æ¢¯åº¦æ¶ˆå¤±**ï¼šåœ¨æ·±å±‚ç½‘ç»œä¸­æ¢¯åº¦å¯èƒ½è¿‡å°ï¼Œå¯¼è‡´å­¦ä¹ å›°éš¾ï¼ˆç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨ sigmoid/tanh æ—¶ï¼‰

**æ¢¯åº¦çˆ†ç‚¸**ï¼šæ¢¯åº¦å¯èƒ½å˜å¾—è¿‡å¤§ï¼Œä½¿è®­ç»ƒä¸ç¨³å®š

**è¿‡æ‹Ÿåˆ**ï¼šæ¨¡å‹ç»“æ„è¿‡äºå¤æ‚æ—¶ï¼Œå¯èƒ½è®°ä½è®­ç»ƒé›†è€Œéå­¦ä¹ ä¸€èˆ¬æ€§è§„å¾‹

> **Challenges with Back Propagation**
>
> While Back Propagation is useful it does face some challenges:
>
> 1. **Vanishing Gradient Problem**: In deep networks the gradients can become very small during Back Propagation making it difficult for the network to learn. This is common when using activation functions like sigmoid or tanh.
> 2. **Exploding Gradients**: The gradients can also become excessively large causing the network to diverge during training.
> 3. **Overfitting:** If the network is too complex it might memorize the training data instead of learning general patterns.



### 4.1.7 å®Œæ•´ä»£ç 

`xor_nn.py`

```python
# å¯¹äºXORé—®é¢˜ï¼ˆè¾“å…¥ä¸º[0,0], [0,1], [1,0], [1,1]ï¼‰ï¼ŒæœŸæœ›è¾“å‡ºä¸º[0,1,1,0]
# æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­ï¼Œæ²¡æœ‰ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¿™æœ‰åŠ©äºç†è§£åº•å±‚åŸç†
# https://www.geeksforgeeks.org/backpropagation-in-neural-network/
import numpy as np


class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size  # è¾“å…¥ç‰¹å¾ç»´åº¦
        self.hidden_size = hidden_size  # éšè—å±‚ç¥ç»å…ƒæ•°é‡
        self.output_size = output_size  # è¾“å‡ºå±‚ç¥ç»å…ƒæ•°é‡

        # è¾“å…¥å±‚åˆ°éšè—å±‚çš„æƒé‡ï¼Œå½¢çŠ¶ä¸º (è¾“å…¥ç»´åº¦, éšè—å±‚ç»´åº¦)
        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)
        # éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡ï¼Œå½¢çŠ¶ä¸º (éšè—å±‚ç»´åº¦, è¾“å‡ºå±‚ç»´åº¦)
        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)

        # éšè—å±‚çš„åç½®ï¼Œå½¢çŠ¶ä¸º (1, éšè—å±‚ç»´åº¦)
        self.bias_hidden = np.zeros((1, self.hidden_size))
        # è¾“å‡ºå±‚çš„åç½®ï¼Œå½¢çŠ¶ä¸º (1, è¾“å‡ºå±‚ç»´åº¦)
        self.bias_output = np.zeros((1, self.output_size))

    def sigmoid(self, x):  # æ¿€æ´»å‡½æ•°ï¼Œå°†è¾“å…¥å‹ç¼©åˆ°(0,1)åŒºé—´
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)  # Sigmoidçš„å¯¼æ•°ï¼Œç”¨äºåå‘ä¼ æ’­ä¸­çš„æ¢¯åº¦è®¡ç®—

    def feedforward(self, X):
        # éšè—å±‚è®¡ç®—
        self.hidden_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden  # çº¿æ€§å˜æ¢
        self.hidden_output = self.sigmoid(self.hidden_activation)  # æ¿€æ´»å‡½æ•°

        # è¾“å‡ºå±‚è®¡ç®—
        self.output_activation = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output
        self.predicted_output = self.sigmoid(self.output_activation)

        return self.predicted_output

    def backward(self, X, y, learning_rate):
        # è®¡ç®—è¾“å‡ºå±‚è¯¯å·®
        output_error = y - self.predicted_output  # è¯¯å·® = çœŸå®å€¼ - é¢„æµ‹å€¼
        # è®¡ç®—è¾“å‡ºå±‚çš„deltaï¼ˆæ¢¯åº¦çš„ä¸€éƒ¨åˆ†ï¼ŒæŸå¤±å¯¹æ¿€æ´»è¾“å…¥çš„æ¢¯åº¦ï¼‰
        output_delta = output_error * self.sigmoid_derivative(self.predicted_output)  # Delta = è¯¯å·® Ã— æ¿€æ´»å‡½æ•°å¯¼æ•°
        # output_delta = (y - Å·) * Ïƒ'(z_output)

        # è®¡ç®—éšè—å±‚è¯¯å·®ï¼ˆåå‘ä¼ æ’­ï¼‰
        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)  # å°†è¯¯å·®ä»è¾“å‡ºå±‚åå‘ä¼ æ’­åˆ°éšè—å±‚
        # hidden_error = output_delta @ W_hidden_output^T
        # è®¡ç®—éšè—å±‚çš„deltaï¼ˆæŸå¤±å¯¹éšè—å±‚æ¿€æ´»è¾“å…¥çš„æ¢¯åº¦ï¼‰
        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)  # Delta = è¯¯å·® Ã— æ¿€æ´»å‡½æ•°å¯¼æ•°
        # hidden_delta = (hidden_error) * Ïƒ'(z_hidden)

        # æ›´æ–°æƒé‡å’Œåç½®ï¼ˆä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ï¼‰
        # è®¡ç®—å¹¶æ›´æ–°éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡
        self.weights_hidden_output += np.dot(self.hidden_output.T,
                                             output_delta) * learning_rate  # æƒé‡æ›´æ–°é‡ = å­¦ä¹ ç‡ Ã— (éšè—å±‚è¾“å‡ºè½¬ç½® Ã— è¾“å‡ºå±‚delta)
        # W_hidden_output += learning_rate * (hidden_output^T @ output_delta)

        # æ›´æ–°è¾“å‡ºå±‚åç½®ï¼ŒåŸºäºæ‰€æœ‰æ ·æœ¬çš„è¾“å‡ºå±‚deltaæ²¿åˆ—æ±‚å’Œ
        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate  # åç½®æ›´æ–°é‡ = å­¦ä¹ ç‡ Ã— (æ²¿åˆ—æ±‚å’Œè¾“å‡ºå±‚delta)
        # b_output += learning_rate * sum(output_delta)

        # è®¡ç®—å¹¶æ›´æ–°ä»è¾“å…¥å±‚åˆ°éšè—å±‚çš„æƒé‡çš„æ¢¯åº¦
        self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate  # æƒé‡æ›´æ–°é‡ = å­¦ä¹ ç‡ Ã— (è¾“å…¥æ•°æ®è½¬ç½® Ã— éšè—å±‚delta)
        # W_input_hidden += learning_rate * (X^T @ hidden_delta)

        # æ›´æ–°éšè—å±‚åç½®ï¼ŒåŸºäºæ‰€æœ‰æ ·æœ¬çš„éšè—å±‚deltaæ²¿åˆ—æ±‚å’Œ
        # axis=0ï¼šæ²¿åˆ—æ±‚å’Œï¼Œèšåˆæ‰€æœ‰æ ·æœ¬çš„æ¢¯åº¦
        # keepdims=Trueï¼šä¿æŒåŸçŸ©é˜µçš„è¡Œæ•°ç»´åº¦ï¼Œç¡®ä¿åç½®æ›´æ–°çš„å½¢çŠ¶å…¼å®¹æ€§
        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate  # åç½®æ›´æ–°é‡ = å­¦ä¹ ç‡ Ã— (æ²¿åˆ—æ±‚å’Œéšè—å±‚delta)
        # b_hidden += learning_rate * sum(hidden_delta)

    def train(self, X, y, epochs, learning_rate):
        for epoch in range(epochs):
            output = self.feedforward(X)  # å‰å‘ä¼ æ’­
            self.backward(X, y, learning_rate)  # åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°
            if epoch % 4000 == 0:
                loss = np.mean(np.square(y - output))  # è®¡ç®—å‡æ–¹è¯¯å·®
                print(f"Epoch {epoch}, Loss:{loss}")


X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# è¾“å…¥ç»´åº¦ 2ï¼ˆäºŒç»´äºŒè¿›åˆ¶ç‰¹å¾ï¼‰ï¼Œéšè—å±‚4ä¸ªç¥ç»å…ƒï¼Œè¾“å‡ºå±‚1ä¸ªç¥ç»å…ƒï¼ˆäºŒåˆ†ç±»é—®é¢˜ï¼‰
nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)
# è®­ç»ƒæ€»è½®æ¬¡, å­¦ä¹ ç‡
nn.train(X, y, epochs=10000, learning_rate=0.1)

output = nn.feedforward(X)
print("Predictions after training:")
print(output)
"""
Epoch 0, Loss:0.2653166263520884
Epoch 4000, Loss:0.007000926683956338
Epoch 8000, Loss:0.001973630232951721
Predictions after training:
[[0.03613239]
 [0.96431351]
 [0.96058291]
 [0.03919372]]
"""
```



æœ€ç»ˆè®­ç»ƒåï¼Œè¯¥ç½‘ç»œå¯ä»¥å‡†ç¡®å­¦ä¹ XORé€»è¾‘ï¼ˆè®­ç»ƒæ•°æ®ï¼š${([0,0]\to0),([0,1]\to1),([1,0]\to1),([1,1]\to0)}$ï¼‰ï¼Œè¾“å‡ºæ¥è¿‘é¢„æœŸã€‚è¯¥ç¤ºä¾‹éªŒè¯äº†å¤šå±‚ç½‘ç»œå’Œåå‘ä¼ æ’­èƒ½è§£å†³çº¿æ€§æ¨¡å‹æ— æ³•å¤„ç†çš„é—®é¢˜ã€‚



## 4.2 åŸºäºç®€å•çš„å…¨è¿æ¥ç½‘ç»œé¸¢å°¾èŠ±æ•°æ®é›†åˆ†ç±»

**ä»»åŠ¡æè¿°**ï¼šä½¿ç”¨å…¨è¿æ¥ç¥ç»ç½‘ç»œå¯¹ç»å…¸çš„Irisï¼ˆé¸¢å°¾èŠ±ï¼‰æ•°æ®é›†è¿›è¡Œå¤šåˆ†ç±»ã€‚é¸¢ï¼ˆyuÄnï¼‰å°¾èŠ±æ•°æ®é›†åŒ…å«ä¸‰ä¸ªç±»åˆ«ï¼Œæ¯ä¸ªç±»åˆ«æœ‰50ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰å››ä¸ªç‰¹å¾ï¼šèŠ±è¼é•¿åº¦ã€èŠ±è¼å®½åº¦ã€èŠ±ç“£é•¿åº¦ã€èŠ±ç“£å®½åº¦ã€‚ç›®æ ‡æ˜¯æ ¹æ®è¿™å››ä¸ªç‰¹å¾é¢„æµ‹èŠ±çš„ç§ç±»ï¼Œå±äºå¤šåˆ†ç±»é—®é¢˜ã€‚

**å…³é”®æ­¥éª¤**ï¼šæ•°æ®é¢„å¤„ç†ï¼ˆæ ‡å‡†åŒ–ã€è®­ç»ƒ/æµ‹è¯•é›†åˆ’åˆ†ï¼‰ã€æ¨¡å‹æ„å»ºã€è®­ç»ƒä¸è¯„ä¼°ã€‚ç¤ºä¾‹ä»£ç ï¼ˆPyTorchï¼‰ï¼š



### å‡†å¤‡æ•°æ®

é¸¢å°¾èŠ±æ•°æ®é›†é€šå¸¸æ˜¯é€šè¿‡scikit-learnçš„datasetsæ¨¡å—è·å–çš„ï¼Œæ‰€ä»¥å¯èƒ½éœ€è¦ç»“åˆscikit-learnæ¥åŠ è½½æ•°æ®ï¼Œç„¶åè½¬æ¢æˆPyTorchçš„Tensorã€‚

æ•°æ®é¢„å¤„ç†æ–¹é¢ï¼Œéœ€è¦å°†ç‰¹å¾æ•°æ®å’Œæ ‡ç­¾åˆ†å¼€ã€‚ç‰¹å¾æ•°æ®éœ€è¦æ ‡å‡†åŒ–æˆ–å½’ä¸€åŒ–ï¼Œå› ä¸ºä¸åŒç‰¹å¾çš„é‡çº²å¯èƒ½ä¸åŒï¼Œè¿™å¯¹ç¥ç»ç½‘ç»œçš„è®­ç»ƒæœ‰å¸®åŠ©ã€‚æ ‡ç­¾éœ€è¦è½¬æ¢æˆæ•°å€¼å½¢å¼ï¼Œæ¯”å¦‚0ã€1ã€2ï¼Œç„¶åå¯èƒ½è¿˜éœ€è¦è½¬æ¢ä¸ºé•¿æ•´å‹å¼ é‡ï¼Œå› ä¸ºäº¤å‰ç†µæŸå¤±å‡½æ•°åœ¨PyTorchä¸­é€šå¸¸è¦æ±‚è¿™æ ·çš„æ ¼å¼ã€‚



### è®­ç»ƒæ¨¡å‹å’ŒéªŒè¯

æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹ã€‚è€ƒè™‘åˆ°é¸¢å°¾èŠ±æ•°æ®é›†ç›¸å¯¹ç®€å•ï¼Œå¯èƒ½ä¸éœ€è¦å¾ˆæ·±çš„ç½‘ç»œã€‚ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç½‘ç»œå¯èƒ½å°±è¶³å¤Ÿäº†ã€‚æ¯”å¦‚ï¼Œè¾“å…¥å±‚4ä¸ªèŠ‚ç‚¹ï¼Œéšè—å±‚å¯ä»¥é€‰æ‹©10ä¸ªèŠ‚ç‚¹ï¼Œè¾“å‡ºå±‚3ä¸ªèŠ‚ç‚¹å¯¹åº”ä¸‰ä¸ªç±»åˆ«ã€‚æ¿€æ´»å‡½æ•°å¯ä»¥ç”¨ReLUï¼Œè¾“å‡ºå±‚ç”¨Softmaxï¼Œä¸è¿‡æ›´å¸¸è§çš„åšæ³•æ˜¯ä½¿ç”¨CrossEntropyLossï¼Œå®ƒå†…éƒ¨å·²ç»ç»“åˆäº†Softmaxï¼Œæ‰€ä»¥è¾“å‡ºå±‚ä¸éœ€è¦æ˜¾å¼åº”ç”¨Softmaxã€‚

æ¥ä¸‹æ¥æ˜¯æ•°æ®é›†çš„åˆ’åˆ†ï¼Œé€šå¸¸åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œé¸¢å°¾èŠ±æ•°æ®é›†æ ·æœ¬è¾ƒå°‘ï¼Œå¯èƒ½éœ€è¦è¿›è¡Œåˆ†å±‚æŠ½æ ·ï¼Œç¡®ä¿æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„æ¯”ä¾‹ä¸€è‡´ã€‚æˆ–è€…ä½¿ç”¨äº¤å‰éªŒè¯ï¼Œä½†ç”±äºç”¨æˆ·å¯èƒ½å¸Œæœ›ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œå¯èƒ½ç›´æ¥é‡‡ç”¨80-20çš„åˆ’åˆ†ã€‚

å°†æ•°æ®è½¬æ¢ä¸ºPyTorchçš„DataLoaderï¼Œæ–¹ä¾¿æ‰¹é‡è®­ç»ƒã€‚æ•°æ®é›†è¿›è¡Œè®­ç»ƒå¯ä»¥åˆ†å°æ‰¹é‡å¤„ç†ã€‚

è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ã€‚äº¤å‰ç†µæŸå¤±å‡½æ•°é€‚ç”¨äºå¤šåˆ†ç±»é—®é¢˜ï¼Œä¼˜åŒ–å™¨å¯ä»¥é€‰æ‹©Adamæˆ–SGDã€‚å­¦ä¹ ç‡éœ€è¦é€‚å½“è®¾ç½®ï¼Œæ¯”å¦‚0.01æˆ–0.001ã€‚

è®­ç»ƒå¾ªç¯éƒ¨åˆ†ï¼Œéœ€è¦è¿­ä»£å¤šä¸ªepochï¼Œåœ¨æ¯ä¸ªepochä¸­è¿›è¡Œå‰å‘ä¼ æ’­ã€è®¡ç®—æŸå¤±ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°ã€‚åŒæ—¶ï¼Œå¯ä»¥ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å’Œå‡†ç¡®ç‡çš„å˜åŒ–ã€‚

éªŒè¯é˜¶æ®µï¼Œä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œç¡®ä¿æ¨¡å‹æ²¡æœ‰è¿‡æ‹Ÿåˆï¼Œå¹¶ä¸”æ³›åŒ–èƒ½åŠ›è‰¯å¥½ã€‚

å¯èƒ½éœ€è¦æ³¨æ„çš„é—®é¢˜åŒ…æ‹¬æ•°æ®æ ‡å‡†åŒ–åº”è¯¥åœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼Œç„¶ååº”ç”¨åˆ°æµ‹è¯•é›†ï¼Œé¿å…æ•°æ®æ³„æ¼ã€‚å¦å¤–ï¼Œæ ‡ç­¾æ˜¯å¦éœ€è¦è½¬æ¢ä¸ºone-hotç¼–ç ï¼Ÿåœ¨PyTorchä¸­ï¼ŒCrossEntropyLossä¸éœ€è¦ï¼Œå› ä¸ºæ ‡ç­¾æ˜¯ç±»åˆ«çš„ç´¢å¼•ã€‚

ç°åœ¨ï¼Œå°†è¿™äº›æ­¥éª¤æ•´åˆæˆä»£ç ï¼š

1. å¯¼å…¥å¿…è¦çš„åº“ï¼štorch, sklearn.datasetsä¸­çš„load_irisï¼Œsklearn.model_selectionçš„train_test_splitï¼Œä»¥åŠç›¸å…³çš„PyTorchæ¨¡å—ã€‚
2. åŠ è½½æ•°æ®ï¼Œåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ã€‚ä½¿ç”¨StandardScalerå¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå¤„ç†æ—¶æ³¨æ„ä»…æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚
3. è½¬æ¢æ•°æ®ä¸ºPyTorchçš„Tensorï¼Œå¹¶åˆ›å»ºDataLoaderã€‚å¯èƒ½ç”±äºæ•°æ®é‡å°ï¼Œç›´æ¥ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†ä½œä¸ºæ‰¹é‡ã€‚
4. å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹çš„ç»“æ„ï¼Œä½¿ç”¨å…¨è¿æ¥å±‚å’ŒReLUæ¿€æ´»å‡½æ•°ã€‚
5. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ã€‚
6. ç¼–å†™è®­ç»ƒå¾ªç¯ï¼Œè¿­ä»£epochï¼Œè®¡ç®—æŸå¤±ï¼Œåå‘ä¼ æ’­ï¼Œè®°å½•å‡†ç¡®ç‡ã€‚
7. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

å¦å¤–ï¼Œç”±äºé¸¢å°¾èŠ±æ•°æ®é›†æ ·æœ¬è¾ƒå°‘ï¼Œæ¨¡å‹å¯èƒ½ä¼šå¾ˆå¿«è¿‡æ‹Ÿåˆï¼Œæ‰€ä»¥å¯èƒ½éœ€è¦æ—©åœæˆ–è€…è°ƒæ•´ç½‘ç»œç»“æ„ï¼Œæ¯”å¦‚å‡å°‘éšè—å±‚çš„å¤§å°ï¼Œæˆ–è€…æ·»åŠ æ­£åˆ™åŒ–ï¼Œæ¯”å¦‚Dropoutå±‚ã€‚ä¸è¿‡ï¼Œä½œä¸ºç¤ºä¾‹ï¼Œå¯èƒ½ä¿æŒç®€å•å³å¯ã€‚

è¿™æ ·åº”è¯¥èƒ½å®Œæˆä¸€ä¸ªåŸºæœ¬çš„åˆ†ç±»ä»»åŠ¡ã€‚ä¸è¿‡å¯èƒ½éœ€è¦è°ƒæ•´è¶…å‚æ•°ï¼Œæ¯”å¦‚å­¦ä¹ ç‡ã€epochæ¬¡æ•°ï¼Œæˆ–è€…ç½‘ç»œç»“æ„ï¼Œä»¥è¾¾åˆ°æ›´å¥½çš„æ•ˆæœã€‚ä¾‹å¦‚ï¼Œå¦‚æœå‡†ç¡®ç‡ä¸å¤Ÿé«˜ï¼Œå¯ä»¥å°è¯•å¢åŠ éšè—å±‚çš„å¤§å°ï¼Œæˆ–è€…å¢åŠ æ›´å¤šçš„å±‚ã€‚ä½†é¸¢å°¾èŠ±æ•°æ®é›†ç›¸å¯¹ç®€å•ï¼Œå¯èƒ½ç®€å•çš„æ¨¡å‹å°±è¶³å¤Ÿã€‚

å¦å¤–ï¼Œå¯èƒ½éœ€è¦æ³¨æ„æ•°æ®åˆ’åˆ†æ—¶çš„éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°ã€‚æ­¤å¤–ï¼Œæ ‡å‡†åŒ–å¤„ç†æ˜¯å¦æ­£ç¡®åº”ç”¨ï¼Œé¿å…æ•°æ®æ³„æ¼ã€‚åœ¨ç¤ºä¾‹ä¸­ï¼Œå·²ç»æ­£ç¡®ä½¿ç”¨fit_transformåœ¨è®­ç»ƒé›†ï¼Œtransformåœ¨æµ‹è¯•é›†ã€‚



ä»¥ä¸‹æ˜¯ä½¿ç”¨PyTorchå®ç°Fisheré¸¢å°¾èŠ±æ•°æ®é›†åˆ†ç±»çš„Pythonç¨‹åº`iris_neural_network.py`ï¼Œå¼ºè°ƒä½¿ç”¨äº†ç¥ç»ç½‘ç»œè¿›è¡Œåˆ†ç±»ã€‚

å¦‚æœç¼ºå°‘moduleï¼Œå®‰è£…å³å¯ï¼Œä¾‹å¦‚ï¼špip install torch scikit-learn torchvision matplotlibç­‰ã€‚



**å®Œæ•´`iris_neural_network.py`ä»£ç **

```python
import torch, torch.nn as nn, torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


# å®šä¹‰æ¨¡å‹ç»“æ„
class IrisNet(nn.Module):
    def __init__(self, input_size=4, hidden_size=10, num_classes=3):
        super(IrisNet, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, num_classes)
        )

    def forward(self, x):
        return self.net(x)


# è®­ç»ƒå‡½æ•°
def train(model, dataloader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    for batch_X, batch_y in dataloader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)

        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * batch_X.size(0)

    return running_loss / len(dataloader.dataset)


# æµ‹è¯•å‡½æ•°
def evaluate(model, X, y, device):
    model.eval()
    with torch.no_grad():
        X, y = X.to(device), y.to(device)
        outputs = model(X)
        _, predicted = torch.max(outputs, 1)
        accuracy = (predicted == y).float().mean().item()
    return accuracy, predicted


# ä¸»ç¨‹åº
def main():
    # è®¾ç½®è®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device("mps")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")

    # åŠ è½½æ•°æ®
    iris = load_iris()
    X, y = iris.data, iris.target

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    """
    random_state=42
    è®¾å®šéšæœºæ•°ç§å­ï¼Œä»è€Œç¡®ä¿æ¯æ¬¡è¿è¡Œä»£ç æ—¶æ•°æ®åˆ’åˆ†çš„ç»“æœéƒ½æ˜¯ç›¸åŒçš„ã€‚è¿™æ ·åšå¯ä»¥ä½¿å®éªŒå…·æœ‰å¯é‡å¤æ€§ï¼Œ
    æœ‰åˆ©äºè°ƒè¯•å’Œç»“æœå¯¹æ¯”ã€‚

    stratify=y
    è¿™ä¸ªå‚æ•°è¡¨ç¤ºæŒ‰ç…§ y ä¸­çš„æ ‡ç­¾è¿›è¡Œåˆ†å±‚æŠ½æ ·ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­å„ç±»åˆ«çš„
    æ¯”ä¾‹ä¼šä¸åŸå§‹æ•°æ®ä¸­çš„ç±»åˆ«æ¯”ä¾‹ä¿æŒä¸€è‡´ã€‚è¿™å¯¹äºç±»åˆ«ä¸å¹³è¡¡çš„æ•°æ®é›†å°¤ä¸ºé‡è¦ï¼Œå¯ä»¥
    é¿å…æŸä¸€ç±»åˆ«åœ¨åˆ’åˆ†æ—¶è¢«ä¸¥é‡ä½ä¼°æˆ–è¿‡é‡‡æ ·ã€‚
    """

    # æ ‡å‡†åŒ–ï¼šåªåœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼Œå†å°†ç›¸åŒçš„å˜æ¢åº”ç”¨åˆ°æµ‹è¯•é›†ä¸Š
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # è½¬æ¢ä¸º Tensor
    X_train = torch.tensor(X_train, dtype=torch.float32)
    X_test = torch.tensor(X_test, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.long)
    y_test = torch.tensor(y_test, dtype=torch.long)

    # æ„é€  DataLoader
    train_dataset = TensorDataset(X_train, y_train)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

    # æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
    model = IrisNet().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    # è®­ç»ƒ
    num_epochs = 100
    for epoch in range(1, num_epochs + 1):
        loss = train(model, train_loader, criterion, optimizer, device)
        if epoch % 10 == 0:
            print(f"Epoch [{epoch:3d}/{num_epochs}], Loss: {loss:.4f}")

    # è¯„ä¼°
    test_acc, test_pred = evaluate(model, X_test, y_test, device)
    print(f"\nâœ… Test Accuracy: {test_acc * 100:.2f}%")

    # ç¤ºä¾‹é¢„æµ‹
    sample = X_test[0].unsqueeze(0)
    sample_pred = model(sample.to(device))
    pred_class = torch.argmax(sample_pred, dim=1).item()
    print(f"ğŸ” Sample Prediction: True = {y_test[0].item()}, Predicted = {pred_class}")


if __name__ == "__main__":
    main()

```

> å¦‚æœæ— æ³•ä½¿ç”¨GPU
>
> **åœ¨è¿è¡Œæ—¶å¼ºåˆ¶ä½¿ç”¨CPUè°ƒè¯•**
>
> ```
> CUDA_VISIBLE_DEVICES="" python iris_neural_network.py
> ```
>
> è¿™æ ·ç¦ç”¨CUDAï¼Œä½¿ç”¨CPUã€‚



> äº‘è™šæ‹Ÿæœºè¿è¡Œç»“æœï¼š
>
> ![image-20250915152854981](https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250915152854981.png)



**ä»£ç è¯´æ˜ï¼š**

1. **æ•°æ®å‡†å¤‡**ï¼š
   - ä½¿ç”¨scikit-learnåŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†
   - å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆ80%ï¼‰å’Œæµ‹è¯•é›†ï¼ˆ20%ï¼‰
   - ä½¿ç”¨æ ‡å‡†åŒ–å¤„ç†ï¼ˆStandardScalerï¼‰å¯¹ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–

2. **ç¥ç»ç½‘ç»œç»“æ„**ï¼š
   - è¾“å…¥å±‚ï¼š4ä¸ªç¥ç»å…ƒï¼ˆå¯¹åº”4ä¸ªç‰¹å¾ï¼‰
   - éšè—å±‚ï¼š10ä¸ªç¥ç»å…ƒï¼ˆä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°ï¼‰
   - è¾“å‡ºå±‚ï¼š3ä¸ªç¥ç»å…ƒï¼ˆå¯¹åº”3ä¸ªç±»åˆ«ï¼‰

3. **è®­ç»ƒé…ç½®**ï¼š
   - ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆCrossEntropyLossï¼‰
   - ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼ˆå­¦ä¹ ç‡0.01ï¼‰
   - è®­ç»ƒ100ä¸ªepoch

4. **è®­ç»ƒè¿‡ç¨‹**ï¼š
   - æ¯ä¸ªepochè®°å½•æŸå¤±å’Œå‡†ç¡®ç‡
   - æ¯10ä¸ªepochæ‰“å°è®­ç»ƒè¿›åº¦

5. **è¯„ä¼°ä¸é¢„æµ‹**ï¼š
   - æœ€ç»ˆåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹å‡†ç¡®ç‡
   - åŒ…å«ä¸€ä¸ªé¢„æµ‹ç¤ºä¾‹å±•ç¤º



è¯¥ç½‘ç»œç»è¿‡è®­ç»ƒåï¼Œé€šå¸¸èƒ½åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°90%ä»¥ä¸Šçš„å‡†ç¡®ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¤šå±‚å…¨è¿æ¥ç½‘ç»œå³å¯è¾ƒå¥½è§£å†³è¯¥å¤šåˆ†ç±»ä»»åŠ¡ï¼ˆIrisæ•°æ®é›†è§„æ¨¡å°ï¼Œç½‘ç»œä¸éœ€è¿‡æ·±ï¼‰ã€‚



> **æ³¨æ„äº‹é¡¹ï¼š**
>
> 1. ç”±äºæ•°æ®é›†è¾ƒå°ï¼Œæ¨¡å‹å¯èƒ½å¾ˆå¿«è¾¾åˆ°100%è®­ç»ƒå‡†ç¡®ç‡
> 2. å¯ä»¥è°ƒæ•´ä»¥ä¸‹å‚æ•°ä¼˜åŒ–æ€§èƒ½ï¼š
>    - éšè—å±‚å¤§å°ï¼ˆ10ï¼‰
>    - å­¦ä¹ ç‡ï¼ˆ0.01ï¼‰
>    - epochæ•°é‡ï¼ˆ100ï¼‰
>    - ä¼˜åŒ–å™¨ï¼ˆå°è¯•SGDç­‰ï¼‰
> 3. æ·»åŠ æ­£åˆ™åŒ–ï¼ˆå¦‚Dropoutå±‚ï¼‰å¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
> 4. å¯ä»¥ä½¿ç”¨GPUåŠ é€Ÿï¼ˆå°†æ•°æ®å’Œæ¨¡å‹ç§»åŠ¨åˆ°`cuda`è®¾å¤‡ï¼‰



> **å¯è§†åŒ–ï¼šç›‘ç£å­¦ä¹  + æ— ç›‘ç£å­¦ä¹ ï¼ˆIris æ•°æ®é›†ï¼‰**
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> from sklearn.datasets import load_iris
> from sklearn.linear_model import LogisticRegression
> from sklearn.cluster import KMeans
> from sklearn.model_selection import train_test_split
> from sklearn.preprocessing import StandardScaler
> from sklearn.metrics import accuracy_score
> 
> # 1. åŠ è½½æ•°æ®
> iris = load_iris()
> X = iris.data
> y = iris.target
> 
> # 2. æ ‡å‡†åŒ–æ•°æ®
> scaler = StandardScaler()
> X_scaled = scaler.fit_transform(X)
> 
> # 3. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆç”¨äºç›‘ç£å­¦ä¹ ï¼‰
> train_x, test_x, train_y, test_y = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
> 
> # 4. ç›‘ç£å­¦ä¹ ï¼šé€»è¾‘å›å½’åˆ†ç±»
> clf = LogisticRegression(max_iter=200)
> clf.fit(train_x, train_y)
> pred = clf.predict(test_x)
> print("Logistic Regression Accuracy:", accuracy_score(test_y, pred))
> 
> # 5. æ— ç›‘ç£å­¦ä¹ ï¼šKMeans èšç±»ï¼ˆèšæˆ3ç±»ï¼‰
> kmeans = KMeans(n_clusters=3, random_state=42)
> clusters = kmeans.fit_predict(X_scaled)
> 
> # 6. å¯è§†åŒ–èšç±»ï¼ˆé™ç»´åˆ°äºŒç»´ï¼‰
> from sklearn.decomposition import PCA
> pca = PCA(n_components=2)
> X_2d = pca.fit_transform(X_scaled)
> 
> plt.figure(figsize=(10, 5))
> 
> # èšç±»ç»“æœ
> plt.subplot(1, 2, 1)
> plt.scatter(X_2d[:, 0], X_2d[:, 1], c=clusters, cmap='viridis', s=50)
> plt.title("KMeans Clustering (unsupervised)")
> 
> # åŸå§‹æ ‡ç­¾
> plt.subplot(1, 2, 2)
> plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='Set1', s=50)
> plt.title("Ground Truth Labels (supervised)")
> 
> plt.show()
> 
> ```
>
> > ä½¿ç”¨ `LogisticRegression` è®­ç»ƒä¸€ä¸ªæœ‰ç›‘ç£åˆ†ç±»å™¨ï¼Œå¹¶è¾“å‡ºæµ‹è¯•é›†å‡†ç¡®ç‡ï¼›
> >
> > ä½¿ç”¨ `KMeans` è¿›è¡Œæ— ç›‘ç£èšç±»ï¼›
> >
> > ä½¿ç”¨ PCA å°† 4 ç»´æ•°æ®é™ç»´ä¸º 2 ç»´ï¼Œä»¥ä¾¿å¯è§†åŒ–èšç±»ç»“æœå’ŒçœŸå®æ ‡ç­¾ï¼›
> >
>
> å¦‚å›¾æ‰€ç¤ºï¼Œé€šè¿‡ä½¿ç”¨PCAå°†ç‰¹å¾é™è‡³äºŒç»´ï¼Œå¯è§†åŒ–èšç±»æ•ˆæœä¸çœŸå®åˆ†ç±»çš„å¯¹æ¯”ï¼š
>
> 
>
> <img src="https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250727143806445.png" alt="image-20250727143806445" style="zoom: 67%;" />
>
> <center>å›¾ï¼šIris æ•°æ®èšç±»ï¼ˆå·¦ï¼šç½‘ç»œèšç±»ç»“æœï¼›å³ï¼šçœŸå®ç±»åˆ«ï¼‰</center>
>



## 4.3 åŸºäº ResNet18 çš„MNISTæ‰‹å†™æ•°å­—è¯†åˆ«

é€šè¿‡æ‰‹å†™æ•°å­—è¯†åˆ«çš„å®ä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¥ç»ç½‘ç»œçš„å¼ºå¤§ï¼Œä¹Ÿå¯ä»¥æ›´å¥½åœ°ç†è§£å®ƒæ˜¯å¦‚ä½•è¿è¡Œçš„ã€‚

MNIST æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸæœ€ç»å…¸çš„åŸºå‡†æ•°æ®é›†ä¹‹ä¸€ï¼ŒåŒ…å« 60,000 å¼ è®­ç»ƒå›¾åƒå’Œ 10,000 å¼ æµ‹è¯•å›¾åƒï¼Œå‡ä¸º 28Ã—28 åƒç´ çš„ç°åº¦æ‰‹å†™æ•°å­—ï¼ˆ0â€“9ï¼‰ã€‚å°½ç®¡ä»»åŠ¡çœ‹ä¼¼ç®€å•ï¼Œä½†å®ƒå¸¸è¢«ç”¨äºéªŒè¯æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åŸºæœ¬èƒ½åŠ›ã€‚

æœ¬ç¤ºä¾‹ä½¿ç”¨ **ResNet18** æ¶æ„å¯¹ MNIST è¿›è¡Œåˆ†ç±»ã€‚è™½ç„¶ ResNet18 åŸä¸º RGB å›¾åƒï¼ˆ3é€šé“ï¼‰è®¾è®¡ï¼Œä½†é€šè¿‡é€‚å½“è°ƒæ•´è¾“å…¥å±‚ï¼Œæˆ‘ä»¬å¯å°†å…¶æˆåŠŸåº”ç”¨äºå•é€šé“ç°åº¦å›¾åƒï¼Œå¹¶åœ¨ MNIST ä¸Šè½»æ¾è¾¾åˆ° **99%+ çš„å‡†ç¡®ç‡**ã€‚

**å…³é”®æ­¥éª¤**

1. **æ•°æ®åŠ è½½ä¸å¢å¼º**

   - ä½¿ç”¨ `torchvision.datasets.MNIST` åŠ è½½æ•°æ®ã€‚
   - å¯¹è®­ç»ƒé›†åº”ç”¨è½»å¾®çš„æ•°æ®å¢å¼ºï¼ˆå¦‚éšæœºæ—‹è½¬ Â±10Â°ï¼‰ï¼Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚
   - å¯¹å›¾åƒè¿›è¡Œæ ‡å‡†åŒ–ï¼šå‡å€¼å’Œæ ‡å‡†å·®å‡ä¸º `0.5`ï¼ˆå› åƒç´ å€¼èŒƒå›´ä¸º [0,1]ï¼‰ã€‚

2. **æ¨¡å‹é€‚é…**

   - å°† ResNet18 çš„é¦–å±‚å·ç§¯ä» 3 é€šé“è¾“å…¥æ”¹ä¸º **1 é€šé“**ï¼š

     ```python
     net.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
     ```

   - ä¿®æ”¹å…¨è¿æ¥å±‚è¾“å‡ºç»´åº¦ä¸º **10**ï¼ˆå¯¹åº” 0â€“9 åä¸ªç±»åˆ«ï¼‰ã€‚

3. **è®­ç»ƒç­–ç•¥**

   - ä¼˜åŒ–å™¨ï¼šSGDï¼ˆå­¦ä¹ ç‡ 0.01ï¼ŒåŠ¨é‡ 0.9ï¼Œæƒé‡è¡°å‡ 5e-4ï¼‰
   - å­¦ä¹ ç‡è°ƒåº¦ï¼šä½™å¼¦é€€ç«ï¼ˆ`CosineAnnealingLR`ï¼‰
   - æ—©åœæœºåˆ¶ï¼šè‹¥è¿ç»­ 10 ä¸ª epoch éªŒè¯æŸå¤±æ— æ˜¾è‘—ä¸‹é™ï¼Œåˆ™æå‰ç»ˆæ­¢

4. **è¯„ä¼°ä¸å¯è§†åŒ–**

   - æŠ¥å‘Šæ•´ä½“å‡†ç¡®ç‡åŠæ¯ç±»å‡†ç¡®ç‡
   - å¯è§†åŒ–éƒ¨åˆ†æµ‹è¯•æ ·æœ¬åŠå…¶é¢„æµ‹ç»“æœ



**å®Œæ•´`mnist_resnet18.py`ä»£ç **

```python
import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import matplotlib.pyplot as plt
import numpy as np
import time

def main():
    # 1. æ•°æ®å¢å¼º + é¢„å¤„ç†
    transform_train = transforms.Compose([
        transforms.RandomRotation(10),  # éšæœºæ—‹è½¬
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))  # MNIST æ˜¯å•é€šé“ï¼Œä½¿ç”¨ (0.5,) æ¥è§„èŒƒåŒ–
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])

    # åŠ è½½ MNIST æ•°æ®é›†
    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)

    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)
    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2, pin_memory=True)

    classes = [str(i) for i in range(10)]  # MNIST ç±»åˆ«æ˜¯ 0 åˆ° 9

    # 2. è®¾ç½®è®¾å¤‡å’Œæ¨¡å‹
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print("Using device:", device)

    # åŠ è½½é¢„å®šä¹‰çš„ ResNet18 å¹¶ä¿®æ”¹è¾“å…¥å±‚å’Œè¾“å‡ºå±‚
    net = models.resnet18(weights=None)
    # ä¿®æ”¹è¾“å…¥å±‚çš„ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼Œä½¿å…¶æ¥å—å•é€šé“ï¼ˆ1é€šé“ç°åº¦å›¾åƒï¼‰
    net.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    net.fc = nn.Linear(net.fc.in_features, 10)  # MNIST 10 ç±»
    net.to(device)

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

    # 3. è®­ç»ƒè¿‡ç¨‹
    best_loss = float('inf')
    patience = 10  # æé«˜è€å¿ƒ
    patience_counter = 0

    start_time = time.time()
    print("Starting training with early stopping...")
    for epoch in range(800):  # å¯é€‚å½“å¢å¤§ epoch
        net.train()
        epoch_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            if i % 100 == 99:
                print(f"[{epoch + 1}, {i + 1:5d}] loss: {loss.item():.3f}")

        avg_loss = epoch_loss / len(trainloader)
        print(f"[{epoch+1}] Avg Loss: {avg_loss:.3f}")

        if avg_loss < best_loss - 1e-4:
            best_loss = avg_loss
            patience_counter = 0
        else:
            patience_counter += 1
            print(f"No improvement. Patience: {patience_counter}/{patience}")
            if patience_counter >= patience:
                print("Early stopping triggered.")
                break

    end_time = time.time()
    execution_time_minutes = (end_time - start_time) / 60
    print(f"âœ… Training completed in {execution_time_minutes:.2f} minutes.")

    # ä¿å­˜æ¨¡å‹
    torch.save(net.state_dict(), './resnet18_mnist.pth')

    # 4. æµ‹è¯•å‡†ç¡®ç‡
    correct = 0
    total = 0
    net.eval()
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f"Accuracy on test images: {100 * correct / total:.2f}%")

    # æ¯ç±»å‡†ç¡®ç‡
    class_correct = list(0. for _ in range(10))
    class_total = list(0. for _ in range(10))
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            c = (predicted == labels).squeeze()
            for i in range(len(labels)):
                class_correct[labels[i]] += c[i].item()
                class_total[labels[i]] += 1

    for i in range(10):
        print(f'Accuracy of {classes[i]:5s}: {100 * class_correct[i] / class_total[i]:.2f}%')

    # --- å¯è§†åŒ–é¢„æµ‹ ---

    def imshow_grid(images, labels, preds=None, classes=None, rows=8, cols=8):
        images = images.cpu() / 2 + 0.5  # unnormalize
        npimg = images.numpy()
        fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.5, rows * 1.5))
        for i in range(rows * cols):
            r, c = divmod(i, cols)
            ax = axes[r, c]
            img = np.transpose(npimg[i], (1, 2, 0))
            ax.imshow(img.squeeze(), cmap="gray")
            title = f'{classes[labels[i]]}'
            if preds is not None:
                title += f'\nâ†’ {classes[preds[i]]}'
            ax.set_title(title, fontsize=8)
            ax.axis('off')
        plt.tight_layout()
        plt.show()

    # è·å–ä¸€æ‰¹å›¾åƒç”¨äºæ˜¾ç¤º
    dataiter = iter(testloader)
    images, labels = next(dataiter)
    while images.size(0) < 64:
        more_images, more_labels = next(dataiter)
        images = torch.cat([images, more_images], dim=0)
        labels = torch.cat([labels, more_labels], dim=0)
    images = images[:64]
    labels = labels[:64]

    # é¢„æµ‹
    net.eval()
    with torch.no_grad():
        outputs = net(images.to(device))
        _, predicted = torch.max(outputs, 1)

    # æ˜¾ç¤ºå›¾åƒç½‘æ ¼
    imshow_grid(images, labels, predicted.cpu(), classes=classes, rows=8, cols=8)

if __name__ == "__main__":
    import torch.multiprocessing
    torch.multiprocessing.set_start_method('spawn', force=True)
    main()


```

å…¸å‹ç»“æœï¼šä½¿ç”¨ResNet18èƒ½åœ¨MNISTä¸Šè¾¾åˆ°99%ä»¥ä¸Šçš„å‡†ç¡®ç‡ã€‚è¯¥ä»»åŠ¡å±•ç¤ºäº†æ·±åº¦å·ç§¯ç½‘ç»œåœ¨å›¾åƒåˆ†ç±»ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚



**å¸¸è§é—®é¢˜è§£ç­”**

**Q: ä¸ºä»€ä¹ˆéœ€è¦ä¿®æ”¹ `net.conv1`ï¼Ÿ**

> ResNet18 é»˜è®¤è¾“å…¥ä¸º 3 é€šé“ï¼ˆRGBï¼‰ï¼Œè€Œ MNIST æ˜¯å•é€šé“ç°åº¦å›¾ã€‚å¿…é¡»å°†ç¬¬ä¸€ä¸ªå·ç§¯å±‚çš„è¾“å…¥é€šé“æ•°ä» 3 æ”¹ä¸º 1ï¼Œå¦åˆ™ä¼šæŠ¥ç»´åº¦ä¸åŒ¹é…é”™è¯¯ã€‚å…¶ä½™ç»“æ„ï¼ˆå¦‚ kernel size=7ã€stride=2 ç­‰ï¼‰ä¿æŒä¸å˜ä»¥ä¿ç•™åŸå§‹è®¾è®¡æ„å›¾ã€‚
>
> **`nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)`** è¿™ä¸€è¡Œå°±æ˜¯ä¿®æ”¹åŸæ¥ `ResNet18` ä¸­ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼ˆ`conv1`ï¼‰çš„å®šä¹‰ï¼Œä»¥ä½¿å…¶èƒ½æ¥æ”¶å•é€šé“çš„ç°åº¦å›¾åƒï¼ˆ`1` é€šé“ï¼‰ã€‚
>
> - **`1`**: è¾“å…¥å›¾åƒçš„é€šé“æ•°ï¼ˆå³ MNIST å›¾åƒçš„ç°åº¦é€šé“æ•°ï¼‰ã€‚
> - **`64`**: è¾“å‡ºçš„å·ç§¯é€šé“æ•°ï¼ˆResNet18 ä¸­é€šå¸¸æ˜¯ 64ï¼‰ã€‚
> - **`kernel_size=(7, 7)`**: å·ç§¯æ ¸çš„å¤§å°ä¸º 7x7ã€‚è¿™ä¸ªå€¼ä¸åŸå§‹ ResNet18 ä¸­çš„è®¾ç½®ä¸€è‡´ã€‚
> - **`stride=(2, 2)`**: å·ç§¯çš„æ­¥é•¿ä¸º 2ï¼Œè¿™æ„å‘³ç€æ¯æ¬¡å·ç§¯åï¼Œå›¾åƒå°ºå¯¸ä¼šå‡å°‘ä¸€åŠã€‚
> - **`padding=(3, 3)`**: å¡«å……ä¸º 3ï¼Œä¿æŒè¾“å…¥å›¾åƒçš„å°ºå¯¸åœ¨å·ç§¯åä¸è‡³äºå˜åŒ–å¤ªå¤§ï¼ˆç¡®ä¿å·ç§¯åè¾“å‡ºçš„ç©ºé—´ç»´åº¦é€‚å½“ï¼‰ã€‚
> - **`bias=False`**: é€šå¸¸åœ¨æ·±åº¦ç½‘ç»œä¸­ï¼Œå¦‚æœä½¿ç”¨äº†æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatchNormï¼‰ç­‰å±‚ï¼Œå·ç§¯å±‚å¯ä»¥å»æ‰åç½®é¡¹ã€‚

**Q: èƒ½å¦åœ¨ CPU ä¸Šè¿è¡Œï¼Ÿ**

> å¯ä»¥ã€‚è‹¥æ—  GPU æˆ– MPSï¼ˆApple Siliconï¼‰æ”¯æŒï¼Œç¨‹åºä¼šè‡ªåŠ¨å›é€€åˆ° CPUã€‚å¦‚éœ€å¼ºåˆ¶ä½¿ç”¨ CPUï¼ˆä¾‹å¦‚è°ƒè¯•æ—¶ï¼‰ï¼Œå¯è¿è¡Œï¼š
>
> ```bash
> CUDA_VISIBLE_DEVICES="" python mnist_resnet18.py
> ```



**æ€»ç»“**

æœ¬é¡¹ç›®å±•ç¤ºäº†å¦‚ä½•å°†ä¸ºè‡ªç„¶å›¾åƒè®¾è®¡çš„ç°ä»£ CNNï¼ˆå¦‚ ResNet18ï¼‰è¿ç§»åˆ°ç®€å•ä½†ç»å…¸çš„ MNIST ä»»åŠ¡ä¸Šã€‚é€šè¿‡**è¾“å…¥å±‚é€‚é…**ã€**åˆç†è®­ç»ƒç­–ç•¥**å’Œ**ç»“æœå¯è§†åŒ–**ï¼Œä¸ä»…éªŒè¯äº†æ¨¡å‹æœ‰æ•ˆæ€§ï¼Œä¹ŸåŠ æ·±äº†å¯¹ç¥ç»ç½‘ç»œå·¥ä½œåŸç†çš„ç†è§£ã€‚

> **æç¤º**ï¼šå¯¹äº MNIST è¿™ç±»ç®€å•ä»»åŠ¡ï¼Œå°å‹ CNNï¼ˆå¦‚ LeNet-5ï¼‰å·²è¶³å¤Ÿé«˜æ•ˆã€‚ä½¿ç”¨ ResNet18 æ›´å¤šæ˜¯ä¸ºäº†æ¼”ç¤ºæ¨¡å‹è¿ç§»ä¸é€‚é…æŠ€å·§ã€‚



### å®éªŒç»“æœ

#### åœ¨16GBå†…å­˜çš„ Mac mini è¿è¡Œ

> è¿è¡Œæœºå™¨
>
> <img src="https://raw.githubusercontent.com/GMyhf/img/main/img/202507261935350.png" alt="b452b39cfb47eb8bf5b640c828b6b71b" style="zoom:50%;" />
>
> 
>
> è¯¦ç»†è®­ç»ƒæ—¥å¿—
>
> ```
> /Users/hfyan/miniconda3/bin/python /Users/hfyan/git/2025spring-cs201/LLM/mnist_resnet18.py 
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [02:52<00:00, 57.6kB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 97.2kB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:04<00:00, 374kB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 6.74kB/s]
> Using device: mps
> Starting training with early stopping...
> /Users/hfyan/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.
>   warnings.warn(warn_msg)
> [1,   100] loss: 0.136
> [1,   200] loss: 0.132
> [1,   300] loss: 0.035
> [1,   400] loss: 0.098
> [1] Avg Loss: 0.150
> [2,   100] loss: 0.137
> [2,   200] loss: 0.030
> [2,   300] loss: 0.030
> [2,   400] loss: 0.015
> [2] Avg Loss: 0.052
> [3,   100] loss: 0.018
> [3,   200] loss: 0.105
> [3,   300] loss: 0.078
> [3,   400] loss: 0.026
> [3] Avg Loss: 0.039
> [4,   100] loss: 0.032
> [4,   200] loss: 0.056
> [4,   300] loss: 0.008
> [4,   400] loss: 0.013
> [4] Avg Loss: 0.031
> [5,   100] loss: 0.003
> [5,   200] loss: 0.025
> [5,   300] loss: 0.029
> [5,   400] loss: 0.022
> [5] Avg Loss: 0.027
> [6,   100] loss: 0.041
> [6,   200] loss: 0.022
> [6,   300] loss: 0.047
> [6,   400] loss: 0.005
> [6] Avg Loss: 0.023
> 
> ...
> 
> No improvement. Patience: 6/10
> [43,   100] loss: 0.010
> [43,   200] loss: 0.000
> [43,   300] loss: 0.012
> [43,   400] loss: 0.002
> [43] Avg Loss: 0.008
> No improvement. Patience: 7/10
> [44,   100] loss: 0.001
> [44,   200] loss: 0.004
> [44,   300] loss: 0.035
> [44,   400] loss: 0.000
> [44] Avg Loss: 0.011
> No improvement. Patience: 8/10
> [45,   100] loss: 0.002
> [45,   200] loss: 0.014
> [45,   300] loss: 0.010
> [45,   400] loss: 0.014
> [45] Avg Loss: 0.010
> No improvement. Patience: 9/10
> [46,   100] loss: 0.001
> [46,   200] loss: 0.076
> [46,   300] loss: 0.001
> [46,   400] loss: 0.004
> [46] Avg Loss: 0.009
> No improvement. Patience: 10/10
> Early stopping triggered.
> âœ… Training completed in 27.72 minutes.
> Accuracy on test images: 99.57%
> Accuracy of 0    : 99.59%
> Accuracy of 1    : 99.91%
> Accuracy of 2    : 99.71%
> Accuracy of 3    : 99.80%
> Accuracy of 4    : 99.49%
> Accuracy of 5    : 99.33%
> Accuracy of 6    : 99.37%
> Accuracy of 7    : 99.22%
> Accuracy of 8    : 99.90%
> Accuracy of 9    : 99.31%
> 
> Process finished with exit code 0
> 
> ```
> 
> 
> 
> <img src="https://raw.githubusercontent.com/GMyhf/img/main/img/202507261936331.png" alt="22485e1e277b7dfea954fe0cd8a1af4f" style="zoom:50%;" />
> 
> 



#### åœ¨16GBå†…å­˜çš„ window æœºå™¨è¿è¡Œ

> åœ¨windowæœºå™¨ï¼Œç”¨ WSL å®‰è£… Ubuntuï¼Œç”¨cpuè¿è¡Œ
>
> ç¯å¢ƒè®¾ç½®ï¼Œå¯ä»¥å‚è€ƒï¼šhttps://github.com/GMyhf/2025fall-cs201/blob/main/LLM/Build%20LLM%20Setup_window.md
>
> ```
> $ CUDA_VISIBLE_DEVICES="" python mnist_resnet18.py
> ```
>
> Windows 10 ä¸“ä¸šç‰ˆï¼Œç‰ˆæœ¬å·22H2ï¼Œå®‰è£…æ—¥æœŸ 2021/6/12
>
> å¤„ç†å™¨ Intel(R)Xeon(R)W-2223 CPU @ 3.60GHz 3.60GHz
> æœºå¸¦ RAM 16.0 GB (15.7 GB å¯ç”¨)
> ç³»ç»Ÿç±»å‹ 64 ä½æ“ä½œç³»ç»Ÿ, åŸºäº x64 çš„å¤„ç†å™¨
>
> ```
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:06<00:00, 1.52MB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 133kB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:01<00:00, 905kB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 9.23MB/s]
> Using device: cpu
> Starting training with early stopping...
> /home/yhf/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
>   warnings.warn(warn_msg)
> [1,   100] loss: 0.099
> [1,   200] loss: 0.072
> [1,   300] loss: 0.038
> [1,   400] loss: 0.079
> [1] Avg Loss: 0.149
> [2,   100] loss: 0.028
> [2,   200] loss: 0.010
> [2,   300] loss: 0.012
> [2,   400] loss: 0.112
> [2] Avg Loss: 0.053
> [3,   100] loss: 0.017
> [3,   200] loss: 0.032
> [3,   300] loss: 0.009
> [3,   400] loss: 0.027
> [3] Avg Loss: 0.037
> [4,   100] loss: 0.024
> [4,   200] loss: 0.087
> [4,   300] loss: 0.010
> [4,   400] loss: 0.045
> [4] Avg Loss: 0.032
> [5,   100] loss: 0.034
> [5,   200] loss: 0.009
> [5,   300] loss: 0.019
> [5,   400] loss: 0.038
> [5] Avg Loss: 0.026
> [6,   100] loss: 0.032
> [6,   200] loss: 0.046
> [6,   300] loss: 0.037
> [6,   400] loss: 0.039
> [6] Avg Loss: 0.023
> [7,   100] loss: 0.038
> 
> ...
> 
> [43] Avg Loss: 0.008
> No improvement. Patience: 5/10
> [44,   100] loss: 0.046
> [44,   200] loss: 0.002
> [44,   300] loss: 0.023
> [44,   400] loss: 0.008
> [44] Avg Loss: 0.010
> No improvement. Patience: 6/10
> [45,   100] loss: 0.021
> [45,   200] loss: 0.001
> [45,   300] loss: 0.006
> [45,   400] loss: 0.001
> [45] Avg Loss: 0.011
> No improvement. Patience: 7/10
> [46,   100] loss: 0.008
> [46,   200] loss: 0.006
> [46,   300] loss: 0.002
> [46,   400] loss: 0.049
> [46] Avg Loss: 0.010
> No improvement. Patience: 8/10
> [47,   100] loss: 0.001
> [47,   200] loss: 0.015
> [47,   300] loss: 0.001
> [47,   400] loss: 0.026
> [47] Avg Loss: 0.008
> No improvement. Patience: 9/10
> [48,   100] loss: 0.002
> [48,   200] loss: 0.002
> [48,   300] loss: 0.002
> [48,   400] loss: 0.004
> [48] Avg Loss: 0.010
> No improvement. Patience: 10/10
> Early stopping triggered.
> âœ… Training completed in 75.11 minutes.
> Accuracy on test images: 99.35%
> Accuracy of 0    : 99.59%
> Accuracy of 1    : 99.91%
> Accuracy of 2    : 99.22%
> Accuracy of 3    : 99.50%
> Accuracy of 4    : 99.59%
> Accuracy of 5    : 99.22%
> Accuracy of 6    : 99.37%
> Accuracy of 7    : 99.22%
> Accuracy of 8    : 99.38%
> Accuracy of 9    : 98.41%
> /home/yhf/NNCode/mnist_resnet18.py:142: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
> plt.show()
> ```
> 
> 



#### åœ¨32GBå†…å­˜çš„ clab äº‘è™šæ‹Ÿæœºè¿è¡Œ

2025å¹´11æœˆ26æ—¥ï¼Œåœ¨clabäº‘è™šæ‹Ÿæœºè·‘ã€‚è™šæ‹Ÿæœºåªæœ‰CPUã€‚

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/33caace9ba4252b9bcc6707b67f18746.png" alt="33caace9ba4252b9bcc6707b67f18746" style="zoom: 33%;" />



clabè™šæ‹Ÿæœºéœ€è¦ç™»å½•ç½‘å…³ï¼Œèƒ½è®¿é—®å¤–ç½‘ï¼Œå› ä¸ºè¦ä¸‹è½½æ•°æ®

> å¦åˆ™ï¼ŒæŠ¥302é”™è¯¯
>
> (.venv) [rocky@jensen AI_literacy]$ python MNIST_nn.py 
> Traceback (most recent call last):
> File "/home/rocky/AI_literacy/MNIST_nn.py", line 166, in <module>
> main()
> File "/home/rocky/AI_literacy/MNIST_nn.py", line 25, in main
> trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)
> File "/home/rocky/AI_literacy/.venv/lib64/python3.9/site-packages/torchvision/datasets/mnist.py", line 100, in __init__
>
> self.download()
>
> File "/home/rocky/AI_literacy/.venv/lib64/python3.9/site-packages/torchvision/datasets/mnist.py", line 197, in download
> raise RuntimeError(s)
> RuntimeError: Error downloading train-images-idx3-ubyte.gz:
> Tried https://ossci-datasets.s3.amazonaws.com/mnist/, got:
> <urlopen error [Errno 110] Connection timed out>
> Tried http://yann.lecun.com/exdb/mnist/, got:
> HTTP Error 302: Moved Temporarily



> 
>
> ```
> (.venv) [rocky@jensen AI_literacy]$ python MNIST_nn.py 
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [06:09<00:00, 26.8kB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 113kB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:01<00:00, 1.12MB/s]
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 4.41MB/s]
> Using device: cpu
> Starting training with early stopping...
> /home/rocky/AI_literacy/.venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
>   warnings.warn(warn_msg)
> [1,   100] loss: 0.200
> [1,   200] loss: 0.084
> [1,   300] loss: 0.060
> [1,   400] loss: 0.100
> [1] Avg Loss: 0.150
> [2,   100] loss: 0.095
> [2,   200] loss: 0.102
> [2,   300] loss: 0.010
> [2,   400] loss: 0.032
> [2] Avg Loss: 0.049
> [3,   100] loss: 0.024
> [3,   200] loss: 0.051
> [3,   300] loss: 0.064
> [3,   400] loss: 0.019
> [3] Avg Loss: 0.039
> [4,   100] loss: 0.040
> [4,   200] loss: 0.071
> [4,   300] loss: 0.068
> [4,   400] loss: 0.052
> [4] Avg Loss: 0.030
> [5,   100] loss: 0.039
> [5,   200] loss: 0.031
> [5,   300] loss: 0.005
> [5,   400] loss: 0.002
> [5] Avg Loss: 0.025
> [6,   100] loss: 0.081
> [6,   200] loss: 0.019
> [6,   300] loss: 0.040
> [6,   400] loss: 0.006
> [6] Avg Loss: 0.023
> 
> ...
> 
> [40,   200] loss: 0.001
> [40,   300] loss: 0.019
> [40,   400] loss: 0.076
> [40] Avg Loss: 0.010
> No improvement. Patience: 4/10
> [41,   100] loss: 0.001
> [41,   200] loss: 0.012
> [41,   300] loss: 0.014
> [41,   400] loss: 0.009
> [41] Avg Loss: 0.012
> No improvement. Patience: 5/10
> [42,   100] loss: 0.003
> [42,   200] loss: 0.002
> [42,   300] loss: 0.053
> [42,   400] loss: 0.001
> [42] Avg Loss: 0.010
> No improvement. Patience: 6/10
> [43,   100] loss: 0.011
> [43,   200] loss: 0.000
> [43,   300] loss: 0.006
> [43,   400] loss: 0.005
> [43] Avg Loss: 0.008
> No improvement. Patience: 7/10
> [44,   100] loss: 0.013
> [44,   200] loss: 0.030
> [44,   300] loss: 0.021
> [44,   400] loss: 0.004
> [44] Avg Loss: 0.010
> No improvement. Patience: 8/10
> [45,   100] loss: 0.003
> [45,   200] loss: 0.012
> [45,   300] loss: 0.002
> [45,   400] loss: 0.037
> [45] Avg Loss: 0.008
> No improvement. Patience: 9/10
> [46,   100] loss: 0.001
> [46,   200] loss: 0.006
> [46,   300] loss: 0.000
> [46,   400] loss: 0.020
> [46] Avg Loss: 0.010
> No improvement. Patience: 10/10
> Early stopping triggered.
> âœ… Training completed in 91.75 minutes.
> Accuracy on test images: 99.43%
> Accuracy of 0    : 99.80%
> Accuracy of 1    : 99.65%
> Accuracy of 2    : 99.03%
> Accuracy of 3    : 99.70%
> Accuracy of 4    : 99.19%
> Accuracy of 5    : 98.77%
> Accuracy of 6    : 99.37%
> Accuracy of 7    : 99.71%
> Accuracy of 8    : 99.38%
> Accuracy of 9    : 99.60%
> ```
> 
> 



## 4.4 åŸºäº ResNet18 çš„ CIFAR-10 å›¾åƒåˆ†ç±»

æœ¬é¡¹ç›®æ—¨åœ¨å¤ç°å¹¶æ”¹è¿› èœé¸Ÿæ•™ç¨‹ï¼ˆhttps://www.runoob.com/pytorch/pytorch-image-classification.htmlï¼‰ä¸­çš„ CIFAR-10 åˆ†ç±»ç¤ºä¾‹ã€‚åŸå§‹å®ç°ä»…è¾¾åˆ°çº¦ **76.8%** çš„æµ‹è¯•å‡†ç¡®ç‡ï¼Œä½äºç»å…¸åŸºçº¿ï¼ˆå¦‚ cuda-convnet æŠ¥å‘Šçš„ **82%+**ï¼‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥ **ResNet18 æ¶æ„**ã€**å¢å¼ºçš„æ•°æ®é¢„å¤„ç†ç­–ç•¥** å’Œ **å­¦ä¹ ç‡è°ƒåº¦æœºåˆ¶**ï¼Œæ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚

æœ€ç»ˆåœ¨ Mac Studioï¼ˆM1 Ultraï¼‰ä¸ Clab äº‘æœåŠ¡å™¨ä¸Šå‡å®ç°äº† **>83.5%** çš„æµ‹è¯•å‡†ç¡®ç‡ï¼ŒæˆåŠŸè¶…è¶ŠåŸºç¡€ CNN åŸºçº¿ã€‚



### æ•°æ®é›†ç®€ä»‹ï¼šCIFAR-10

CIFAR-10 æ˜¯ç”± Alex Krizhevsky ç­‰äººæ„å»ºçš„ç»å…¸å›¾åƒåˆ†ç±»æ•°æ®é›†ï¼Œæºè‡ª 80 Million Tiny Imagesï¼ˆhttp://people.csail.mit.edu/torralba/tinyimages/ï¼‰ã€‚

- **å›¾åƒæ•°é‡**ï¼š60,000 å¼  32Ã—32 å½©è‰²å›¾åƒ 
- **ç±»åˆ«æ•°**ï¼š10 ç±»ï¼ˆäº’æ–¥ï¼‰ 
  - airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck
- **åˆ’åˆ†**ï¼š
  - è®­ç»ƒé›†ï¼š50,000 å¼ ï¼ˆæ¯ç±» 5,000ï¼‰
  - æµ‹è¯•é›†ï¼š10,000 å¼ ï¼ˆæ¯ç±» 1,000ï¼Œå‡åŒ€é‡‡æ ·ï¼‰

> å®˜ç½‘ï¼šhttps://www.cs.toronto.edu/~kriz/cifar.html

| Class      | ç¤ºä¾‹å›¾åƒ                                                     |
| ---------- | ------------------------------------------------------------ |
| airplane   | ![airplane](https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane1.png) â€¦ |
| automobile | ![automobile](https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile1.png) â€¦ |
| ...        | ï¼ˆå…¶ä½™ç•¥ï¼Œè¯¦è§å®˜ç½‘ï¼‰                                         |

> æ³¨æ„ï¼šâ€œautomobileâ€ æŒ‡è½¿è½¦/SUVï¼Œâ€œtruckâ€ ä»…æŒ‡å¤§å‹å¡è½¦ï¼Œä¸¤è€…æ— é‡å ã€‚
>
> **Baseline results**
>
> You can find some baseline replicable results on this dataset [on the project page for cuda-convnet](http://code.google.com/p/cuda-convnet/). These results were obtained with a convolutional neural network. Briefly, they are **18%** test error without data augmentation and 11% with. Additionally, [Jasper Snoek](http://www.cs.toronto.edu/~jasper/) has a [new paper](http://hips.seas.harvard.edu/content/practical-bayesian-optimization-machine-learning-algorithms) in which he used Bayesian hyperparameter optimization to find nice settings of the weight decay and other hyperparameters, which allowed him to obtain a test error rate of 15% (without data augmentation) using the architecture of the net that got 18%.

------



### æ”¹è¿›æªæ–½

**ç°ä»£ç½‘ç»œæ¶æ„ï¼ˆResNet18ï¼‰ + å……åˆ†çš„æ•°æ®å¢å¼º + å­¦ä¹ ç‡è°ƒåº¦**

| æ”¹è¿›é¡¹         | å…·ä½“å®ç°                                                     |
| -------------- | ------------------------------------------------------------ |
| **ç½‘ç»œæ¶æ„**   | ä½¿ç”¨ `torchvision.models.resnet18(weights=None)`ï¼Œä»å¤´è®­ç»ƒ   |
| **è¾“å…¥å°ºå¯¸**   | ä¿ç•™åŸå§‹ 32Ã—32ï¼ˆæ— éœ€ Resizeï¼Œå› æœªåŠ è½½ ImageNet é¢„è®­ç»ƒæƒé‡ï¼‰  |
| **æ•°æ®å¢å¼º**   | `RandomCrop`, `HorizontalFlip`, `RandomRotation`, `ColorJitter` |
| **ä¼˜åŒ–å™¨**     | SGD + momentum=0.9 + weight_decay=5e-4                       |
| **å­¦ä¹ ç‡è°ƒåº¦** | `CosineAnnealingLR`ï¼ˆå¹³æ»‘è¡°å‡ï¼‰                              |
| **è®­ç»ƒæ§åˆ¶**   | Early stoppingï¼ˆpatience=10ï¼‰é˜²æ­¢è¿‡æ‹Ÿåˆ                      |
| **è¯„ä¼°æŒ‡æ ‡**   | æ€»ä½“å‡†ç¡®ç‡ + æ¯ç±»å‡†ç¡®ç‡ + å¯è§†åŒ–é¢„æµ‹ç½‘æ ¼                     |



å®Œæ•´ `image_classification-ResNet18-RandomCropFlipLR_Cosine.py`ä»£ç 

```python
import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import matplotlib.pyplot as plt
import numpy as np
import time

def main():
    # === 1. æ•°æ®å¢å¼ºä¸åŠ è½½ ===
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),  # éšæœºæ—‹è½¬
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # è‰²å½©è°ƒæ•´
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)

    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2, pin_memory=True)

    classes = ('plane', 'car', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck')

    # === 2. æ¨¡å‹ä¸è®­ç»ƒé…ç½® ===
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print("Using device:", device)

    # åŠ è½½é¢„å®šä¹‰çš„ ResNet18 å¹¶ä¿®æ”¹è¾“å‡ºå±‚
    net = models.resnet18(weights=None)
    net.fc = nn.Linear(net.fc.in_features, 10)  # CIFAR10 10 ç±»
    net.to(device)

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

    # === 3. è®­ç»ƒå¾ªç¯ï¼ˆå« Early Stoppingï¼‰===
    best_loss = float('inf')
    patience = 10 # æé«˜è€å¿ƒ
    patience_counter = 0

    start_time = time.time()
    print("Starting training with early stopping...")
    for epoch in range(800):  # å¯é€‚å½“å¢å¤§ epoch
        net.train()
        epoch_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            if i % 100 == 99:
                print(f"[{epoch + 1}, {i + 1:5d}] loss: {loss.item():.3f}")

        avg_loss = epoch_loss / len(trainloader)
        print(f"[{epoch+1}] Avg Loss: {avg_loss:.3f}")

        if avg_loss < best_loss - 1e-4:
            best_loss = avg_loss
            patience_counter = 0
        else:
            patience_counter += 1
            print(f"No improvement. Patience: {patience_counter}/{patience}")
            if patience_counter >= patience:
                print("Early stopping triggered.")
                break



    end_time = time.time()
    execution_time_minutes = (end_time - start_time) / 60
    print(f"âœ… Training completed in {execution_time_minutes:.2f} minutes.")


    # ä¿å­˜æ¨¡å‹
    torch.save(net.state_dict(), './resnet18_cifar10_data_augument.pth')

    # === 4. æµ‹è¯•è¯„ä¼° ===
    correct = 0
    total = 0
    net.eval()
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f"Accuracy on test images: {100 * correct / total:.2f}%")

    # æ¯ç±»å‡†ç¡®ç‡
    class_correct = list(0. for _ in range(10))
    class_total = list(0. for _ in range(10))
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            c = (predicted == labels).squeeze()
            for i in range(len(labels)):
                class_correct[labels[i]] += c[i].item()
                class_total[labels[i]] += 1

    for i in range(10):
        print(f'Accuracy of {classes[i]:5s}: {100 * class_correct[i] / class_total[i]:.2f}%')

    # === 5. å¯è§†åŒ–é¢„æµ‹ç»“æœ ===
    def imshow_grid(images, labels, preds=None, classes=None, rows=8, cols=8):
        images = images.cpu() / 2 + 0.5  # unnormalize
        npimg = images.numpy()
        fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.5, rows * 1.5))
        for i in range(rows * cols):
            r, c = divmod(i, cols)
            ax = axes[r, c]
            img = np.transpose(npimg[i], (1, 2, 0))
            ax.imshow(img)
            title = f'{classes[labels[i]]}'
            if preds is not None:
                title += f'\nâ†’ {classes[preds[i]]}'
            ax.set_title(title, fontsize=8)
            ax.axis('off')
        plt.tight_layout()
        plt.show()

    # è·å–ä¸€æ‰¹å›¾åƒç”¨äºæ˜¾ç¤º
    dataiter = iter(testloader)
    images, labels = next(dataiter)
    while images.size(0) < 64:
        more_images, more_labels = next(dataiter)
        images = torch.cat([images, more_images], dim=0)
        labels = torch.cat([labels, more_labels], dim=0)
    images = images[:64]
    labels = labels[:64]

    # é¢„æµ‹
    net.eval()
    with torch.no_grad():
        outputs = net(images.to(device))
        _, predicted = torch.max(outputs, 1)

    # æ˜¾ç¤ºå›¾åƒç½‘æ ¼
    imshow_grid(images, labels, predicted.cpu(), classes=classes, rows=8, cols=8)

if __name__ == "__main__":
    import torch.multiprocessing
    torch.multiprocessing.set_start_method('spawn', force=True)
    main()

```



### å®éªŒç»“æœ

#### åœ¨ Apple M1 Ultra (Mac Studio, 64GB RAM)

- **è®¾å¤‡**ï¼š`mps`
- **è®­ç»ƒè€—æ—¶**ï¼š79.91 åˆ†é’Ÿ
- **æ—©åœè½®æ¬¡**ï¼šç¬¬ 208 è½®
- **æµ‹è¯•å‡†ç¡®ç‡**ï¼š**83.57%**

| ç±»åˆ«    | å‡†ç¡®ç‡              |
| ------- | ------------------- |
| plane   | 83.70%              |
| car     | 92.20%              |
| bird    | 78.70%              |
| **cat** | **60.40%** â† æœ€å¼±é¡¹ |
| deer    | 79.30%              |
| dog     | 77.40%              |
| frog    | 90.30%              |
| horse   | 92.50%              |
| ship    | 88.50%              |
| truck   | 92.70%              |

> é¢„æµ‹å¯è§†åŒ–ï¼š
> <img src="https://raw.githubusercontent.com/GMyhf/img/main/img/202507280020181.jpg" alt="é¢„æµ‹ç»“æœ" style="zoom: 50%;" />

> è¿è¡Œè¾“å‡ºç»“æœå¦‚ä¸‹ï¼š
>
> ```
> /Users/hfyan/miniconda3/bin/python /Users/hfyan/Desktop/LLMs-from-scratch-main/runoob/pytorch-image-classification/image_classification-ResNet18-RandomCropFlipLR_Cosine.py 
> Using device: mps
> Starting training with early stopping...
> [1,   100] loss: 1.752
> [1,   200] loss: 1.675
> [1,   300] loss: 1.654
> [1] Avg Loss: 1.806
> [2,   100] loss: 1.497
> [2,   200] loss: 1.459
> [2,   300] loss: 1.453
> [2] Avg Loss: 1.520
> [3,   100] loss: 1.534
> [3,   200] loss: 1.383
> [3,   300] loss: 1.167
> [3] Avg Loss: 1.372
> [4,   100] loss: 1.390
> [4,   200] loss: 1.221
> [4,   300] loss: 1.238
> [4] Avg Loss: 1.244
> [5,   100] loss: 1.089
> [5,   200] loss: 1.020
> [5,   300] loss: 1.133
> [5] Avg Loss: 1.159
> ......
> No improvement. Patience: 1/10
> [192,   100] loss: 0.187
> [192,   200] loss: 0.293
> [192,   300] loss: 0.356
> [192] Avg Loss: 0.302
> [193,   100] loss: 0.223
> [193,   200] loss: 0.348
> [193,   300] loss: 0.309
> [193] Avg Loss: 0.301
> [194,   100] loss: 0.303
> [194,   200] loss: 0.219
> [194,   300] loss: 0.280
> [194] Avg Loss: 0.304
> No improvement. Patience: 1/10
> [195,   100] loss: 0.279
> [195,   200] loss: 0.296
> [195,   300] loss: 0.313
> [195] Avg Loss: 0.296
> [196,   100] loss: 0.254
> [196,   200] loss: 0.385
> [196,   300] loss: 0.280
> [196] Avg Loss: 0.300
> No improvement. Patience: 1/10
> [197,   100] loss: 0.216
> [197,   200] loss: 0.298
> [197,   300] loss: 0.290
> [197] Avg Loss: 0.298
> No improvement. Patience: 2/10
> [198,   100] loss: 0.267
> [198,   200] loss: 0.218
> [198,   300] loss: 0.367
> [198] Avg Loss: 0.290
> [199,   100] loss: 0.270
> [199,   200] loss: 0.240
> [199,   300] loss: 0.351
> [199] Avg Loss: 0.301
> No improvement. Patience: 1/10
> [200,   100] loss: 0.251
> [200,   200] loss: 0.227
> [200,   300] loss: 0.302
> [200] Avg Loss: 0.299
> No improvement. Patience: 2/10
> [201,   100] loss: 0.348
> [201,   200] loss: 0.301
> [201,   300] loss: 0.193
> [201] Avg Loss: 0.299
> No improvement. Patience: 3/10
> [202,   100] loss: 0.313
> [202,   200] loss: 0.329
> [202,   300] loss: 0.305
> [202] Avg Loss: 0.295
> No improvement. Patience: 4/10
> [203,   100] loss: 0.266
> [203,   200] loss: 0.254
> [203,   300] loss: 0.307
> [203] Avg Loss: 0.294
> No improvement. Patience: 5/10
> [204,   100] loss: 0.372
> [204,   200] loss: 0.295
> [204,   300] loss: 0.348
> [204] Avg Loss: 0.300
> No improvement. Patience: 6/10
> [205,   100] loss: 0.392
> [205,   200] loss: 0.353
> [205,   300] loss: 0.306
> [205] Avg Loss: 0.296
> No improvement. Patience: 7/10
> [206,   100] loss: 0.262
> [206,   200] loss: 0.213
> [206,   300] loss: 0.396
> [206] Avg Loss: 0.293
> No improvement. Patience: 8/10
> [207,   100] loss: 0.293
> [207,   200] loss: 0.204
> [207,   300] loss: 0.337
> [207] Avg Loss: 0.291
> No improvement. Patience: 9/10
> [208,   100] loss: 0.413
> [208,   200] loss: 0.294
> [208,   300] loss: 0.315
> [208] Avg Loss: 0.295
> No improvement. Patience: 10/10
> Early stopping triggered.
> âœ… Training completed in 79.91 minutes.
> Accuracy on test images: 83.57%
> Accuracy of plane: 83.70%
> Accuracy of car  : 92.20%
> Accuracy of bird : 78.70%
> Accuracy of cat  : 60.40%
> Accuracy of deer : 79.30%
> Accuracy of dog  : 77.40%
> Accuracy of frog : 90.30%
> Accuracy of horse: 92.50%
> Accuracy of ship : 88.50%
> Accuracy of truck: 92.70%
> 
> Process finished with exit code 0
> ```
>
> 



#### åœ¨ Clab äº‘æœåŠ¡å™¨ (32 vCPU, 32GB RAM)

- **è®¾å¤‡**ï¼š`cpu`
- **è®­ç»ƒè€—æ—¶**ï¼š636.54 åˆ†é’Ÿï¼ˆçº¦ 10.6 å°æ—¶ï¼‰
- **æ—©åœè½®æ¬¡**ï¼šç¬¬ 277 è½®
- **æµ‹è¯•å‡†ç¡®ç‡**ï¼š**83.67%**

| ç±»åˆ«    | å‡†ç¡®ç‡                |
| ------- | --------------------- |
| plane   | 84.70%                |
| car     | 91.90%                |
| bird    | 84.00%                |
| **cat** | **65.90%**** â† æœ€å¼±é¡¹ |
| deer    | 81.10%                |
| dog     | 77.60%                |
| frog    | 89.40%                |
| horse   | 85.00%                |
| ship    | 91.70%                |
| truck   | 85.40%                |

> é¢„æµ‹å¯è§†åŒ–ï¼š
> <img src="https://raw.githubusercontent.com/GMyhf/img/main/img/b353056ebdc1085718fc43ab6d2a1316.png" alt="äº‘æœåŠ¡å™¨ç»“æœ" style="zoom:50%;" />



> ç»“æœå¦‚ä¸‹ï¼š
>
> ```
> /home/rocky/AI_literacy/.venv/bin/python /home/rocky/AI_literacy/CIFAR-10_nn.py 
> 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:15<00:00, 11.1MB/s]
> /usr/lib64/python3.9/tarfile.py:2288: RuntimeWarning: The default behavior of tarfile extraction has been changed to disallow common exploits (including CVE-2007-4559). By default, absolute/parent paths are disallowed and some mode bits are cleared. See https://access.redhat.com/articles/7004769 for more details.
>   warnings.warn(
> Using device: cpu
> Starting training with early stopping...
> /home/rocky/AI_literacy/.venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
>   warnings.warn(warn_msg)
> [1,   100] loss: 1.893
> [1,   200] loss: 1.658
> [1,   300] loss: 1.698
> [1] Avg Loss: 1.804
> [2,   100] loss: 1.399
> [2,   200] loss: 1.580
> [2,   300] loss: 1.407
> [2] Avg Loss: 1.517
> [3,   100] loss: 1.346
> [3,   200] loss: 1.492
> [3,   300] loss: 1.097
> [3] Avg Loss: 1.373
> [4,   100] loss: 1.309
> [4,   200] loss: 1.182
> [4,   300] loss: 1.187
> [4] Avg Loss: 1.244
> [5,   100] loss: 1.104
> [5,   200] loss: 1.084
> [5,   300] loss: 1.081
> [5] Avg Loss: 1.151
> ......
> [267,   100] loss: 0.278
> [267,   200] loss: 0.226
> [267,   300] loss: 0.188
> [267] Avg Loss: 0.249
> [268,   100] loss: 0.218
> [268,   200] loss: 0.220
> [268,   300] loss: 0.206
> [268] Avg Loss: 0.256
> No improvement. Patience: 1/10
> [269,   100] loss: 0.200
> [269,   200] loss: 0.175
> [269,   300] loss: 0.298
> [269] Avg Loss: 0.256
> No improvement. Patience: 2/10
> [270,   100] loss: 0.325
> [270,   200] loss: 0.265
> [270,   300] loss: 0.319
> [270] Avg Loss: 0.258
> No improvement. Patience: 3/10
> [271,   100] loss: 0.316
> [271,   200] loss: 0.139
> [271,   300] loss: 0.263
> [271] Avg Loss: 0.251
> No improvement. Patience: 4/10
> [272,   100] loss: 0.260
> [272,   200] loss: 0.184
> [272,   300] loss: 0.141
> [272] Avg Loss: 0.256
> No improvement. Patience: 5/10
> [273,   100] loss: 0.263
> [273,   200] loss: 0.327
> [273,   300] loss: 0.246
> [273] Avg Loss: 0.250
> No improvement. Patience: 6/10
> [274,   100] loss: 0.193
> [274,   200] loss: 0.221
> [274,   300] loss: 0.227
> [274] Avg Loss: 0.250
> No improvement. Patience: 7/10
> [275,   100] loss: 0.185
> [275,   200] loss: 0.354
> [275,   300] loss: 0.254
> [275] Avg Loss: 0.258
> No improvement. Patience: 8/10
> [276,   100] loss: 0.354
> [276,   200] loss: 0.326
> [276,   300] loss: 0.246
> [276] Avg Loss: 0.250
> No improvement. Patience: 9/10
> [277,   100] loss: 0.183
> [277,   200] loss: 0.383
> [277,   300] loss: 0.295
> [277] Avg Loss: 0.256
> No improvement. Patience: 10/10
> Early stopping triggered.
> âœ… Training completed in 636.54 minutes.
> Accuracy on test images: 83.67%
> Accuracy of plane: 84.70%
> Accuracy of car  : 91.90%
> Accuracy of bird : 84.00%
> Accuracy of cat  : 65.90%
> Accuracy of deer : 81.10%
> Accuracy of dog  : 77.60%
> Accuracy of frog : 89.40%
> Accuracy of horse: 85.00%
> Accuracy of ship : 91.70%
> Accuracy of truck: 85.40%
> 
> Process finished with exit code 0
> 
> ```
>
> 
>
> ![f44849630ac92c19ff8f0b2e922801ab](https://raw.githubusercontent.com/GMyhf/img/main/img/f44849630ac92c19ff8f0b2e922801ab.png)
>
> 



### å¸¸è§é—®é¢˜è§£ç­”ï¼ˆFAQï¼‰

**Q1: ä¸ºä»€ä¹ˆä¸ç”¨ `transforms.Resize(224)`ï¼Ÿ**

- **å…³é”®ç‚¹**ï¼šæ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ã€‚
  - è‹¥ `weights=None`ï¼ˆä»å¤´è®­ç»ƒï¼‰â†’ **ä¸éœ€è¦ Resize**ï¼Œå¯ç›´æ¥ç”¨ 32Ã—32 è¾“å…¥ã€‚
  - è‹¥ `weights=ResNet18_Weights.DEFAULT` â†’ å¿…é¡» Resize åˆ° â‰¥224Ã—224ï¼Œæˆ–ä¿®æ”¹é¦–å±‚å·ç§¯æ ¸ã€‚

> æœ¬é¡¹ç›®é€‰æ‹©ä»å¤´è®­ç»ƒï¼Œæ•…ä¿ç•™åŸå§‹åˆ†è¾¨ç‡ï¼Œé¿å…ä¿¡æ¯å¤±çœŸã€‚

------

**Q2: å¦‚ä½•è§£è¯» `_, predicted = torch.max(outputs, 1)`ï¼Ÿ**

- `outputs`ï¼šå½¢çŠ¶ä¸º `(N, 10)` çš„ logits å¼ é‡ã€‚
- `torch.max(..., dim=1)`ï¼šæ²¿ç±»åˆ«ç»´åº¦ï¼ˆdim=1ï¼‰æ‰¾æœ€å¤§å€¼ã€‚
  - è¿”å› `(values, indices)`
  - `_` å¿½ç•¥æœ€å¤§å€¼ï¼Œ`predicted` ä¿ç•™ç±»åˆ«ç´¢å¼•ï¼ˆ0~9ï¼‰

> ç¤ºä¾‹ï¼šè‹¥è¾“å‡º `[2.1, 4.2, ..., 3.3]`ï¼Œåˆ™ `predicted = 1`ï¼ˆå¯¹åº” "automobile"ï¼‰

------

**Q3: æ¯ç±»å‡†ç¡®ç‡æ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Ÿ**

é€šè¿‡éå†æµ‹è¯•é›†ï¼Œå¯¹æ¯ä¸ªæ ·æœ¬ï¼š

1. åˆ¤æ–­ `predicted == label`
2. ç´¯åŠ åˆ°å¯¹åº”ç±»åˆ«çš„ `class_correct[label]`
3. åŒæ—¶ç´¯åŠ  `class_total[label]`

æœ€ç»ˆï¼š`accuracy[i] = class_correct[i] / class_total[i]`

> è¿™èƒ½æ­ç¤ºæ¨¡å‹å¼±ç‚¹ï¼ˆå¦‚â€œçŒ«â€ç±»å‡†ç¡®ç‡æ˜¾è‘—åä½ï¼‰ã€‚



### ä¸ Baseline å¯¹æ¯”æ€»ç»“

| æ–¹æ³•                          | å‡†ç¡®ç‡     | æ˜¯å¦è¶…è¶Š 82%ï¼Ÿ |
| ----------------------------- | ---------- | -------------- |
| åŸå§‹ç®€æ˜“ CNN                  | 76.77%     | âŒ              |
| **æœ¬é¡¹ç›®ï¼ˆResNet18 + å¢å¼ºï¼‰** | **83.57%** | âœ…              |
| cuda-convnetï¼ˆæ— å¢å¼ºï¼‰        | 82%        | â€”              |
| cuda-convnetï¼ˆæœ‰å¢å¼ºï¼‰        | 89%        | âš ï¸ ä»æœ‰å·®è·     |

> ä¸‹ä¸€æ­¥å¯å°è¯•ï¼šMixUpã€Cutoutã€Label Smoothingã€æ›´å¤§æ¨¡å‹ï¼ˆå¦‚ ResNet50ï¼‰ã€è¿ç§»å­¦ä¹ ç­‰ã€‚

------



> é™„å½•ï¼šImageNet Top-5 é”™è¯¯ç‡æ¼”è¿›ï¼ˆå‚è€ƒï¼‰
>
> ![ImageNet Top-5 Error](https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250726164756658.png)
>
> - **2012 AlexNet**ï¼šå¼€å¯æ·±åº¦å­¦ä¹ æ—¶ä»£ï¼ˆ16.4% < 26%ï¼‰
> - **2015 ResNet**ï¼šé¦–æ¬¡è¶…è¶Šäººç±»ï¼ˆ3.6% < 5.0%ï¼‰
> - **Top-5 å®šä¹‰**ï¼šåªè¦çœŸå®æ ‡ç­¾åœ¨æ¨¡å‹é¢„æµ‹çš„å‰5åä¸­ï¼Œå³è§†ä¸ºæ­£ç¡®ã€‚



### ç»“è®º

é€šè¿‡åˆç†ä½¿ç”¨ **ç°ä»£ç½‘ç»œæ¶æ„ï¼ˆResNet18ï¼‰** + **å……åˆ†çš„æ•°æ®å¢å¼º** + **å­¦ä¹ ç‡è°ƒåº¦**ï¼Œæˆ‘ä»¬åœ¨ CIFAR-10 ä¸Šè½»æ¾è¶…è¶Šäº†æ—©æœŸ CNN åŸºçº¿ï¼ŒéªŒè¯äº†â€œæ›´å¼ºæ¨¡å‹ + æ›´å¥½è®­ç»ƒç­–ç•¥ = æ›´é«˜æ€§èƒ½â€çš„åŸåˆ™ã€‚





## 4.5 åŸºäº ResNet50 çš„ Tiny ImageNet å›¾åƒåˆ†ç±»

æœ¬é¡¹ç›®æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ **PyTorch** å’Œ **torchvision** åœ¨ **Tiny ImageNet** æ•°æ®é›†ä¸Šå¾®è°ƒ **é¢„è®­ç»ƒ ResNet50 æ¨¡å‹**ï¼Œå®Œæˆå›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚å°½ç®¡ Tiny ImageNetï¼ˆçº¦ 500MBï¼‰è¿œå°äºå®Œæ•´ ImageNetï¼ˆ150GB+ï¼‰ï¼Œä½†ä»éœ€åˆç†é…ç½®æ•°æ®è·¯å¾„ã€æ‰¹å¤§å°ï¼ˆbatch sizeï¼‰å’Œè®¾å¤‡èµ„æºã€‚æˆ‘ä»¬åœ¨ Apple Siliconï¼ˆMPS åç«¯ï¼‰ä¸ŠæˆåŠŸå®Œæˆ 25 è½®è®­ç»ƒï¼ŒéªŒè¯å‡†ç¡®ç‡è¾¾ **80.14%**ï¼Œè€—æ—¶çº¦ 4.5 å°æ—¶ã€‚



### 1. èƒŒæ™¯çŸ¥è¯†

#### 1.1 ä¸ºä»€ä¹ˆé€‰æ‹© Tiny ImageNetï¼Ÿ

| ç‰¹æ€§     | ImageNet                 | Tiny ImageNet     |
| -------- | ------------------------ | ----------------- |
| ç±»åˆ«æ•°   | 1,000                    | 200               |
| è®­ç»ƒå›¾åƒ | ~128 ä¸‡å¼                 | 10 ä¸‡å¼ ï¼ˆ500/ç±»ï¼‰ |
| éªŒè¯å›¾åƒ | 5 ä¸‡å¼                    | 1 ä¸‡å¼ ï¼ˆ50/ç±»ï¼‰   |
| å›¾åƒå°ºå¯¸ | é€šå¸¸ 224Ã—224ï¼ˆåŸå§‹æ›´å¤§ï¼‰ | å›ºå®š 64Ã—64        |
| ç£ç›˜å ç”¨ | ~150 GB                  | ~250 MBï¼ˆå‹ç¼©åï¼‰ |

Tiny ImageNet ä¿ç•™äº† ImageNet çš„è¯­ä¹‰å¤šæ ·æ€§ï¼ŒåŒæ—¶å¤§å¹…é™ä½è®¡ç®—å¼€é”€ï¼Œéå¸¸é€‚åˆæ•™å­¦ã€åŸå‹éªŒè¯å’Œæœ¬åœ°è°ƒè¯•ã€‚

#### 1.2 ä¸ºä»€ä¹ˆä½¿ç”¨ ResNet50ï¼Ÿ

ResNet50 æ˜¯ç”± He et al. (2015) æå‡ºçš„ç»å…¸æ·±åº¦å·ç§¯ç½‘ç»œï¼Œå…¶æ ¸å¿ƒåˆ›æ–°æ˜¯**æ®‹å·®è¿æ¥ï¼ˆskip connectionï¼‰**ï¼Œæœ‰æ•ˆç¼“è§£äº†æ·±å±‚ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±ä¸é€€åŒ–é—®é¢˜ã€‚

- **ç»“æ„**ï¼š50 å±‚ï¼ˆå«å·ç§¯ã€æ± åŒ–ã€å…¨è¿æ¥ï¼‰
- **è¾“å…¥å°ºå¯¸**ï¼š224Ã—224ï¼ˆéœ€å¯¹ 64Ã—64 å›¾åƒè¿›è¡Œä¸Šé‡‡æ ·ï¼‰
- **è¿ç§»å­¦ä¹ ä¼˜åŠ¿**ï¼šåœ¨ ImageNet ä¸Šé¢„è®­ç»ƒçš„æƒé‡å¯æ˜¾è‘—åŠ é€Ÿæ”¶æ•›
- **è¾“å‡ºå±‚**ï¼šé»˜è®¤ 1000 ç±» â†’ å¾®è°ƒä¸º 200 ç±»

------

### 2. æ•°æ®å‡†å¤‡

#### 2.1 ä¸‹è½½ä¸é¢„å¤„ç†

å®˜æ–¹æ•°æ®é›†åœ°å€ï¼š 

```bash
wget http://cs231n.stanford.edu/tiny-imagenet-200.zip
```

> æˆ–è€…ç›´æ¥ä¸‹è½½é¢„å¤„ç†å¥½çš„æ•°æ®ã€‚
>
> tiny-imagenet-200.zip, https://disk.pku.edu.cn/link/AA068C93E37D564808A74B8F282DCE0F11
> Name: tiny-imagenet-200.zip
> Expires: Never



è§£å‹åï¼Œ`val/` ç›®å½•ä¸‹çš„æ‰€æœ‰å›¾åƒä½äºåŒä¸€æ–‡ä»¶å¤¹ï¼Œä¸ç¬¦åˆ `torchvision.datasets.ImageFolder` çš„è¦æ±‚ï¼ˆéœ€æŒ‰ç±»åˆ«åˆ†ç›®å½•ï¼‰ã€‚æˆ‘ä»¬æä¾›è‡ªåŠ¨åŒ–è„šæœ¬ `tinyimagenet.sh` å®Œæˆæ•´ç†ï¼š

```bash
#!/bin/bash

# download and unzip dataset
#wget http://cs231n.stanford.edu/tiny-imagenet-200.zip
unzip tiny-imagenet-200.zip

current="$(pwd)/tiny-imagenet-200"

# training data
cd $current/train
for DIR in $(ls); do
   cd $DIR
   rm *.txt
   mv images/* .
   rm -r images
   cd ..
done

# validation data
cd $current/val
annotate_file="val_annotations.txt"
length=$(cat $annotate_file | wc -l)
for i in $(seq 1 $length); do
    # fetch i th line
    line=$(sed -n ${i}p $annotate_file)
    # get file name and directory name
    file=$(echo $line | cut -f1 -d" " )
    directory=$(echo $line | cut -f2 -d" ")
    mkdir -p $directory
    mv images/$file $directory
done
rm -r images
echo "done"
```

è¿è¡Œåç›®å½•ç»“æ„å¦‚ä¸‹ï¼š

```
tiny-imagenet-200/
â”œâ”€â”€ train/          # 200 ä¸ªå­ç›®å½•ï¼Œæ¯ç±» 500 å¼ å›¾
â”œâ”€â”€ val/            # 200 ä¸ªå­ç›®å½•ï¼Œæ¯ç±» 50 å¼ å›¾
â”œâ”€â”€ test/           # æ— æ ‡ç­¾ï¼ˆç”¨äºç«èµ›ï¼‰
â”œâ”€â”€ wnids.txt       # ç±»åˆ« ID åˆ—è¡¨
â””â”€â”€ words.txt       # ç±»åˆ«åç§°æ˜ å°„
```

> æ€»å¤§å°çº¦ **472 MB**ã€‚

------

### 3. æ¨¡å‹è®­ç»ƒ

#### 3.1 ç¯å¢ƒä¸ç¡¬ä»¶

- **è®¾å¤‡**ï¼šMac Studio (Apple M1 Ultra, 64GB ç»Ÿä¸€å†…å­˜)
- **PyTorch åç«¯**ï¼šMPSï¼ˆMetal Performance Shadersï¼‰
- **å…³é”®é™åˆ¶**ï¼šMPS **ä¸æ”¯æŒ float64**ï¼Œéœ€æ˜¾å¼ä½¿ç”¨ `.float()`

#### 3.2 å®Œæ•´ä»£ç 

 `tiny_imagenet_resnet50_epoch25.py` 

```python
import os
import copy
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, models, transforms

# è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
def train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, num_epochs=25, device='cpu'):
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch+1, num_epochs))
        print('-' * 10)

        # æ¯ä¸ª epoch åˆ†ä¸ºè®­ç»ƒå’ŒéªŒè¯é˜¶æ®µ
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
            else:
                model.eval()   # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

            running_loss = 0.0
            running_corrects = 0

            # éå†æ•°æ®
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                optimizer.zero_grad()  # æ¢¯åº¦æ¸…é›¶

                # å‰å‘ä¼ æ’­
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    # ä»…åœ¨è®­ç»ƒé˜¶æ®µåå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            if phase == 'train':
                scheduler.step()

            epoch_loss = running_loss / dataset_sizes[phase]

            # MPS åç«¯ä¸æ”¯æŒ float64 è¿ç®—ã€‚è§£å†³æ–¹æ³•æ˜¯ä½¿ç”¨ float32ï¼Œå³è°ƒç”¨ .float()ã€‚
            #epoch_acc = running_corrects.double() / dataset_sizes[phase]
            epoch_acc = running_corrects.float() / dataset_sizes[phase]

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))

            # ä¿å­˜æœ€ä½³æ¨¡å‹å‚æ•°
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
        print()

    print('Best val Acc: {:.4f}'.format(best_acc))
    model.load_state_dict(best_model_wts)
    return model

def main():
    # 1. æ•°æ®é¢„å¤„ç†ä¸åŠ è½½
    # æ³¨æ„ï¼šæ­¤å¤„å‡å®šImageNetæ•°æ®é›†æŒ‰ç…§train/valæ–‡ä»¶å¤¹åˆ†åˆ«å­˜æ”¾å„ç±»åˆ«å›¾ç‰‡ï¼Œ
    # ä¸”æ¯ä¸ªç±»åˆ«ä½œä¸ºä¸€ä¸ªå­æ–‡ä»¶å¤¹å­˜åœ¨
    data_transforms = {
        'train': transforms.Compose([
            transforms.RandomResizedCrop(224),           # éšæœºè£å‰ªä¸º224Ã—224
            transforms.RandomHorizontalFlip(),           # éšæœºæ°´å¹³ç¿»è½¬
            transforms.ToTensor(),                         # è½¬ä¸ºTensor
            transforms.Normalize([0.485, 0.456, 0.406],    # ImageNetå‡å€¼
                                 [0.229, 0.224, 0.225])      # ImageNetæ ‡å‡†å·®
        ]),
        'val': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),                           # ä¸­å¿ƒè£å‰ª
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],
                                 [0.229, 0.224, 0.225])
        ]),
    }

    # Tiny ImageNet æ•°æ®è·¯å¾„
    data_dir = '/Users/hfyan/data/tiny-imagenet-200'
    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                                data_transforms[x])
                      for x in ['train', 'val']}

    # è®¾ç½® num_workers ä¸º 8 ä»¥åˆ©ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½
    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],
                                                    batch_size=128,    # å¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                                                    shuffle=True,
                                                    num_workers=8)
                   for x in ['train', 'val']}

    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
    class_names = image_datasets['train'].classes

    #device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # ä½¿ç”¨ MPS ä½œä¸º GPU åç«¯ï¼ˆé€‚ç”¨äº Apple Siliconï¼‰
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("Using MPS device for GPU acceleration")
    else:
        device = torch.device("cpu")
        print("MPS device not available, using CPU")

    #2. æ„å»ºæ¨¡å‹ï¼ˆä½¿ç”¨é¢„è®­ç»ƒ ResNet50ï¼‰
    # è¿™é‡Œæˆ‘ä»¬åŠ è½½é¢„è®­ç»ƒçš„ ResNet50 æ¨¡å‹ï¼Œå¹¶ä¿®æ”¹æœ€åçš„å…¨è¿æ¥å±‚ä»¥é€‚åº”Tiny ImageNetçš„ç±»åˆ«æ•°ï¼ˆ200ç±»ï¼‰
    model_ft = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
    num_ftrs = model_ft.fc.in_features
    model_ft.fc = nn.Linear(num_ftrs, len(class_names))
    model_ft = model_ft.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)

    # å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥ï¼Œæ¯7ä¸ªepoché™ä½ä¸€æ¬¡å­¦ä¹ ç‡
    exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

    #3. è®­ç»ƒæ¨¡å‹
    num_epochs = 25  # å¯æ ¹æ®éœ€è¦è°ƒæ•´epochæ•°é‡
    model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,
                           dataloaders, dataset_sizes, num_epochs=num_epochs, device=device)

    #4. ä¿å­˜æ¨¡å‹ï¼Œæ–‡ä»¶åå»ºè®®ä¸º tiny_imagenet_resnet50_epoch25.pth
    torch.save(model_ft.state_dict(), 'tiny_imagenet_resnet50_epoch25.pth')
    print("Model saved as tiny_imagenet_resnet50_epoch25.pth")

if __name__ == '__main__':
    main()
```

> **è¯´æ˜**
>
> - **æ•°æ®é¢„å¤„ç†**
>   ä½¿ç”¨äº† `transforms` å¯¹æ•°æ®è¿›è¡Œäº†æ•°æ®å¢å¼ºï¼ˆå¦‚éšæœºè£å‰ªã€æ°´å¹³ç¿»è½¬ï¼‰ä»¥åŠå½’ä¸€åŒ–ï¼ˆImageNetå¸¸ç”¨çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼‰ã€‚æ•°æ®æ–‡ä»¶å¤¹éœ€è¦ç¬¦åˆ `ImageFolder` çš„è¦æ±‚ï¼Œæ¯ä¸ªç±»åˆ«å­˜æ”¾åœ¨ç‹¬ç«‹çš„å­æ–‡ä»¶å¤¹ä¸­ã€‚
> - **æ¨¡å‹æ„å»º**
>   æœ¬ç¤ºä¾‹ä¸­é‡‡ç”¨é¢„è®­ç»ƒçš„ ResNet50 æ¨¡å‹ï¼Œå¹¶ä¿®æ”¹äº†æœ€åä¸€å±‚å…¨è¿æ¥å±‚ä»¥è¾“å‡ºä¸ç±»åˆ«æ•°åŒ¹é…çš„æ¦‚ç‡åˆ†å¸ƒã€‚
> - **è®­ç»ƒè¿‡ç¨‹**
>   ä»£ç ä¸­å®šä¹‰äº† `train_model` å‡½æ•°ï¼Œå¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ï¼Œå¹¶åœ¨éªŒè¯é›†ä¸Šé€‰å–å‡†ç¡®ç‡æœ€é«˜çš„æ¨¡å‹å‚æ•°ã€‚å­¦ä¹ ç‡è°ƒåº¦å™¨ç”¨äºé€æ­¥é™ä½å­¦ä¹ ç‡ä»¥ä¾¿æ›´å¥½åœ°æ”¶æ•›ã€‚
> - **æ³¨æ„äº‹é¡¹**
>   - ImageNet æ•°æ®é›†è¾ƒå¤§ï¼Œå»ºè®®åœ¨ä½¿ç”¨æ—¶æ³¨æ„æ•°æ®åŠ è½½ã€å†…å­˜ç®¡ç†å’Œè®­ç»ƒæ—¶é•¿ã€‚
>   - å¦‚éœ€æ›´æ·±å…¥çš„æ¨¡å‹è°ƒä¼˜æˆ–ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·å‚è€ƒ PyTorch å®˜æ–¹æ–‡æ¡£å’Œç›¸å…³èµ„æ–™ã€‚
>
> è¯¥ç¤ºä¾‹ä»£ç ä¸ºå…¥é—¨çº§ç¤ºä¾‹ï¼Œå®é™…é¡¹ç›®ä¸­å¯èƒ½éœ€è¦æ›´å¤šçš„ä¼˜åŒ–å’Œé…ç½®ã€‚
>
> 
>
> **ä¸»å…¥å£ä¿æŠ¤**ï¼šæ‰€æœ‰æ¶‰åŠå¤šè¿›ç¨‹æˆ–å¤šçº¿ç¨‹çš„ä»£ç éƒ½å°è£…åœ¨ `if __name__ == '__main__':` ä¸‹ï¼Œé¿å… macOS ä¸‹çš„å¯åŠ¨é—®é¢˜ã€‚



**2025/2/24 11:30å¼€å§‹è¿è¡Œï¼Œ16:00ç»“æŸ**

>  File "/Users/hfyan/data/tiny_imagenet_resnet50_epoch25.py", line 47, in train_model
>
>  epoch_acc = running_corrects.double() / dataset_sizes[phase]
>
>  TypeError: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.
>
>  (base) hfyan@HongfeideMac-Studio data % python tiny_imagenet_resnet50_epoch25.py 

```
(base) hfyan@HongfeideMac-Studio data % python tiny_imagenet_resnet50_epoch25.py 
Using MPS device for GPU acceleration
Epoch 1/25
----------
train Loss: 5.0366 Acc: 0.0720
val Loss: 4.1348 Acc: 0.2819

Epoch 2/25
----------
train Loss: 3.2563 Acc: 0.3406
val Loss: 1.7006 Acc: 0.6197

Epoch 3/25
----------
train Loss: 2.1834 Acc: 0.5065
val Loss: 1.2068 Acc: 0.7062

Epoch 4/25
----------
train Loss: 1.8635 Acc: 0.5663
val Loss: 1.0010 Acc: 0.7498

Epoch 5/25
----------
train Loss: 1.6788 Acc: 0.6029
val Loss: 0.8927 Acc: 0.7702

Epoch 6/25
----------
train Loss: 1.5723 Acc: 0.6268
val Loss: 0.8407 Acc: 0.7808

Epoch 7/25
----------
train Loss: 1.5044 Acc: 0.6390
val Loss: 0.7990 Acc: 0.7907

Epoch 8/25
----------
train Loss: 1.4324 Acc: 0.6567
val Loss: 0.7788 Acc: 0.7939

Epoch 9/25
----------
train Loss: 1.4212 Acc: 0.6571
val Loss: 0.7701 Acc: 0.7981

Epoch 10/25
----------
train Loss: 1.4054 Acc: 0.6614
val Loss: 0.7669 Acc: 0.7966

Epoch 11/25
----------
train Loss: 1.4035 Acc: 0.6615
val Loss: 0.7634 Acc: 0.7980

Epoch 12/25
----------
train Loss: 1.3995 Acc: 0.6626
val Loss: 0.7595 Acc: 0.7990

Epoch 13/25
----------
train Loss: 1.3882 Acc: 0.6647
val Loss: 0.7558 Acc: 0.7988

Epoch 14/25
----------
train Loss: 1.3747 Acc: 0.6680
val Loss: 0.7517 Acc: 0.7997

Epoch 15/25
----------
train Loss: 1.3754 Acc: 0.6683
val Loss: 0.7490 Acc: 0.8006

Epoch 16/25
----------
train Loss: 1.3685 Acc: 0.6689
val Loss: 0.7592 Acc: 0.7970

Epoch 17/25
----------
train Loss: 1.3771 Acc: 0.6681
val Loss: 0.7567 Acc: 0.8009

Epoch 18/25
----------
train Loss: 1.3690 Acc: 0.6688
val Loss: 0.7508 Acc: 0.8011

Epoch 19/25
----------
train Loss: 1.3716 Acc: 0.6694
val Loss: 0.7521 Acc: 0.8008

Epoch 20/25
----------
train Loss: 1.3729 Acc: 0.6687
val Loss: 0.7527 Acc: 0.8002

Epoch 21/25
----------
train Loss: 1.3709 Acc: 0.6689
val Loss: 0.7501 Acc: 0.8014

Epoch 22/25
----------
train Loss: 1.3706 Acc: 0.6708
val Loss: 0.7516 Acc: 0.8008

Epoch 23/25
----------
train Loss: 1.3681 Acc: 0.6696
val Loss: 0.7502 Acc: 0.8002

Epoch 24/25
----------
train Loss: 1.3725 Acc: 0.6698
val Loss: 0.7508 Acc: 0.8003

Epoch 25/25
----------
train Loss: 1.3708 Acc: 0.6696
val Loss: 0.7480 Acc: 0.8004

Best val Acc: 0.8014
Model saved as tiny_imagenet_resnet50_epoch25.pth

```

è·‘äº†4å°æ—¶30åˆ†é’Ÿã€‚

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250224171542842.png" alt="image-20250224171542842" style="zoom:50%;" />



```
% ls -lh *.pth
-rw-r--r--  1 hfyan  staff    92M Feb 24 16:02 tiny_imagenet_resnet50_epoch25.pth
```







#### 3.3 å¸¸è§é—®é¢˜è§£ç­”

##### Q1ï¼šä¸ºä½•æ¯æ¬¡è¿è¡Œéƒ½é‡æ–°ä¸‹è½½ ResNet50 æƒé‡ï¼Ÿ

PyTorch é»˜è®¤ç¼“å­˜è‡³ `~/.cache/torch/hub/checkpoints/`ã€‚è‹¥é‡å¤ä¸‹è½½ï¼Œè¯·æ£€æŸ¥ï¼š

- æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å®Œæ•´ï¼ˆ`resnet50-11ad3fa6.pth` â‰ˆ 97MBï¼‰
- ç›®å½•å†™æƒé™
- æ˜¯å¦ä½¿ç”¨ä¸´æ—¶ç¯å¢ƒï¼ˆå¦‚å®¹å™¨ï¼‰

å¯é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡æŒ‡å®šç¼“å­˜è·¯å¾„ï¼š

```bash
export TORCH_HOME=/path/to/stable/cache
```

##### Q2ï¼šMPS æŠ¥é”™ â€œCannot convert to float64â€ï¼Ÿ

Apple MPS ä¸æ”¯æŒ double ç²¾åº¦ã€‚åŠ¡å¿…ä½¿ç”¨ï¼š

```python
accuracy = correct.float() / total  # âœ… æ­£ç¡®
# accuracy = correct.double() / total  # âŒ é”™è¯¯
```

##### Q3ï¼šå¦‚ä½•å‘½åæ¨¡å‹æ–‡ä»¶ï¼Ÿ

æ¨èæ ¼å¼ï¼š`{dataset}_{model}_{epochs}.pth`
ç¤ºä¾‹ï¼š`tiny_imagenet_resnet50_epoch25.pth`

------

### 4. æ¨¡å‹è¯„ä¼°

å‰é¢å·²ç»ä¿å­˜äº†æ¨¡å‹æƒé‡ï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹æ­¥éª¤åŠ è½½æ¨¡å‹å¹¶åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼š

1. **åŠ è½½æ¨¡å‹ç»“æ„å’Œæƒé‡**  
   è¯·ç¡®ä¿ä½ å®šä¹‰çš„æ¨¡å‹ç»“æ„ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ã€‚ä½¿ç”¨ `torch.load` åŠ è½½æƒé‡ï¼Œå¹¶ç”¨ `model.load_state_dict` å¯¼å…¥ã€‚

2. **åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼**  
   è°ƒç”¨ `model.eval()` ç¡®ä¿æ¨¡å‹å…³é—­ BatchNormã€Dropout ç­‰è®­ç»ƒæ—¶ç‰¹æœ‰çš„è¡Œä¸ºã€‚

3. **éå†éªŒè¯æ•°æ®å¹¶è®¡ç®—å‡†ç¡®ç‡**  
   ä½¿ç”¨ `torch.no_grad()` å…³é—­æ¢¯åº¦è®¡ç®—ï¼ŒåŠ å¿«éªŒè¯é€Ÿåº¦ï¼Œå¹¶é˜²æ­¢å†…å­˜æµªè´¹ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ä»£ç ï¼Œ`eval_tiny_imagenet_resnet50_epoch25_pth.py `ï¼š

```python
import os
import torch
import torch.nn as nn
from torchvision import datasets, models, transforms

# æ•°æ®é¢„å¤„ç†
data_transforms = {
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])
    ]),
}

# ç¡®ä¿å­è¿›ç¨‹å®‰å…¨å¯åŠ¨
if __name__ == '__main__':
    data_dir = '/Users/hfyan/data/tiny-imagenet-200'
    val_dir = os.path.join(data_dir, 'val')
    val_dataset = datasets.ImageFolder(val_dir, data_transforms['val'])
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64,
                                             shuffle=False, num_workers=4)

    # é€‰æ‹©è®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("Using MPS device for GPU acceleration")
    else:
        device = torch.device("cpu")
        print("MPS device not available, using CPU")

    # åŠ è½½æ¨¡å‹
    model_ft = models.resnet50(pretrained=False)
    num_ftrs = model_ft.fc.in_features
    model_ft.fc = nn.Linear(num_ftrs, len(val_dataset.classes))
    model_ft = model_ft.to(device)

    # åŠ è½½æ¨¡å‹æƒé‡
    model_path = 'tiny_imagenet_resnet50_epoch25.pth'
    model_ft.load_state_dict(torch.load(model_path, map_location=device))

    # è¯„ä¼°æ¨¡å¼
    model_ft.eval()

    # æ¨¡å‹è¯„ä¼°
    running_corrects = 0
    total_samples = 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model_ft(inputs)
            _, preds = torch.max(outputs, 1)
            running_corrects += torch.sum(preds == labels.data)
            total_samples += inputs.size(0)

    val_acc = running_corrects.float() / total_samples
    print('Validation Accuracy: {:.4f}'.format(val_acc))

```

éªŒè¯æ­¥éª¤ï¼š  

> - åŠ è½½ä¸ä½ è®­ç»ƒæ—¶ä¸€è‡´çš„æ¨¡å‹ç»“æ„ã€‚  
> - ä½¿ç”¨ `model.load_state_dict()` åŠ è½½æƒé‡ã€‚  
> - è°ƒç”¨ `model.eval()` è¿›å…¥éªŒè¯æ¨¡å¼ã€‚  
> - éå†éªŒè¯æ•°æ®é›†ï¼Œè®¡ç®—å‡†ç¡®ç‡æˆ–å…¶ä»–æŒ‡æ ‡ã€‚
>
> è¿™æ ·ï¼Œä½ å°±å¯ä»¥åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹å¹¶å¯¹éªŒè¯é›†æ•°æ®è¿›è¡Œæµ‹è¯•ã€‚
>
> ![image-20250224171857564](https://raw.githubusercontent.com/GMyhf/img/main/img/image-20250224171857564.png)
>
> 
>
> ```python
> (base) hfyan@HongfeideMac-Studio data % python eval_tiny_imagenet_resnet50_epoch25_pth.py 
> Using MPS device for GPU acceleration
> /Users/hfyan/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
>   warnings.warn(
> /Users/hfyan/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
>   warnings.warn(msg)
> Validation Accuracy: 0.8014
> (base) hfyan@HongfeideMac-Studio data % 
> ```





### 5. æ‰©å±•ä¸ä¼˜åŒ–

#### 5.1 æ€§èƒ½å¯¹æ¯”ï¼šImageNet vs Tiny ImageNet

| æŒ‡æ ‡      | ImageNet (ResNet50)           | Tiny ImageNet (æœ¬å®éªŒ)   |
| --------- | ----------------------------- | ------------------------ |
| Top-1 Acc | ~76.5%ï¼ˆåŸå§‹ï¼‰â†’ 84%ï¼ˆä¼˜åŒ–åï¼‰ | **80.14%**               |
| è®­ç»ƒæ—¶é—´  | æ•°å¤©ï¼ˆ8Ã—V100ï¼‰                | **4.5 å°æ—¶**ï¼ˆM1 Ultraï¼‰ |
| æ˜¾å­˜éœ€æ±‚  | >16 GB                        | <8 GBï¼ˆbatch=128ï¼‰       |

------

#### 5.2 å…³äºå¤šè¿›ç¨‹ä¸ GIL

- `DataLoader(num_workers>0)` ä½¿ç”¨ **å¤šè¿›ç¨‹**ï¼ˆéçº¿ç¨‹ï¼‰ï¼Œç»•è¿‡ Python GIL
- PyTorch åº•å±‚è®¡ç®—ï¼ˆC++/CUDA/Metalï¼‰ä¸å— GIL é™åˆ¶
- å°è§„æ¨¡ä»»åŠ¡ï¼ˆå¦‚å°çŸ©é˜µä¹˜æ³•ï¼‰å¹¶è¡Œå¯èƒ½å› é€šä¿¡å¼€é”€è€Œå˜æ…¢



#### æ€»ç»“

æœ¬é¡¹ç›®å®Œæ•´å±•ç¤ºäº†ä»æ•°æ®å‡†å¤‡ã€æ¨¡å‹å¾®è°ƒåˆ°è¯„ä¼°çš„å…¨æµç¨‹ï¼Œé€‚åˆå…¥é—¨è€…ç†è§£è¿ç§»å­¦ä¹ ä¸ PyTorch å®è·µã€‚é€šè¿‡åˆç†åˆ©ç”¨ Apple Silicon çš„ MPS åŠ é€Ÿï¼Œå¯åœ¨æ¶ˆè´¹çº§è®¾å¤‡ä¸Šé«˜æ•ˆå®Œæˆä¸­ç­‰è§„æ¨¡è§†è§‰ä»»åŠ¡ã€‚



# 5. è¿è¡Œã€Šä»é›¶æ„å»ºå¤§æ¨¡å‹ã€‹ä»£ç 

ã€Šä»é›¶æ„å»ºå¤§æ¨¡å‹ã€‹ä»£ç ï¼Œhttps://github.com/rasbt/LLMs-from-scratch

Build a Large Language Model (From Scratch)

å¯ä»¥åœ¨æœ¬åœ° mac æˆ–è€… window è¿è¡Œï¼Œé…ç½®æ–¹æ³•æ–¹æ³•å¦‚ï¼š ...Setup_....mdã€‚

https://github.com/GMyhf/2025fall-cs201/tree/main/LLM





# é™„å½•A. PyTorchæ•™ç¨‹@runoob

https://www.runoob.com/pytorch/pytorch-tutorial.html

PyTorch æ˜¯ä¸€ä¸ªå¼€æºçš„æœºå™¨å­¦ä¹ åº“ï¼Œä¸»è¦ç”¨äºè¿›è¡Œè®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸçš„ç ”ç©¶å’Œå¼€å‘ã€‚

PyTorchç”± Facebook çš„äººå·¥æ™ºèƒ½ç ”ç©¶å›¢é˜Ÿå¼€å‘ï¼Œå¹¶åœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ç¤¾åŒºä¸­å¹¿æ³›ä½¿ç”¨ã€‚

PyTorch ä»¥å…¶çµæ´»æ€§å’Œæ˜“ç”¨æ€§è€Œé—»åï¼Œç‰¹åˆ«é€‚åˆäºæ·±åº¦å­¦ä¹ ç ”ç©¶å’Œå¼€å‘ã€‚



## A.1 PyTorchå®‰è£…

åœ¨å·²ç»å®‰è£…å¥½çš„pythonç¯å¢ƒä¸­ï¼Œåœ¨terminalçª—å£å‘½ä»¤è¡Œä¸­ï¼Œæ¿€æ´»ç¯å¢ƒï¼Œå¹¶å®‰è£…torchåŒ…

> Pythonå¼€å‘ç¯å¢ƒé…ç½®æŒ‡å—ï¼Œhttps://github.com/GMyhf/2025fall-cs101/blob/main/Python_Development_Setup_Mac_Windows.md

```
[hfyan@HongfeideMac-Studio MyPythonApp % source .venv/bin/activate
(.venv) hfyan@HongfeideMac-Studio MyPythonApp % uv pip install torch torchvision torchaudio

```



**éªŒè¯å®‰è£…**

ä¸ºäº†ç¡®ä¿ PyTorch å·²æ­£ç¡®å®‰è£…ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ‰§è¡Œä»¥ä¸‹ PyTorch ä»£ç æ¥éªŒè¯æ˜¯å¦å®‰è£…æˆåŠŸï¼š

**å®ä¾‹**

```python
import torch

# å½“å‰å®‰è£…çš„ PyTorch åº“çš„ç‰ˆæœ¬
print(torch.__version__)

# æ£€æµ‹ MPS ä½œä¸º GPU åç«¯æ˜¯å¦å¯ç”¨ï¼ˆé€‚ç”¨äº Apple Siliconï¼‰
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("Using MPS device for GPU acceleration")
else:   # æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨ï¼Œå³ä½ çš„ç³»ç»Ÿæœ‰ NVIDIA çš„ GPU
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print("MPS device not available, using CPU")

"""
2.9.1
Using MPS device for GPU acceleration
"""
```



ä¸€ä¸ªç®€å•çš„å®ä¾‹ï¼Œæ„å»ºä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„å¼ é‡ï¼š

```python
import torch
x = torch.rand(5, 3)
print(x)
```

å¦‚æœå®‰è£…æˆåŠŸï¼Œè¾“å‡ºç»“æœç±»ä¼¼å¦‚ä¸‹ï¼š

```
tensor([[0.8622, 0.5916, 0.8073],
        [0.8955, 0.7416, 0.3482],
        [0.8059, 0.1414, 0.2828],
        [0.0923, 0.0113, 0.4226],
        [0.4438, 0.9789, 0.9048]])
```



**å®ä¾‹**

ä¸‹é¢çš„æ˜¯ PyTorch ä¸­ä¸€äº›åŸºæœ¬çš„å¼ é‡æ“ä½œï¼šå¦‚ä½•åˆ›å»ºéšæœºå¼ é‡ã€è¿›è¡Œé€å…ƒç´ è¿ç®—ã€è®¿é—®ç‰¹å®šå…ƒç´ ä»¥åŠè®¡ç®—æ€»å’Œå’Œæœ€å¤§å€¼ã€‚

```python
import torch

# è®¾ç½®æ•°æ®ç±»å‹å’Œè®¾å¤‡
dtype = torch.float  # å¼ é‡æ•°æ®ç±»å‹ä¸ºæµ®ç‚¹å‹
device = torch.device("cpu")  # æœ¬æ¬¡è®¡ç®—åœ¨ CPU ä¸Šè¿›è¡Œ

# åˆ›å»ºå¹¶æ‰“å°ä¸¤ä¸ªéšæœºå¼ é‡ a å’Œ b
a = torch.randn(2, 3, device=device, dtype=dtype)  # åˆ›å»ºä¸€ä¸ª 2x3 çš„éšæœºå¼ é‡
b = torch.randn(2, 3, device=device, dtype=dtype)  # åˆ›å»ºå¦ä¸€ä¸ª 2x3 çš„éšæœºå¼ é‡

print("å¼ é‡ a:")
print(a)

print("å¼ é‡ b:")
print(b)

# é€å…ƒç´ ç›¸ä¹˜å¹¶è¾“å‡ºç»“æœ
print("a å’Œ b çš„é€å…ƒç´ ä¹˜ç§¯:")
print(a * b)

# è¾“å‡ºå¼ é‡ a æ‰€æœ‰å…ƒç´ çš„æ€»å’Œ
print("å¼ é‡ a æ‰€æœ‰å…ƒç´ çš„æ€»å’Œ:")
print(a.sum())

# è¾“å‡ºå¼ é‡ a ä¸­ç¬¬ 2 è¡Œç¬¬ 3 åˆ—çš„å…ƒç´ ï¼ˆæ³¨æ„ç´¢å¼•ä» 0 å¼€å§‹ï¼‰
print("å¼ é‡ a ç¬¬ 2 è¡Œç¬¬ 3 åˆ—çš„å…ƒç´ :")
print(a[1, 2])

# è¾“å‡ºå¼ é‡ a ä¸­çš„æœ€å¤§å€¼
print("å¼ é‡ a ä¸­çš„æœ€å¤§å€¼:")
print(a.max())

"""
å¼ é‡ a:
tensor([[ 0.8182,  0.4718,  0.8318],
        [ 0.8146,  0.1508, -1.7619]])
å¼ é‡ b:
tensor([[-1.7713,  0.8521,  0.4598],
        [ 0.3082,  0.0558,  0.1254]])
a å’Œ b çš„é€å…ƒç´ ä¹˜ç§¯:
tensor([[-1.4493,  0.4020,  0.3824],
        [ 0.2510,  0.0084, -0.2209]])
å¼ é‡ a æ‰€æœ‰å…ƒç´ çš„æ€»å’Œ:
tensor(1.3252)
å¼ é‡ a ç¬¬ 2 è¡Œç¬¬ 3 åˆ—çš„å…ƒç´ :
tensor(-1.7619)
å¼ é‡ a ä¸­çš„æœ€å¤§å€¼:
tensor(0.8318)
"""
```



## A.2 PyTorch ç®€ä»‹

PyTorch æ˜¯ä¸€ä¸ªå¼€æºçš„ Python æœºå™¨å­¦ä¹ åº“ï¼ŒåŸºäº Torch åº“ï¼Œåº•å±‚ç”± C++ å®ç°ï¼Œåº”ç”¨äºäººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¦‚è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚

PyTorch æœ€åˆç”± Meta Platforms çš„äººå·¥æ™ºèƒ½ç ”ç©¶å›¢é˜Ÿå¼€å‘ï¼Œç°åœ¨å± äºLinux åŸºé‡‘ä¼šçš„ä¸€éƒ¨åˆ†ã€‚

è®¸å¤šæ·±åº¦å­¦ä¹ è½¯ä»¶éƒ½æ˜¯åŸºäº PyTorch æ„å»ºçš„ï¼ŒåŒ…æ‹¬ç‰¹æ–¯æ‹‰è‡ªåŠ¨é©¾é©¶ã€Uber çš„ Pyroã€Hugging Face çš„ Transformersã€ PyTorch Lightning å’Œ Catalystã€‚

**PyTorch ä¸»è¦æœ‰ä¸¤å¤§ç‰¹å¾ï¼š**

- ç±»ä¼¼äº NumPy çš„å¼ é‡è®¡ç®—ï¼Œèƒ½åœ¨ GPU æˆ– MPS ç­‰ç¡¬ä»¶åŠ é€Ÿå™¨ä¸ŠåŠ é€Ÿã€‚
- åŸºäºå¸¦è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿçš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚

PyTorch åŒ…æ‹¬ torch.autogradã€torch.nnã€torch.optim ç­‰å­æ¨¡å—ã€‚

PyTorch åŒ…å«å¤šç§æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬ MSEï¼ˆå‡æ–¹è¯¯å·® = L2 èŒƒæ•°ï¼‰ã€äº¤å‰ç†µæŸå¤±å’Œè´Ÿç†µä¼¼ç„¶æŸå¤±ï¼ˆå¯¹åˆ†ç±»å™¨æœ‰ç”¨ï¼‰ç­‰ã€‚

### PyTorch ç‰¹æ€§

- **åŠ¨æ€è®¡ç®—å›¾ï¼ˆDynamic Computation Graphsï¼‰**ï¼š PyTorch çš„è®¡ç®—å›¾æ˜¯åŠ¨æ€çš„ï¼Œè¿™æ„å‘³ç€å®ƒä»¬åœ¨è¿è¡Œæ—¶æ„å»ºï¼Œå¹¶ä¸”å¯ä»¥éšæ—¶æ”¹å˜ã€‚è¿™ä¸ºå®éªŒå’Œè°ƒè¯•æä¾›äº†æå¤§çš„çµæ´»æ€§ï¼Œå› ä¸ºå¼€å‘è€…å¯ä»¥é€è¡Œæ‰§è¡Œä»£ç ï¼ŒæŸ¥çœ‹ä¸­é—´ç»“æœã€‚
- **è‡ªåŠ¨å¾®åˆ†ï¼ˆAutomatic Differentiationï¼‰**ï¼š PyTorch çš„è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿå…è®¸å¼€å‘è€…è½»æ¾åœ°è®¡ç®—æ¢¯åº¦ï¼Œè¿™å¯¹äºè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹è‡³å…³é‡è¦ã€‚å®ƒé€šè¿‡åå‘ä¼ æ’­ç®—æ³•è‡ªåŠ¨è®¡ç®—å‡ºæŸå¤±å‡½æ•°å¯¹æ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚
- **å¼ é‡è®¡ç®—ï¼ˆTensor Computationï¼‰**ï¼š PyTorch æä¾›äº†ç±»ä¼¼äº NumPy çš„å¼ é‡æ“ä½œï¼Œè¿™äº›æ“ä½œå¯ä»¥åœ¨ CPU å’Œ GPU ä¸Šæ‰§è¡Œï¼Œä»è€ŒåŠ é€Ÿè®¡ç®—è¿‡ç¨‹ã€‚å¼ é‡æ˜¯ PyTorch ä¸­çš„åŸºæœ¬æ•°æ®ç»“æ„ï¼Œç”¨äºå­˜å‚¨å’Œæ“ä½œæ•°æ®ã€‚
- **ä¸°å¯Œçš„ API**ï¼š PyTorch æä¾›äº†å¤§é‡çš„é¢„å®šä¹‰å±‚ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç®—æ³•ï¼Œè¿™äº›éƒ½æ˜¯æ„å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¸¸ç”¨ç»„ä»¶ã€‚
- **å¤šè¯­è¨€æ”¯æŒ**ï¼š PyTorch è™½ç„¶ä»¥ Python ä¸ºä¸»è¦æ¥å£ï¼Œä½†ä¹Ÿæä¾›äº† C++ æ¥å£ï¼Œå…è®¸æ›´åº•å±‚çš„é›†æˆå’Œæ§åˆ¶ã€‚

#### åŠ¨æ€è®¡ç®—å›¾ï¼ˆDynamic Computation Graphï¼‰

PyTorch æœ€æ˜¾è‘—çš„ç‰¹ç‚¹ä¹‹ä¸€æ˜¯å…¶åŠ¨æ€è®¡ç®—å›¾çš„æœºåˆ¶ã€‚

ä¸ TensorFlow çš„é™æ€è®¡ç®—å›¾ï¼ˆgraphï¼‰ä¸åŒï¼ŒPyTorch åœ¨æ‰§è¡Œæ—¶æ„å»ºè®¡ç®—å›¾ï¼Œè¿™æ„å‘³ç€åœ¨æ¯æ¬¡è®¡ç®—æ—¶ï¼Œå›¾éƒ½ä¼šæ ¹æ®è¾“å…¥æ•°æ®çš„å½¢çŠ¶è‡ªåŠ¨å˜åŒ–ã€‚

**åŠ¨æ€è®¡ç®—å›¾çš„ä¼˜ç‚¹ï¼š**

- æ›´åŠ çµæ´»ï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦æ¡ä»¶åˆ¤æ–­æˆ–é€’å½’çš„åœºæ™¯ã€‚
- æ–¹ä¾¿è°ƒè¯•å’Œä¿®æ”¹ï¼Œèƒ½å¤Ÿç›´æ¥æŸ¥çœ‹ä¸­é—´ç»“æœã€‚
- æ›´æ¥è¿‘ Python ç¼–ç¨‹çš„é£æ ¼ï¼Œæ˜“äºä¸Šæ‰‹ã€‚

#### å¼ é‡ï¼ˆTensorï¼‰ä¸è‡ªåŠ¨æ±‚å¯¼ï¼ˆAutogradï¼‰

PyTorch ä¸­çš„æ ¸å¿ƒæ•°æ®ç»“æ„æ˜¯ **å¼ é‡ï¼ˆTensorï¼‰**ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¤šç»´çŸ©é˜µï¼Œå¯ä»¥åœ¨ CPU æˆ– GPU ä¸Šé«˜æ•ˆåœ°è¿›è¡Œè®¡ç®—ã€‚å¼ é‡çš„æ“ä½œæ”¯æŒè‡ªåŠ¨æ±‚å¯¼ï¼ˆAutogradï¼‰æœºåˆ¶ï¼Œä½¿å¾—åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼Œè¿™å¯¹äºæ·±åº¦å­¦ä¹ ä¸­çš„æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•è‡³å…³é‡è¦ã€‚

**å¼ é‡ï¼ˆTensorï¼‰ï¼š**

- æ”¯æŒåœ¨ CPU å’Œ GPU ä¹‹é—´è¿›è¡Œåˆ‡æ¢ã€‚
- æä¾›äº†ç±»ä¼¼ NumPy çš„æ¥å£ï¼Œæ”¯æŒå…ƒç´ çº§è¿ç®—ã€‚
- æ”¯æŒè‡ªåŠ¨æ±‚å¯¼ï¼Œå¯ä»¥æ–¹ä¾¿åœ°è¿›è¡Œæ¢¯åº¦è®¡ç®—ã€‚

**è‡ªåŠ¨æ±‚å¯¼ï¼ˆAutogradï¼‰ï¼š**

- PyTorch å†…ç½®çš„è‡ªåŠ¨æ±‚å¯¼å¼•æ“ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¿½è¸ªæ‰€æœ‰å¼ é‡çš„æ“ä½œï¼Œå¹¶åœ¨åå‘ä¼ æ’­æ—¶è®¡ç®—æ¢¯åº¦ã€‚
- é€šè¿‡ `requires_grad` å±æ€§ï¼Œå¯ä»¥æŒ‡å®šå¼ é‡éœ€è¦è®¡ç®—æ¢¯åº¦ã€‚
- æ”¯æŒé«˜æ•ˆçš„åå‘ä¼ æ’­ï¼Œé€‚ç”¨äºç¥ç»ç½‘ç»œçš„è®­ç»ƒã€‚

#### æ¨¡å‹å®šä¹‰ä¸è®­ç»ƒ

PyTorch æä¾›äº† `torch.nn` æ¨¡å—ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡ç»§æ‰¿ `nn.Module` ç±»æ¥å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚ä½¿ç”¨ `forward` å‡½æ•°æŒ‡å®šå‰å‘ä¼ æ’­ï¼Œè‡ªåŠ¨åå‘ä¼ æ’­ï¼ˆé€šè¿‡ `autograd`ï¼‰å’Œæ¢¯åº¦è®¡ç®—ä¹Ÿç”± PyTorch å†…éƒ¨å¤„ç†ã€‚

**ç¥ç»ç½‘ç»œæ¨¡å—ï¼ˆtorch.nnï¼‰ï¼š**

- æä¾›äº†å¸¸ç”¨çš„å±‚ï¼ˆå¦‚çº¿æ€§å±‚ã€å·ç§¯å±‚ã€æ± åŒ–å±‚ç­‰ï¼‰ã€‚
- æ”¯æŒå®šä¹‰å¤æ‚çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼ˆåŒ…æ‹¬å¤šè¾“å…¥ã€å¤šè¾“å‡ºçš„ç½‘ç»œï¼‰ã€‚
- å…¼å®¹ä¸ä¼˜åŒ–å™¨ï¼ˆå¦‚ `torch.optim`ï¼‰ä¸€èµ·ä½¿ç”¨ã€‚

#### GPU åŠ é€Ÿ

PyTorch å®Œå…¨æ”¯æŒåœ¨ GPU ä¸Šè¿è¡Œï¼Œä»¥åŠ é€Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒã€‚é€šè¿‡ç®€å•çš„ `.to(device)` æ–¹æ³•ï¼Œç”¨æˆ·å¯ä»¥å°†æ¨¡å‹å’Œå¼ é‡è½¬ç§»åˆ° GPU ä¸Šè¿›è¡Œè®¡ç®—ã€‚PyTorch æ”¯æŒå¤š GPU è®­ç»ƒï¼Œèƒ½å¤Ÿåˆ©ç”¨ NVIDIA CUDA æŠ€æœ¯æ˜¾è‘—æé«˜è®¡ç®—æ•ˆç‡ã€‚

**GPU æ”¯æŒï¼š**

- è‡ªåŠ¨é€‰æ‹© GPU æˆ– CPUã€‚
- æ”¯æŒé€šè¿‡ CUDA åŠ é€Ÿè¿ç®—ã€‚
- æ”¯æŒå¤š GPU å¹¶è¡Œè®¡ç®—ï¼ˆ`DataParallel` æˆ– `torch.distributed`ï¼‰ã€‚

#### ç”Ÿæ€ç³»ç»Ÿä¸ç¤¾åŒºæ”¯æŒ

PyTorch ä½œä¸ºä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæ‹¥æœ‰ä¸€ä¸ªåºå¤§çš„ç¤¾åŒºå’Œç”Ÿæ€ç³»ç»Ÿã€‚å®ƒä¸ä»…åœ¨å­¦æœ¯ç•Œå¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œä¹Ÿåœ¨å·¥ä¸šç•Œï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸä¸­å¾—åˆ°äº†å¹¿æ³›éƒ¨ç½²ã€‚PyTorch è¿˜æä¾›äº†è®¸å¤šä¸æ·±åº¦å­¦ä¹ ç›¸å…³çš„å·¥å…·å’Œåº“ï¼Œå¦‚ï¼š

- **torchvision**ï¼šç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„æ•°æ®é›†å’Œæ¨¡å‹ã€‚
- **torchtext**ï¼šç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ•°æ®é›†å’Œé¢„å¤„ç†å·¥å…·ã€‚
- **torchaudio**ï¼šç”¨äºéŸ³é¢‘å¤„ç†çš„å·¥å…·åŒ…ã€‚
- **PyTorch Lightning**ï¼šä¸€ç§ç®€åŒ– PyTorch ä»£ç çš„é«˜å±‚åº“ï¼Œä¸“æ³¨äºç ”ç©¶å’Œå®éªŒçš„å¿«é€Ÿè¿­ä»£ã€‚

------

### ä¸å…¶ä»–æ¡†æ¶çš„å¯¹æ¯”

PyTorch ç”±äºå…¶çµæ´»æ€§ã€æ˜“ç”¨æ€§å’Œç¤¾åŒºæ”¯æŒï¼Œå·²ç»æˆä¸ºå¾ˆå¤šæ·±åº¦å­¦ä¹ ç ”ç©¶è€…å’Œå¼€å‘è€…çš„é¦–é€‰æ¡†æ¶ã€‚

#### TensorFlow vs PyTorch

- PyTorch çš„åŠ¨æ€è®¡ç®—å›¾ä½¿å¾—å®ƒæ›´åŠ çµæ´»ï¼Œé€‚åˆå¿«é€Ÿå®éªŒå’Œç ”ç©¶ï¼›è€Œ TensorFlow çš„é™æ€è®¡ç®—å›¾åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ›´å…·ä¼˜åŒ–ç©ºé—´ã€‚
- PyTorch åœ¨è°ƒè¯•æ—¶æ›´åŠ æ–¹ä¾¿ï¼ŒTensorFlow åˆ™åœ¨éƒ¨ç½²ä¸Šæ›´åŠ æˆç†Ÿï¼Œæ”¯æŒæ›´å¹¿æ³›çš„ç¡¬ä»¶å’Œå¹³å°ã€‚
- è¿‘å¹´æ¥ï¼ŒTensorFlow ä¹Ÿå¼•å…¥äº†åŠ¨æ€å›¾ï¼ˆå¦‚ TensorFlow 2.xï¼‰ï¼Œä½¿å¾—ä¸¤è€…åœ¨åŠŸèƒ½ä¸Šè¶‹äºæ¥è¿‘ã€‚
- å…¶ä»–æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¦‚ Kerasã€Caffe ç­‰ä¹Ÿæœ‰ä¸€å®šåº”ç”¨ï¼Œä½† PyTorch ç”±äºå…¶çµæ´»æ€§ã€æ˜“ç”¨æ€§å’Œç¤¾åŒºæ”¯æŒï¼Œå·²ç»æˆä¸ºå¾ˆå¤šæ·±åº¦å­¦ä¹ ç ”ç©¶è€…å’Œå¼€å‘è€…çš„é¦–é€‰æ¡†æ¶ã€‚

| ç‰¹æ€§               | **TensorFlow**                                          | **PyTorch**                                                  |
| :----------------- | :------------------------------------------------------ | :----------------------------------------------------------- |
| **å¼€å‘å…¬å¸**       | Google                                                  | Facebook (FAIR)                                              |
| **è®¡ç®—å›¾ç±»å‹**     | é™æ€è®¡ç®—å›¾ï¼ˆå®šä¹‰åå†æ‰§è¡Œï¼‰                              | åŠ¨æ€è®¡ç®—å›¾ï¼ˆå®šä¹‰å³æ‰§è¡Œï¼‰                                     |
| **çµæ´»æ€§**         | ä½ï¼ˆè®¡ç®—å›¾åœ¨ç¼–è¯‘æ—¶æ„å»ºï¼Œä¸æ˜“ä¿®æ”¹ï¼‰                      | é«˜ï¼ˆè®¡ç®—å›¾åœ¨æ‰§è¡Œæ—¶åŠ¨æ€åˆ›å»ºï¼Œæ˜“äºä¿®æ”¹å’Œè°ƒè¯•ï¼‰                 |
| **è°ƒè¯•**           | è¾ƒéš¾ï¼ˆéœ€è¦ä½¿ç”¨ `tf.debugging` æˆ–å¤–éƒ¨å·¥å…·è°ƒè¯•ï¼‰          | å®¹æ˜“ï¼ˆå¯ä»¥ç›´æ¥åœ¨ Python ä¸­è¿›è¡Œè°ƒè¯•ï¼‰                         |
| **æ˜“ç”¨æ€§**         | ä½ï¼ˆè¾ƒå¤æ‚ï¼ŒAPI è¾ƒå¤šï¼Œå­¦ä¹ æ›²çº¿è¾ƒé™¡å³­ï¼‰                  | é«˜ï¼ˆAPI ç®€æ´ï¼Œè¯­æ³•æ›´åŠ æ¥è¿‘ Pythonï¼Œå®¹æ˜“ä¸Šæ‰‹ï¼‰                |
| **éƒ¨ç½²**           | å¼ºï¼ˆæ”¯æŒå¹¿æ³›çš„ç¡¬ä»¶ï¼Œå¦‚ TensorFlow Liteã€TensorFlow.jsï¼‰ | è¾ƒå¼±ï¼ˆéƒ¨ç½²å·¥å…·å’Œå¹³å°ç›¸å¯¹è¾ƒå°‘ï¼Œè™½ç„¶æœ‰ TensorFlow æ”¯æŒï¼‰       |
| **ç¤¾åŒºæ”¯æŒ**       | å¾ˆå¼ºï¼ˆæˆç†Ÿä¸”åºå¤§çš„ç¤¾åŒºï¼Œå¹¿æ³›çš„æ•™ç¨‹å’Œæ–‡æ¡£ï¼‰              | å¾ˆå¼ºï¼ˆç¤¾åŒºæ´»è·ƒï¼Œç‰¹åˆ«æ˜¯åœ¨å­¦æœ¯ç•Œï¼Œå¿«é€Ÿå‘å±•çš„ç”Ÿæ€ï¼‰             |
| **æ¨¡å‹è®­ç»ƒ**       | æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒï¼Œæ”¯æŒå¤šç§è®¾å¤‡ï¼ˆå¦‚ CPUã€GPUã€TPUï¼‰        | æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒï¼Œæ”¯æŒå¤š GPUã€CPU å’Œ TPU                       |
| **API å±‚çº§**       | é«˜çº§APIï¼šKerasï¼›ä½çº§APIï¼šTensorFlow Core                | é«˜çº§APIï¼šTorchVisionã€TorchText ç­‰ï¼›ä½çº§APIï¼šTorch           |
| **æ€§èƒ½**           | é«˜ï¼ˆä¼˜åŒ–æ–¹é¢æˆç†Ÿï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒï¼‰                        | é«˜ï¼ˆé€‚åˆç ”ç©¶å’ŒåŸå‹å¼€å‘ï¼Œç”Ÿäº§æ€§èƒ½ä¹Ÿåœ¨æå‡ï¼‰                   |
| **è‡ªåŠ¨æ±‚å¯¼**       | é€šè¿‡ `tf.GradientTape` å®ç°åŠ¨æ€æ±‚å¯¼ï¼ˆè¾ƒå¤æ‚ï¼‰           | é€šè¿‡ `autograd` åŠ¨æ€æ±‚å¯¼ï¼ˆæ›´ç®€æ´å’Œç›´è§‚ï¼‰                     |
| **è°ƒä¼˜ä¸å¯æ‰©å±•æ€§** | å¼ºï¼ˆæ”¯æŒåœ¨å¤šå¹³å°ä¸Šè¿è¡Œï¼Œå¦‚ TensorFlow Serving ç­‰ï¼‰      | è¾ƒå¼±ï¼ˆè™½ç„¶åœ¨å­¦æœ¯å’Œå®éªŒç¯å¢ƒä¸­è¡¨ç°ä¼˜è¶Šï¼Œä½†ç”Ÿäº§ç¯å¢ƒæ”¯æŒç›¸å¯¹è¾ƒå°‘ï¼‰ |
| **æ¡†æ¶çµæ´»æ€§**     | è¾ƒä½ï¼ˆTensorFlow 2.x å¼•å…¥äº†åŠ¨æ€å›¾ç‰¹æ€§ï¼Œä½†ä»ä¸å®Œå…¨çµæ´»ï¼‰ | é«˜ï¼ˆåŠ¨æ€å›¾å¸¦æ¥æ›´é«˜çš„çµæ´»æ€§ï¼‰                                 |
| **æ”¯æŒå¤šç§è¯­è¨€**   | æ”¯æŒå¤šç§è¯­è¨€ï¼ˆPython, C++, Java, JavaScript, etc.ï¼‰     | ä¸»è¦æ”¯æŒ Pythonï¼ˆä½†ä¹Ÿæœ‰ C++ APIï¼‰                            |
| **å…¼å®¹æ€§ä¸è¿ç§»**   | TensorFlow 2.x ä¸æ—§ç‰ˆæœ¬å…¼å®¹æ€§è¾ƒå¥½                       | ä¸ TensorFlow å…¼å®¹æ€§å·®ï¼Œè¿ç§»è¾ƒéš¾                             |

#### PyTorch vs NumPy

| ç‰¹æ€§         | PyTorch            | NumPy            |
| :----------- | :----------------- | :--------------- |
| **ç›®æ ‡**     | æ·±åº¦å­¦ä¹ ä¸“ç”¨       | é€šç”¨ç§‘å­¦è®¡ç®—     |
| **GPU æ”¯æŒ** | åŸç”Ÿæ”¯æŒ CUDA      | ä¸ç›´æ¥æ”¯æŒ       |
| **è‡ªåŠ¨å¾®åˆ†** | å†…ç½®è‡ªåŠ¨æ±‚å¯¼       | éœ€è¦æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦ |
| **ç¥ç»ç½‘ç»œ** | ä¸°å¯Œçš„ç¥ç»ç½‘ç»œæ¨¡å— | éœ€è¦ä»é›¶å®ç°     |
| **å­¦ä¹ æˆæœ¬** | ç›¸å¯¹è¾ƒé«˜           | ç›¸å¯¹è¾ƒä½         |

------

### PyTorch çš„å†å²ä¸å‘å±•

PyTorch çš„å‰èº«æ˜¯ Torchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº Lua è¯­è¨€çš„ç§‘å­¦è®¡ç®—æ¡†æ¶ã€‚éšç€ Python åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸçš„å…´èµ·ï¼ŒFacebook å›¢é˜Ÿå†³å®šå°† Torch çš„æ ¸å¿ƒæ€æƒ³ç§»æ¤åˆ° Python ä¸Šï¼Œä»è€Œè¯ç”Ÿäº† PyTorchã€‚

- **2016å¹´**ï¼šFacebook å‘å¸ƒ PyTorch 0.1 ç‰ˆæœ¬
- **2017å¹´**ï¼šPyTorch 0.2 å¼•å…¥åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
- **2018å¹´**ï¼šPyTorch 1.0 å‘å¸ƒï¼Œå¢åŠ äº†ç”Ÿäº§éƒ¨ç½²èƒ½åŠ›
- **2019å¹´**ï¼šPyTorch 1.3 å¼•å…¥ç§»åŠ¨ç«¯æ”¯æŒ
- **2020å¹´**ï¼šPyTorch 1.6 å¢åŠ äº†è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ
- **2021å¹´**ï¼šPyTorch 1.9 å¼•å…¥ TorchScript å’Œ C++ å‰ç«¯
- **2022å¹´**ï¼šPyTorch 1.12 ä¼˜åŒ–äº†æ€§èƒ½å’Œç¨³å®šæ€§
- **2023å¹´**ï¼šPyTorch 2.0 å‘å¸ƒï¼Œå¼•å…¥ç¼–è¯‘æ¨¡å¼å¤§å¹…æå‡æ€§èƒ½



## A.3 PyTorch åŸºç¡€

PyTorch æ˜¯ä¸€ä¸ªå¼€æºçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä»¥å…¶çµæ´»æ€§å’ŒåŠ¨æ€è®¡ç®—å›¾è€Œå¹¿å—æ¬¢è¿ã€‚

PyTorch ä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªåŸºç¡€æ¦‚å¿µï¼šå¼ é‡ï¼ˆTensorï¼‰ã€è‡ªåŠ¨æ±‚å¯¼ï¼ˆAutogradï¼‰ã€ç¥ç»ç½‘ç»œæ¨¡å—ï¼ˆnn.Moduleï¼‰ã€ä¼˜åŒ–å™¨ï¼ˆoptimï¼‰ç­‰ã€‚

- **å¼ é‡ï¼ˆTensorï¼‰**ï¼šPyTorch çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œæ”¯æŒå¤šç»´æ•°ç»„ï¼Œå¹¶å¯ä»¥åœ¨ CPU æˆ– GPU ä¸Šè¿›è¡ŒåŠ é€Ÿè®¡ç®—ã€‚
- **è‡ªåŠ¨æ±‚å¯¼ï¼ˆAutogradï¼‰**ï¼šPyTorch æä¾›äº†è‡ªåŠ¨æ±‚å¯¼åŠŸèƒ½ï¼Œå¯ä»¥è½»æ¾è®¡ç®—æ¨¡å‹çš„æ¢¯åº¦ï¼Œä¾¿äºè¿›è¡Œåå‘ä¼ æ’­å’Œä¼˜åŒ–ã€‚
- **ç¥ç»ç½‘ç»œï¼ˆnn.Moduleï¼‰**ï¼šPyTorch æä¾›äº†ç®€å•ä¸”å¼ºå¤§çš„ API æ¥æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå¯ä»¥æ–¹ä¾¿åœ°è¿›è¡Œå‰å‘ä¼ æ’­å’Œæ¨¡å‹å®šä¹‰ã€‚
- **ä¼˜åŒ–å™¨ï¼ˆOptimizersï¼‰**ï¼šä½¿ç”¨ä¼˜åŒ–å™¨ï¼ˆå¦‚ Adamã€SGD ç­‰ï¼‰æ¥æ›´æ–°æ¨¡å‹çš„å‚æ•°ï¼Œä½¿å¾—æŸå¤±æœ€å°åŒ–ã€‚
- **è®¾å¤‡ï¼ˆDeviceï¼‰**ï¼šå¯ä»¥å°†æ¨¡å‹å’Œå¼ é‡ç§»åŠ¨åˆ° GPU ä¸Šä»¥åŠ é€Ÿè®¡ç®—ã€‚

------

### PyTorch æ¶æ„æ€»è§ˆ

PyTorch é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œç”±å¤šä¸ªç›¸äº’åä½œçš„æ ¸å¿ƒç»„ä»¶æ„æˆã€‚ç†è§£è¿™äº›ç»„ä»¶çš„ä½œç”¨å’Œç›¸äº’å…³ç³»ï¼Œæ˜¯æŒæ¡ PyTorch çš„å…³é”®ã€‚

**PyTorch æ¶æ„å›¾**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PyTorch ç”Ÿæ€ç³»ç»Ÿ                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  torchvision  â”‚  torchtext  â”‚  torchaudio  â”‚  å…¶ä»–ä¸“ä¸šåº“     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     PyTorch æ ¸å¿ƒ                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   torch.nn    â”‚   torch.optim   â”‚      torch.utils          â”‚
â”‚   (ç¥ç»ç½‘ç»œ)   â”‚   (ä¼˜åŒ–å™¨)      â”‚      (å·¥å…·å‡½æ•°)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚               â”‚                 â”‚   torch.utils.data        â”‚
â”‚  torch æ ¸å¿ƒ   â”‚  autograd       â”‚   (æ•°æ®åŠ è½½)              â”‚
â”‚  (å¼ é‡è®¡ç®—)   â”‚  (è‡ªåŠ¨å¾®åˆ†)     â”‚                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

PyTorch é‡‡ç”¨**åˆ†å±‚æ¶æ„**è®¾è®¡ï¼Œä»ä¸Šå±‚åˆ°åº•å±‚ä¾æ¬¡ä¸ºï¼š 

**1ã€Python APIï¼ˆé¡¶å±‚ï¼‰**

- `torch`ï¼šæ ¸å¿ƒå¼ é‡è®¡ç®—ï¼ˆç±»ä¼¼NumPyï¼Œæ”¯æŒGPUï¼‰ã€‚ 
- `torch.nn`ï¼šç¥ç»ç½‘ç»œå±‚ã€æŸå¤±å‡½æ•°ç­‰ã€‚ 
- `torch.autograd`ï¼šè‡ªåŠ¨å¾®åˆ†ï¼ˆåå‘ä¼ æ’­ï¼‰ã€‚ 
- å¼€å‘è€…ç›´æ¥è°ƒç”¨çš„æ¥å£ï¼Œç®€å•æ˜“ç”¨ã€‚ 

**2ã€C++æ ¸å¿ƒï¼ˆä¸­å±‚ï¼‰**

- **ATen**ï¼šå¼ é‡è¿ç®—æ ¸å¿ƒåº“ï¼ˆ400+æ“ä½œï¼‰ã€‚ 
- **JIT**ï¼šå³æ—¶ç¼–è¯‘ä¼˜åŒ–æ¨¡å‹ã€‚ 
- **Autogradå¼•æ“**ï¼šè‡ªåŠ¨å¾®åˆ†çš„åº•å±‚å®ç°ã€‚ 
- é«˜æ€§èƒ½è®¡ç®—ï¼Œè¿æ¥Pythonä¸åº•å±‚ç¡¬ä»¶ã€‚ 

**3ã€åŸºç¡€åº“ï¼ˆåº•å±‚ï¼‰**

- **TH/THNN**ï¼šCè¯­è¨€å®ç°çš„åŸºç¡€å¼ é‡å’Œç¥ç»ç½‘ç»œæ“ä½œã€‚ 
- **THC/THCUNN**ï¼šå¯¹åº”çš„CUDAï¼ˆGPUï¼‰ç‰ˆæœ¬ã€‚ 
- ç›´æ¥æ“ä½œç¡¬ä»¶ï¼ˆCPU/GPUï¼‰ï¼Œæè‡´ä¼˜åŒ–é€Ÿåº¦ã€‚ 

**æ‰§è¡Œæµç¨‹**ï¼š
Pythonä»£ç  â†’ C++æ ¸å¿ƒè®¡ç®— â†’ åº•å±‚CUDA/Cåº“åŠ é€Ÿ â†’ è¿”å›ç»“æœã€‚
æ—¢ä¿æŒæ˜“ç”¨æ€§ï¼Œåˆç¡®ä¿é«˜æ€§èƒ½ã€‚

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/iGWbOXL.png" alt="img" style="zoom:67%;" />

**å¼ é‡ï¼ˆTensorï¼‰**

å¼ é‡ï¼ˆTensorï¼‰æ˜¯ PyTorch ä¸­çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œç”¨äºå­˜å‚¨å’Œæ“ä½œå¤šç»´æ•°ç»„ã€‚

å¼ é‡å¯ä»¥è§†ä¸ºä¸€ä¸ªå¤šç»´æ•°ç»„ï¼Œæ”¯æŒåŠ é€Ÿè®¡ç®—çš„æ“ä½œã€‚

åœ¨ PyTorch ä¸­ï¼Œå¼ é‡çš„æ¦‚å¿µç±»ä¼¼äº NumPy ä¸­çš„æ•°ç»„ï¼Œä½†æ˜¯ PyTorch çš„å¼ é‡å¯ä»¥è¿è¡Œåœ¨ä¸åŒçš„è®¾å¤‡ä¸Šï¼Œæ¯”å¦‚ CPU å’Œ GPUï¼Œè¿™ä½¿å¾—å®ƒä»¬éå¸¸é€‚åˆäºè¿›è¡Œå¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—ï¼Œç‰¹åˆ«æ˜¯åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸã€‚

- **ç»´åº¦ï¼ˆDimensionalityï¼‰**ï¼šå¼ é‡çš„ç»´åº¦æŒ‡çš„æ˜¯æ•°æ®çš„å¤šç»´æ•°ç»„ç»“æ„ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªæ ‡é‡ï¼ˆ0ç»´å¼ é‡ï¼‰æ˜¯ä¸€ä¸ªå•ç‹¬çš„æ•°å­—ï¼Œä¸€ä¸ªå‘é‡ï¼ˆ1ç»´å¼ é‡ï¼‰æ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼Œä¸€ä¸ªçŸ©é˜µï¼ˆ2ç»´å¼ é‡ï¼‰æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œä»¥æ­¤ç±»æ¨ã€‚
- **å½¢çŠ¶ï¼ˆShapeï¼‰**ï¼šå¼ é‡çš„å½¢çŠ¶æ˜¯æŒ‡æ¯ä¸ªç»´åº¦ä¸Šçš„å¤§å°ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå½¢çŠ¶ä¸º`(3, 4)`çš„å¼ é‡æ„å‘³ç€å®ƒæœ‰3è¡Œ4åˆ—ã€‚
- **æ•°æ®ç±»å‹ï¼ˆDtypeï¼‰**ï¼šå¼ é‡ä¸­çš„æ•°æ®ç±»å‹å®šä¹‰äº†å­˜å‚¨æ¯ä¸ªå…ƒç´ æ‰€éœ€çš„å†…å­˜å¤§å°å’Œè§£é‡Šæ–¹å¼ã€‚PyTorchæ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼ŒåŒ…æ‹¬æ•´æ•°å‹ï¼ˆå¦‚`torch.int8`ã€`torch.int32`ï¼‰ã€æµ®ç‚¹å‹ï¼ˆå¦‚`torch.float32`ã€`torch.float64`ï¼‰å’Œå¸ƒå°”å‹ï¼ˆ`torch.bool`ï¼‰ã€‚

**å¼ é‡åˆ›å»ºï¼š**

**å®ä¾‹**

```python
import torch

# åˆ›å»ºä¸€ä¸ª 2x3 çš„å…¨ 0 å¼ é‡
a = torch.zeros(2, 3)
print(a)

# åˆ›å»ºä¸€ä¸ª 2x3 çš„å…¨ 1 å¼ é‡
b = torch.ones(2, 3)
print(b)

# åˆ›å»ºä¸€ä¸ª 2x3 çš„éšæœºæ•°å¼ é‡
c = torch.randn(2, 3)
print(c)

# ä» NumPy æ•°ç»„åˆ›å»ºå¼ é‡
import numpy as np
numpy_array = np.array([[1, 2], [3, 4]])
tensor_from_numpy = torch.from_numpy(numpy_array)
print(tensor_from_numpy)

# åœ¨æŒ‡å®šè®¾å¤‡ï¼ˆCPU/GPUï¼‰ä¸Šåˆ›å»ºå¼ é‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
d = torch.randn(2, 3, device=device)
print(d)
```

è¾“å‡ºç»“æœç±»ä¼¼å¦‚ä¸‹ï¼š

```
tensor([[0., 0., 0.],
        [0., 0., 0.]])
tensor([[1., 1., 1.],
        [1., 1., 1.]])
tensor([[ 1.0189, -0.5718, -1.2814],
        [-0.5865,  1.0855,  1.1727]])
tensor([[1, 2],
        [3, 4]])
tensor([[-0.3360,  0.2203,  1.3463],
        [-0.5982, -0.2704,  0.5429]])
```

**å¸¸ç”¨å¼ é‡æ“ä½œï¼š**

**å®ä¾‹**

```py
# å¼ é‡ç›¸åŠ 
e = torch.randn(2, 3)
f = torch.randn(2, 3)
print(e + f)

# é€å…ƒç´ ä¹˜æ³•
print(e * f)

# å¼ é‡çš„è½¬ç½®
g = torch.randn(3, 2)
print(g.t()) # æˆ–è€… g.transpose(0, 1)

# å¼ é‡çš„å½¢çŠ¶
print(g.shape) # è¿”å›å½¢çŠ¶
```



**å¼ é‡ä¸è®¾å¤‡**

PyTorch å¼ é‡å¯ä»¥å­˜åœ¨äºä¸åŒçš„è®¾å¤‡ä¸Šï¼ŒåŒ…æ‹¬CPUå’ŒGPUï¼Œä½ å¯ä»¥å°†å¼ é‡ç§»åŠ¨åˆ° GPU ä¸Šä»¥åŠ é€Ÿè®¡ç®—ï¼š

```
if torch.cuda.is_available():
    tensor_gpu = tensor_from_list.to('cuda')  # å°†å¼ é‡ç§»åŠ¨åˆ°GPU
```

**æ¢¯åº¦å’Œè‡ªåŠ¨å¾®åˆ†**

PyTorchçš„å¼ é‡æ”¯æŒè‡ªåŠ¨å¾®åˆ†ï¼Œè¿™æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„å…³é”®ç‰¹æ€§ã€‚å½“ä½ åˆ›å»ºä¸€ä¸ªéœ€è¦æ¢¯åº¦çš„å¼ é‡æ—¶ï¼ŒPyTorchå¯ä»¥è‡ªåŠ¨è®¡ç®—å…¶æ¢¯åº¦ï¼š

**å®ä¾‹**

```python
# åˆ›å»ºä¸€ä¸ªéœ€è¦æ¢¯åº¦çš„å¼ é‡
tensor_requires_grad = torch.tensor([1.0], requires_grad=True)

# è¿›è¡Œä¸€äº›æ“ä½œ
tensor_result = tensor_requires_grad * 2

# è®¡ç®—æ¢¯åº¦
tensor_result.backward()
print(tensor_requires_grad.grad) # è¾“å‡ºæ¢¯åº¦
```



**å†…å­˜å’Œæ€§èƒ½**

PyTorch å¼ é‡è¿˜æä¾›äº†ä¸€äº›å†…å­˜ç®¡ç†åŠŸèƒ½ï¼Œæ¯”å¦‚.clone()ã€.detach() å’Œ .to() æ–¹æ³•ï¼Œå®ƒä»¬å¯ä»¥å¸®åŠ©ä½ ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œæé«˜æ€§èƒ½ã€‚

------

### è‡ªåŠ¨æ±‚å¯¼ï¼ˆAutogradï¼‰

è‡ªåŠ¨æ±‚å¯¼ï¼ˆAutomatic Differentiationï¼Œç®€ç§°Autogradï¼‰æ˜¯æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒç‰¹æ€§ï¼Œå®ƒå…è®¸è®¡ç®—æœºè‡ªåŠ¨è®¡ç®—æ•°å­¦å‡½æ•°çš„å¯¼æ•°ã€‚

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œè‡ªåŠ¨æ±‚å¯¼ä¸»è¦ç”¨äºä¸¤ä¸ªæ–¹é¢ï¼š**ä¸€æ˜¯åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶è®¡ç®—æ¢¯åº¦**ï¼Œ**äºŒæ˜¯è¿›è¡Œåå‘ä¼ æ’­ç®—æ³•çš„å®ç°**ã€‚

è‡ªåŠ¨æ±‚å¯¼åŸºäºé“¾å¼æ³•åˆ™ï¼ˆChain Ruleï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè®¡ç®—å¤æ‚å‡½æ•°å¯¼æ•°çš„æ•°å­¦æ³•åˆ™ã€‚é“¾å¼æ³•åˆ™è¡¨æ˜ï¼Œå¤åˆå‡½æ•°çš„å¯¼æ•°æ˜¯å…¶å„ä¸ªç»„æˆéƒ¨åˆ†å¯¼æ•°çš„ä¹˜ç§¯ã€‚åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ¨¡å‹é€šå¸¸æ˜¯ç”±è®¸å¤šå±‚ç»„æˆçš„å¤æ‚å‡½æ•°ï¼Œè‡ªåŠ¨æ±‚å¯¼èƒ½å¤Ÿé«˜æ•ˆåœ°è®¡ç®—è¿™äº›å±‚çš„æ¢¯åº¦ã€‚

**åŠ¨æ€å›¾ä¸é™æ€å›¾ï¼š**

- **åŠ¨æ€å›¾ï¼ˆDynamic Graphï¼‰**ï¼šåœ¨åŠ¨æ€å›¾ä¸­ï¼Œè®¡ç®—å›¾åœ¨è¿è¡Œæ—¶åŠ¨æ€æ„å»ºã€‚æ¯æ¬¡æ‰§è¡Œæ“ä½œæ—¶ï¼Œè®¡ç®—å›¾éƒ½ä¼šæ›´æ–°ï¼Œè¿™ä½¿å¾—è°ƒè¯•å’Œä¿®æ”¹æ¨¡å‹å˜å¾—æ›´åŠ å®¹æ˜“ã€‚PyTorchä½¿ç”¨çš„æ˜¯åŠ¨æ€å›¾ã€‚
- **é™æ€å›¾ï¼ˆStatic Graphï¼‰**ï¼šåœ¨é™æ€å›¾ä¸­ï¼Œè®¡ç®—å›¾åœ¨å¼€å§‹æ‰§è¡Œä¹‹å‰æ„å»ºå®Œæˆï¼Œå¹¶ä¸”ä¸ä¼šæ”¹å˜ã€‚TensorFlowæœ€åˆä½¿ç”¨çš„æ˜¯é™æ€å›¾ï¼Œä½†åæ¥ä¹Ÿæ”¯æŒåŠ¨æ€å›¾ã€‚

PyTorch æä¾›äº†è‡ªåŠ¨æ±‚å¯¼åŠŸèƒ½ï¼Œé€šè¿‡ autograd æ¨¡å—æ¥è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ã€‚

torch.Tensor å¯¹è±¡æœ‰ä¸€ä¸ª requires_grad å±æ€§ï¼Œç”¨äºæŒ‡ç¤ºæ˜¯å¦éœ€è¦è®¡ç®—è¯¥å¼ é‡çš„æ¢¯åº¦ã€‚

å½“ä½ åˆ›å»ºä¸€ä¸ª requires_grad=True çš„å¼ é‡æ—¶ï¼ŒPyTorch ä¼šè‡ªåŠ¨è·Ÿè¸ªæ‰€æœ‰å¯¹å®ƒçš„æ“ä½œï¼Œä»¥ä¾¿åœ¨ä¹‹åè®¡ç®—æ¢¯åº¦ã€‚

åˆ›å»ºéœ€è¦æ¢¯åº¦çš„å¼ é‡:

**å®ä¾‹**

```python
# åˆ›å»ºä¸€ä¸ªéœ€è¦è®¡ç®—æ¢¯åº¦çš„å¼ é‡
x = torch.randn(2, 2, requires_grad=True)
print(x)

# æ‰§è¡ŒæŸäº›æ“ä½œ
y = x + 2
z = y * y * 3
out = z.mean()

print(out)


```

è¾“å‡ºç»“æœç±»ä¼¼å¦‚ä¸‹ï¼š

```
tensor([[0., 0., 0.],
        [0., 0., 0.]])
tensor([[1., 1., 1.],
        [1., 1., 1.]])
tensor([[ 1.0189, -0.5718, -1.2814],
        [-0.5865,  1.0855,  1.1727]])
tensor([[1, 2],
        [3, 4]])
tensor([[-0.3360,  0.2203,  1.3463],
        [-0.5982, -0.2704,  0.5429]])
tianqixin@Mac-mini runoob-test % python3 test.py
tensor([[-0.1908,  0.2811],
        [ 0.8068,  0.8002]], requires_grad=True)
tensor(18.1469, grad_fn=<MeanBackward0>)
```

### åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰

ä¸€æ—¦å®šä¹‰äº†è®¡ç®—å›¾ï¼Œå¯ä»¥é€šè¿‡ **.backward()** æ–¹æ³•æ¥è®¡ç®—æ¢¯åº¦ã€‚

**å®ä¾‹**

```python
# åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦
out.backward()

# æŸ¥çœ‹ x çš„æ¢¯åº¦
print(x.grad)
```

åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¸­ï¼Œè‡ªåŠ¨æ±‚å¯¼ä¸»è¦ç”¨äºå®ç°åå‘ä¼ æ’­ç®—æ³•ã€‚

åå‘ä¼ æ’­æ˜¯ä¸€ç§é€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°å…³äºç½‘ç»œå‚æ•°çš„æ¢¯åº¦æ¥è®­ç»ƒç¥ç»ç½‘ç»œçš„æ–¹æ³•ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œç½‘ç»œçš„å‰å‘ä¼ æ’­ä¼šè®¡ç®—è¾“å‡ºå’ŒæŸå¤±ï¼Œç„¶ååå‘ä¼ æ’­ä¼šè®¡ç®—æŸå¤±å…³äºæ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¢¯åº¦æ¥æ›´æ–°å‚æ•°ã€‚

**åœæ­¢æ¢¯åº¦è®¡ç®—**

å¦‚æœä½ ä¸å¸Œæœ›æŸäº›å¼ é‡çš„æ¢¯åº¦è¢«è®¡ç®—ï¼ˆä¾‹å¦‚ï¼Œå½“ä½ ä¸éœ€è¦åå‘ä¼ æ’­æ—¶ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ **torch.no_grad()** æˆ–è®¾ç½® **requires_grad=False**ã€‚

**å®ä¾‹**

```python
# ä½¿ç”¨ torch.no_grad() ç¦ç”¨æ¢¯åº¦è®¡ç®—
with torch.no_grad():
  y = x * 2
```



------

### ç¥ç»ç½‘ç»œï¼ˆnn.Moduleï¼‰

ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§æ¨¡ä»¿äººè„‘ç¥ç»å…ƒè¿æ¥çš„è®¡ç®—æ¨¡å‹ï¼Œç”±å¤šå±‚èŠ‚ç‚¹ï¼ˆç¥ç»å…ƒï¼‰ç»„æˆï¼Œç”¨äºå­¦ä¹ æ•°æ®ä¹‹é—´çš„å¤æ‚æ¨¡å¼å’Œå…³ç³»ã€‚

ç¥ç»ç½‘ç»œé€šè¿‡è°ƒæ•´ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥æƒé‡æ¥ä¼˜åŒ–é¢„æµ‹ç»“æœï¼Œè¿™ä¸€è¿‡ç¨‹æ¶‰åŠå‰å‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°ã€‚

ç¥ç»ç½‘ç»œçš„ç±»å‹åŒ…æ‹¬å‰é¦ˆç¥ç»ç½‘ç»œã€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ï¼Œå®ƒä»¬åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³å¤„ç†ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚

PyTorch æä¾›äº†ä¸€ä¸ªéå¸¸æ–¹ä¾¿çš„æ¥å£æ¥æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå³ **torch.nn.Module**ã€‚

æˆ‘ä»¬å¯ä»¥ç»§æ‰¿ nn.Module ç±»å¹¶å®šä¹‰è‡ªå·±çš„ç½‘ç»œå±‚ã€‚

åˆ›å»ºä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œï¼š

**å®ä¾‹**

```python
import torch.nn as nn
import torch.optim as optim

# å®šä¹‰ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œ
class SimpleNN(nn.Module):
  def init(self):
    super(SimpleNN, self).__init__()
    self.fc1 = nn.Linear(2, 2) # è¾“å…¥å±‚åˆ°éšè—å±‚
    self.fc2 = nn.Linear(2, 1) # éšè—å±‚åˆ°è¾“å‡ºå±‚

  def forward(self, x):
    x = torch.relu(self.fc1(x)) # ReLU æ¿€æ´»å‡½æ•°
    x = self.fc2(x)
    return x

# åˆ›å»ºç½‘ç»œå®ä¾‹
model = SimpleNN()

# æ‰“å°æ¨¡å‹ç»“æ„
print(model)

```

è¾“å‡ºç»“æœï¼š

```
SimpleNN(
  (fc1): Linear(in_features=2, out_features=2, bias=True)
  (fc2): Linear(in_features=2, out_features=1, bias=True)
)
```

**è®­ç»ƒè¿‡ç¨‹ï¼š**

1. **å‰å‘ä¼ æ’­ï¼ˆForward Propagationï¼‰**ï¼š åœ¨å‰å‘ä¼ æ’­é˜¶æ®µï¼Œè¾“å…¥æ•°æ®é€šè¿‡ç½‘ç»œå±‚ä¼ é€’ï¼Œæ¯å±‚åº”ç”¨æƒé‡å’Œæ¿€æ´»å‡½æ•°ï¼Œç›´åˆ°äº§ç”Ÿè¾“å‡ºã€‚
2. **è®¡ç®—æŸå¤±ï¼ˆCalculate Lossï¼‰**ï¼š æ ¹æ®ç½‘ç»œçš„è¾“å‡ºå’ŒçœŸå®æ ‡ç­¾ï¼Œè®¡ç®—æŸå¤±å‡½æ•°çš„å€¼ã€‚
3. **åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰**ï¼š åå‘ä¼ æ’­åˆ©ç”¨è‡ªåŠ¨æ±‚å¯¼æŠ€æœ¯è®¡ç®—æŸå¤±å‡½æ•°å…³äºæ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚
4. **å‚æ•°æ›´æ–°ï¼ˆParameter Updateï¼‰**ï¼š ä½¿ç”¨ä¼˜åŒ–å™¨æ ¹æ®æ¢¯åº¦æ›´æ–°ç½‘ç»œçš„æƒé‡å’Œåç½®ã€‚
5. **è¿­ä»£ï¼ˆIterationï¼‰**ï¼š é‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼Œç›´åˆ°æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„æ€§èƒ½è¾¾åˆ°æ»¡æ„çš„æ°´å¹³ã€‚

### å‰å‘ä¼ æ’­ä¸æŸå¤±è®¡ç®—

**å®ä¾‹**

```python
# éšæœºè¾“å…¥
x = torch.randn(1, 2)

# å‰å‘ä¼ æ’­
output = model(x)
print(output)

# å®šä¹‰æŸå¤±å‡½æ•°ï¼ˆä¾‹å¦‚å‡æ–¹è¯¯å·® MSEï¼‰
criterion = nn.MSELoss()

# å‡è®¾ç›®æ ‡å€¼ä¸º 1
target = torch.randn(1, 1)

# è®¡ç®—æŸå¤±
loss = criterion(output, target)
print(loss)
```



### ä¼˜åŒ–å™¨ï¼ˆOptimizersï¼‰

ä¼˜åŒ–å™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°ç¥ç»ç½‘ç»œçš„å‚æ•°ï¼Œä»¥å‡å°‘æŸå¤±å‡½æ•°çš„å€¼ã€‚

PyTorch æä¾›äº†å¤šç§ä¼˜åŒ–å™¨ï¼Œä¾‹å¦‚ SGDã€Adam ç­‰ã€‚

ä½¿ç”¨ä¼˜åŒ–å™¨è¿›è¡Œå‚æ•°æ›´æ–°ï¼š

**å®ä¾‹**

```python
# å®šä¹‰ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨ Adam ä¼˜åŒ–å™¨ï¼‰
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒæ­¥éª¤
optimizer.zero_grad() # æ¸…ç©ºæ¢¯åº¦
loss.backward() # åå‘ä¼ æ’­
optimizer.step() # æ›´æ–°å‚æ•°
```



------

### è®­ç»ƒæ¨¡å‹

è®­ç»ƒæ¨¡å‹æ˜¯æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä¸­çš„æ ¸å¿ƒè¿‡ç¨‹ï¼Œæ—¨åœ¨é€šè¿‡å¤§é‡æ•°æ®å­¦ä¹ æ¨¡å‹å‚æ•°ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿå¯¹æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®åšå‡ºå‡†ç¡®çš„é¢„æµ‹ã€‚

è®­ç»ƒæ¨¡å‹é€šå¸¸åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1. **æ•°æ®å‡†å¤‡**ï¼š
   - æ”¶é›†å’Œå¤„ç†æ•°æ®ï¼ŒåŒ…æ‹¬æ¸…æ´—ã€æ ‡å‡†åŒ–å’Œå½’ä¸€åŒ–ã€‚
   - å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚
2. **å®šä¹‰æ¨¡å‹**ï¼š
   - é€‰æ‹©æ¨¡å‹æ¶æ„ï¼Œä¾‹å¦‚å†³ç­–æ ‘ã€ç¥ç»ç½‘ç»œç­‰ã€‚
   - åˆå§‹åŒ–æ¨¡å‹å‚æ•°ï¼ˆæƒé‡å’Œåç½®ï¼‰ã€‚
3. **é€‰æ‹©æŸå¤±å‡½æ•°**ï¼š
   - æ ¹æ®ä»»åŠ¡ç±»å‹ï¼ˆå¦‚åˆ†ç±»ã€å›å½’ï¼‰é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°ã€‚
4. **é€‰æ‹©ä¼˜åŒ–å™¨**ï¼š
   - é€‰æ‹©ä¸€ä¸ªä¼˜åŒ–ç®—æ³•ï¼Œå¦‚SGDã€Adamç­‰ï¼Œæ¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
5. **å‰å‘ä¼ æ’­**ï¼š
   - åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå°†è¾“å…¥æ•°æ®é€šè¿‡æ¨¡å‹ä¼ é€’ï¼Œè®¡ç®—é¢„æµ‹è¾“å‡ºã€‚
6. **è®¡ç®—æŸå¤±**ï¼š
   - ä½¿ç”¨æŸå¤±å‡½æ•°è¯„ä¼°é¢„æµ‹è¾“å‡ºä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ã€‚
7. **åå‘ä¼ æ’­**ï¼š
   - åˆ©ç”¨è‡ªåŠ¨æ±‚å¯¼è®¡ç®—æŸå¤±ç›¸å¯¹äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚
8. **å‚æ•°æ›´æ–°**ï¼š
   - æ ¹æ®è®¡ç®—å‡ºçš„æ¢¯åº¦å’Œä¼˜åŒ–å™¨çš„ç­–ç•¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
9. **è¿­ä»£ä¼˜åŒ–**ï¼š
   - é‡å¤æ­¥éª¤5-8ï¼Œç›´åˆ°æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½ä¸å†æå‡æˆ–è¾¾åˆ°é¢„å®šçš„è¿­ä»£æ¬¡æ•°ã€‚
10. **è¯„ä¼°å’Œæµ‹è¯•**ï¼š
    - ä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°æ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½ï¼Œç¡®ä¿æ¨¡å‹æ²¡æœ‰è¿‡æ‹Ÿåˆã€‚
11. **æ¨¡å‹è°ƒä¼˜**ï¼š
    - æ ¹æ®æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°è¿›è¡Œè°ƒå‚ï¼Œå¦‚æ”¹å˜å­¦ä¹ ç‡ã€å¢åŠ æ­£åˆ™åŒ–ç­‰ã€‚
12. **éƒ¨ç½²æ¨¡å‹**ï¼š
    - å°†è®­ç»ƒå¥½çš„æ¨¡å‹éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œç”¨äºå®é™…çš„é¢„æµ‹ä»»åŠ¡ã€‚

**å®ä¾‹**

```py
import torch
import torch.nn as nn
import torch.optim as optim

# 1. å®šä¹‰ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œæ¨¡å‹
class SimpleNN(nn.Module):
  def init(self):
    super(SimpleNN, self).init()
    self.fc1 = nn.Linear(2, 2) # è¾“å…¥å±‚åˆ°éšè—å±‚
    self.fc2 = nn.Linear(2, 1) # éšè—å±‚åˆ°è¾“å‡ºå±‚

  def forward(self, x):
    x = torch.relu(self.fc1(x)) # ReLU æ¿€æ´»å‡½æ•°
    x = self.fc2(x)
    return x

# 2. åˆ›å»ºæ¨¡å‹å®ä¾‹
model = SimpleNN()

# 3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.MSELoss() # å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°
optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam ä¼˜åŒ–å™¨

# 4. å‡è®¾æˆ‘ä»¬æœ‰è®­ç»ƒæ•°æ® X å’Œ Y
X = torch.randn(10, 2) # 10 ä¸ªæ ·æœ¬ï¼Œ2 ä¸ªç‰¹å¾
Y = torch.randn(10, 1) # 10 ä¸ªç›®æ ‡å€¼

# 5. è®­ç»ƒå¾ªç¯
for epoch in range(100):  # è®­ç»ƒ 100 è½®
  optimizer.zero_grad() # æ¸…ç©ºä¹‹å‰çš„æ¢¯åº¦
  output = model(X) # å‰å‘ä¼ æ’­
  loss = criterion(output, Y) # è®¡ç®—æŸå¤±
  loss.backward() # åå‘ä¼ æ’­
  optimizer.step() # æ›´æ–°å‚æ•°

  # æ¯ 10 è½®è¾“å‡ºä¸€æ¬¡æŸå¤±
  if (epoch+1) % 10 == 0:
    print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```
Epoch [10/100], Loss: 1.7180
Epoch [20/100], Loss: 1.6352
Epoch [30/100], Loss: 1.5590
Epoch [40/100], Loss: 1.4896
Epoch [50/100], Loss: 1.4268
Epoch [60/100], Loss: 1.3704
Epoch [70/100], Loss: 1.3198
Epoch [80/100], Loss: 1.2747
Epoch [90/100], Loss: 1.2346
Epoch [100/100], Loss: 1.1991
```

åœ¨æ¯ 10 è½®ï¼Œç¨‹åºä¼šè¾“å‡ºå½“å‰çš„æŸå¤±å€¼ï¼Œå¸®åŠ©æˆ‘ä»¬è·Ÿè¸ªæ¨¡å‹çš„è®­ç»ƒè¿›åº¦ã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼ŒæŸå¤±å€¼åº”è¯¥ä¼šé€æ¸é™ä½ï¼Œè¡¨ç¤ºæ¨¡å‹åœ¨ä¸æ–­å­¦ä¹ å¹¶ä¼˜åŒ–å…¶å‚æ•°ã€‚

è®­ç»ƒæ¨¡å‹æ˜¯ä¸€ä¸ªè¿­ä»£çš„è¿‡ç¨‹ï¼Œéœ€è¦ä¸æ–­åœ°è°ƒæ•´å’Œä¼˜åŒ–ï¼Œç›´åˆ°è¾¾åˆ°æ»¡æ„çš„æ€§èƒ½ã€‚è¿™ä¸ªè¿‡ç¨‹æ¶‰åŠåˆ°å¤§é‡çš„å®éªŒå’Œè°ƒä¼˜ï¼Œç›®çš„æ˜¯ä½¿æ¨¡å‹åœ¨æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ä¸Šä¹Ÿèƒ½æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

------

### è®¾å¤‡ï¼ˆDeviceï¼‰

PyTorch å…è®¸ä½ å°†æ¨¡å‹å’Œæ•°æ®ç§»åŠ¨åˆ° GPU ä¸Šè¿›è¡ŒåŠ é€Ÿã€‚

ä½¿ç”¨ **torch.device** æ¥æŒ‡å®šè®¡ç®—è®¾å¤‡ã€‚

å°†æ¨¡å‹å’Œæ•°æ®ç§»è‡³ GPU:

**å®ä¾‹**

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
model.to(device)

# å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
X = X.to(device)
Y = Y.to(device)
```

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ‰€æœ‰å¼ é‡å’Œæ¨¡å‹éƒ½åº”è¯¥ç§»åˆ°åŒä¸€ä¸ªè®¾å¤‡ä¸Šï¼ˆè¦ä¹ˆéƒ½åœ¨ CPU ä¸Šï¼Œè¦ä¹ˆéƒ½åœ¨ GPU ä¸Šï¼‰ã€‚



## A.4 PyTorch å¼ é‡ï¼ˆTensorï¼‰

å¼ é‡æ˜¯ä¸€ä¸ªå¤šç»´æ•°ç»„ï¼Œå¯ä»¥æ˜¯æ ‡é‡ã€å‘é‡ã€çŸ©é˜µæˆ–æ›´é«˜ç»´åº¦çš„æ•°æ®ç»“æ„ã€‚

åœ¨ PyTorch ä¸­ï¼Œå¼ é‡ï¼ˆTensorï¼‰æ˜¯æ•°æ®çš„æ ¸å¿ƒè¡¨ç¤ºå½¢å¼ï¼Œç±»ä¼¼äº NumPy çš„å¤šç»´æ•°ç»„ï¼Œä½†å…·æœ‰æ›´å¼ºå¤§çš„åŠŸèƒ½ï¼Œä¾‹å¦‚æ”¯æŒ GPU åŠ é€Ÿå’Œè‡ªåŠ¨æ¢¯åº¦è®¡ç®—ã€‚

å¼ é‡æ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼ˆæ•´å‹ã€æµ®ç‚¹å‹ã€å¸ƒå°”å‹ç­‰ï¼‰ã€‚

å¼ é‡å¯ä»¥å­˜å‚¨åœ¨ CPU æˆ– GPU ä¸­ï¼ŒGPU å¼ é‡å¯æ˜¾è‘—åŠ é€Ÿè®¡ç®—ã€‚

ä¸‹å›¾å±•ç¤ºäº†ä¸åŒç»´åº¦çš„å¼ é‡ï¼ˆTensorï¼‰åœ¨ PyTorch ä¸­çš„è¡¨ç¤ºæ–¹æ³•ï¼š

![img](https://raw.githubusercontent.com/GMyhf/img/main/img/1__D5ZvufDS38WkhK9rK32hQ.jpg)

**è¯´æ˜ï¼š**

- **1D Tensor / Vectorï¼ˆä¸€ç»´å¼ é‡/å‘é‡ï¼‰:** æœ€åŸºæœ¬çš„å¼ é‡å½¢å¼ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå›¾ä¸­çš„ä¾‹å­æ˜¯ä¸€ä¸ªåŒ…å« 10 ä¸ªå…ƒç´ çš„å‘é‡ã€‚
- **2D Tensor / Matrixï¼ˆäºŒç»´å¼ é‡/çŸ©é˜µï¼‰:** äºŒç»´æ•°ç»„ï¼Œé€šå¸¸ç”¨äºè¡¨ç¤ºçŸ©é˜µï¼Œå›¾ä¸­çš„ä¾‹å­æ˜¯ä¸€ä¸ª 4x5 çš„çŸ©é˜µï¼ŒåŒ…å«äº† 20 ä¸ªå…ƒç´ ã€‚
- **3D Tensor / Cubeï¼ˆä¸‰ç»´å¼ é‡/ç«‹æ–¹ä½“ï¼‰:** ä¸‰ç»´æ•°ç»„ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ç”±å¤šä¸ªçŸ©é˜µå †å è€Œæˆçš„ç«‹æ–¹ä½“ï¼Œå›¾ä¸­çš„ä¾‹å­å±•ç¤ºäº†ä¸€ä¸ª 3x4x5 çš„ç«‹æ–¹ä½“ï¼Œå…¶ä¸­æ¯ä¸ª 5x5 çš„çŸ©é˜µä»£è¡¨ç«‹æ–¹ä½“çš„ä¸€ä¸ª"å±‚"ã€‚
- **4D Tensor / Vector of Cubesï¼ˆå››ç»´å¼ é‡/ç«‹æ–¹ä½“å‘é‡ï¼‰:** å››ç»´æ•°ç»„ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ç”±å¤šä¸ªç«‹æ–¹ä½“ç»„æˆçš„å‘é‡ï¼Œå›¾ä¸­çš„ä¾‹å­æ²¡æœ‰å…·ä½“æ•°å€¼ï¼Œä½†å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªåŒ…å«å¤šä¸ª 3D å¼ é‡çš„é›†åˆã€‚
- **5D Tensor / Matrix of Cubesï¼ˆäº”ç»´å¼ é‡/ç«‹æ–¹ä½“çŸ©é˜µï¼‰:** äº”ç»´æ•°ç»„ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ç”±å¤šä¸ª4Då¼ é‡ç»„æˆçš„çŸ©é˜µï¼Œå›¾ä¸­çš„ä¾‹å­åŒæ ·æ²¡æœ‰å…·ä½“æ•°å€¼ï¼Œä½†å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªåŒ…å«å¤šä¸ª 4D å¼ é‡çš„é›†åˆã€‚

------

**åˆ›å»ºå¼ é‡**

å¼ é‡åˆ›å»ºçš„æ–¹å¼æœ‰ï¼š

| **æ–¹æ³•**                            | **è¯´æ˜**                                               | **ç¤ºä¾‹ä»£ç **                                |
| :---------------------------------- | :----------------------------------------------------- | :------------------------------------------ |
| `torch.tensor(data)`                | ä» Python åˆ—è¡¨æˆ– NumPy æ•°ç»„åˆ›å»ºå¼ é‡ã€‚                  | `x = torch.tensor([[1, 2], [3, 4]])`        |
| `torch.zeros(size)`                 | åˆ›å»ºä¸€ä¸ªå…¨ä¸ºé›¶çš„å¼ é‡ã€‚                                 | `x = torch.zeros((2, 3))`                   |
| `torch.ones(size)`                  | åˆ›å»ºä¸€ä¸ªå…¨ä¸º 1 çš„å¼ é‡ã€‚                                | `x = torch.ones((2, 3))`                    |
| `torch.empty(size)`                 | åˆ›å»ºä¸€ä¸ªæœªåˆå§‹åŒ–çš„å¼ é‡ã€‚                               | `x = torch.empty((2, 3))`                   |
| `torch.rand(size)`                  | åˆ›å»ºä¸€ä¸ªæœä»å‡åŒ€åˆ†å¸ƒçš„éšæœºå¼ é‡ï¼Œå€¼åœ¨ `[0, 1)`ã€‚        | `x = torch.rand((2, 3))`                    |
| `torch.randn(size)`                 | åˆ›å»ºä¸€ä¸ªæœä»æ­£æ€åˆ†å¸ƒçš„éšæœºå¼ é‡ï¼Œå‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1ã€‚ | `x = torch.randn((2, 3))`                   |
| `torch.arange(start, end, step)`    | åˆ›å»ºä¸€ä¸ªä¸€ç»´åºåˆ—å¼ é‡ï¼Œç±»ä¼¼äº Python çš„ `range`ã€‚       | `x = torch.arange(0, 10, 2)`                |
| `torch.linspace(start, end, steps)` | åˆ›å»ºä¸€ä¸ªåœ¨æŒ‡å®šèŒƒå›´å†…ç­‰é—´éš”çš„åºåˆ—å¼ é‡ã€‚                 | `x = torch.linspace(0, 1, 5)`               |
| `torch.eye(size)`                   | åˆ›å»ºä¸€ä¸ªå•ä½çŸ©é˜µï¼ˆå¯¹è§’çº¿ä¸º 1ï¼Œå…¶ä»–ä¸º 0ï¼‰ã€‚             | `x = torch.eye(3)`                          |
| `torch.from_numpy(ndarray)`         | å°† NumPy æ•°ç»„è½¬æ¢ä¸ºå¼ é‡ã€‚                              | `x = torch.from_numpy(np.array([1, 2, 3]))` |

ä½¿ç”¨ **torch.tensor()** å‡½æ•°ï¼Œä½ å¯ä»¥å°†ä¸€ä¸ªåˆ—è¡¨æˆ–æ•°ç»„è½¬æ¢ä¸ºå¼ é‡ï¼š

**å®ä¾‹**

```python
import torch

tensor = torch.tensor([1, 2, 3])
print(tensor)
```

è¾“å‡ºå¦‚ä¸‹ï¼š

```
tensor([1, 2, 3])
```

å¦‚æœä½ æœ‰ä¸€ä¸ª NumPy æ•°ç»„ï¼Œå¯ä»¥ä½¿ç”¨ torch.from_numpy() å°†å…¶è½¬æ¢ä¸ºå¼ é‡ï¼š

å®ä¾‹

```python
import numpy as np

np_array = np.array([1, 2, 3])
tensor = torch.from_numpy(np_array)
print(tensor)
```

è¾“å‡ºå¦‚ä¸‹ï¼š

```
tensor([1, 2, 3])
```

åˆ›å»º 2D å¼ é‡ï¼ˆçŸ©é˜µï¼‰ï¼š

**å®ä¾‹**

```python
import torch

tensor_2d = torch.tensor([
  [-9, 4, 2, 5, 7],
  [3, 0, 12, 8, 6],
  [1, 23, -6, 45, 2],
  [22, 3, -1, 72, 6]
])
print("2D Tensor (Matrix):\n", tensor_2d)
print("Shape:", tensor_2d.shape) # å½¢çŠ¶
```

è¾“å‡ºå¦‚ä¸‹ï¼š

```
2D Tensor (Matrix):
 tensor([[-9,  4,  2,  5,  7],
        [ 3,  0, 12,  8,  6],
        [ 1, 23, -6, 45,  2],
        [22,  3, -1, 72,  6]])
Shape: torch.Size([4, 5])
```

å…¶ä»–ç»´åº¦çš„åˆ›å»ºï¼š

```python
# åˆ›å»º 3D å¼ é‡ï¼ˆç«‹æ–¹ä½“ï¼‰
tensor_3d = torch.stack([tensor_2d, tensor_2d + 10, tensor_2d - 5])  # å †å  3 ä¸ª 2D å¼ é‡
print("3D Tensor (Cube):\n", tensor_3d)
print("Shape:", tensor_3d.shape)  # å½¢çŠ¶

# åˆ›å»º 4D å¼ é‡ï¼ˆå‘é‡çš„ç«‹æ–¹ä½“ï¼‰
tensor_4d = torch.stack([tensor_3d, tensor_3d + 100])  # å †å  2 ä¸ª 3D å¼ é‡
print("4D Tensor (Vector of Cubes):\n", tensor_4d)
print("Shape:", tensor_4d.shape)  # å½¢çŠ¶

# åˆ›å»º 5D å¼ é‡ï¼ˆçŸ©é˜µçš„ç«‹æ–¹ä½“ï¼‰
tensor_5d = torch.stack([tensor_4d, tensor_4d + 1000])  # å †å  2 ä¸ª 4D å¼ é‡
print("5D Tensor (Matrix of Cubes):\n", tensor_5d)
print("Shape:", tensor_5d.shape)  # å½¢çŠ¶
```

------

**å¼ é‡çš„å±æ€§**

å¼ é‡çš„å±æ€§å¦‚ä¸‹è¡¨ï¼š

| **å±æ€§**           | **è¯´æ˜**                         | **ç¤ºä¾‹**                 |
| :----------------- | :------------------------------- | :----------------------- |
| `.shape`           | è·å–å¼ é‡çš„å½¢çŠ¶                   | `tensor.shape`           |
| `.size()`          | è·å–å¼ é‡çš„å½¢çŠ¶                   | `tensor.size()`          |
| `.dtype`           | è·å–å¼ é‡çš„æ•°æ®ç±»å‹               | `tensor.dtype`           |
| `.device`          | æŸ¥çœ‹å¼ é‡æ‰€åœ¨çš„è®¾å¤‡ (CPU/GPU)     | `tensor.device`          |
| `.dim()`           | è·å–å¼ é‡çš„ç»´åº¦æ•°                 | `tensor.dim()`           |
| `.requires_grad`   | æ˜¯å¦å¯ç”¨æ¢¯åº¦è®¡ç®—                 | `tensor.requires_grad`   |
| `.numel()`         | è·å–å¼ é‡ä¸­çš„å…ƒç´ æ€»æ•°             | `tensor.numel()`         |
| `.is_cuda`         | æ£€æŸ¥å¼ é‡æ˜¯å¦åœ¨ GPU ä¸Š            | `tensor.is_cuda`         |
| `.T`               | è·å–å¼ é‡çš„è½¬ç½®ï¼ˆé€‚ç”¨äº 2D å¼ é‡ï¼‰ | `tensor.T`               |
| `.item()`          | è·å–å•å…ƒç´ å¼ é‡çš„å€¼               | `tensor.item()`          |
| `.is_contiguous()` | æ£€æŸ¥å¼ é‡æ˜¯å¦è¿ç»­å­˜å‚¨             | `tensor.is_contiguous()` |

**å®ä¾‹**

```python
import torch

# åˆ›å»ºä¸€ä¸ª 2D å¼ é‡
tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)

# å¼ é‡çš„å±æ€§
print("Tensor:\n", tensor)
print("Shape:", tensor.shape) # è·å–å½¢çŠ¶
print("Size:", tensor.size()) # è·å–å½¢çŠ¶ï¼ˆå¦ä¸€ç§æ–¹æ³•ï¼‰
print("Data Type:", tensor.dtype) # æ•°æ®ç±»å‹
print("Device:", tensor.device) # è®¾å¤‡
print("Dimensions:", tensor.dim()) # ç»´åº¦æ•°
print("Total Elements:", tensor.numel()) # å…ƒç´ æ€»æ•°
print("Requires Grad:", tensor.requires_grad) # æ˜¯å¦å¯ç”¨æ¢¯åº¦
print("Is CUDA:", tensor.is_cuda) # æ˜¯å¦åœ¨ GPU ä¸Š
print("Is Contiguous:", tensor.is_contiguous()) # æ˜¯å¦è¿ç»­å­˜å‚¨

\# è·å–å•å…ƒç´ å€¼
single_value = torch.tensor(42)
print("Single Element Value:", single_value.item())

\# è½¬ç½®å¼ é‡
tensor_T = tensor.T
print("Transposed Tensor:\n", tensor_T)
```

è¾“å‡ºç»“æœï¼š

```
Tensor:
 tensor([[1., 2., 3.],
         [4., 5., 6.]])
Shape: torch.Size([2, 3])
Size: torch.Size([2, 3])
Data Type: torch.float32
Device: cpu
Dimensions: 2
Total Elements: 6
Requires Grad: False
Is CUDA: False
Is Contiguous: True
Single Element Value: 42
Transposed Tensor:
 tensor([[1., 4.],
         [2., 5.],
         [3., 6.]])
```

------

**å¼ é‡çš„æ“ä½œ**

å¼ é‡æ“ä½œæ–¹æ³•è¯´æ˜å¦‚ä¸‹ã€‚

**åŸºç¡€æ“ä½œï¼š**

| **æ“ä½œ**                | **è¯´æ˜**                       | **ç¤ºä¾‹ä»£ç **                  |
| :---------------------- | :----------------------------- | :---------------------------- |
| `+`, `-`, `*`, `/`      | å…ƒç´ çº§åŠ æ³•ã€å‡æ³•ã€ä¹˜æ³•ã€é™¤æ³•ã€‚ | `z = x + y`                   |
| `torch.matmul(x, y)`    | çŸ©é˜µä¹˜æ³•ã€‚                     | `z = torch.matmul(x, y)`      |
| `torch.dot(x, y)`       | å‘é‡ç‚¹ç§¯ï¼ˆä»…é€‚ç”¨äº 1D å¼ é‡ï¼‰ã€‚ | `z = torch.dot(x, y)`         |
| `torch.sum(x)`          | æ±‚å’Œã€‚                         | `z = torch.sum(x)`            |
| `torch.mean(x)`         | æ±‚å‡å€¼ã€‚                       | `z = torch.mean(x)`           |
| `torch.max(x)`          | æ±‚æœ€å¤§å€¼ã€‚                     | `z = torch.max(x)`            |
| `torch.min(x)`          | æ±‚æœ€å°å€¼ã€‚                     | `z = torch.min(x)`            |
| `torch.argmax(x, dim)`  | è¿”å›æœ€å¤§å€¼çš„ç´¢å¼•ï¼ˆæŒ‡å®šç»´åº¦ï¼‰ã€‚ | `z = torch.argmax(x, dim=1)`  |
| `torch.softmax(x, dim)` | è®¡ç®— softmaxï¼ˆæŒ‡å®šç»´åº¦ï¼‰ã€‚     | `z = torch.softmax(x, dim=1)` |

å½¢çŠ¶æ“ä½œ

| **æ“ä½œ**                 | **è¯´æ˜**                       | **ç¤ºä¾‹ä»£ç **                   |
| :----------------------- | :----------------------------- | :----------------------------- |
| `x.view(shape)`          | æ”¹å˜å¼ é‡çš„å½¢çŠ¶ï¼ˆä¸æ”¹å˜æ•°æ®ï¼‰ã€‚ | `z = x.view(3, 4)`             |
| `x.reshape(shape)`       | ç±»ä¼¼äº `view`ï¼Œä½†æ›´çµæ´»ã€‚      | `z = x.reshape(3, 4)`          |
| `x.t()`                  | è½¬ç½®çŸ©é˜µã€‚                     | `z = x.t()`                    |
| `x.unsqueeze(dim)`       | åœ¨æŒ‡å®šç»´åº¦æ·»åŠ ä¸€ä¸ªç»´åº¦ã€‚       | `z = x.unsqueeze(0)`           |
| `x.squeeze(dim)`         | å»æ‰æŒ‡å®šç»´åº¦ä¸º 1 çš„ç»´åº¦ã€‚      | `z = x.squeeze(0)`             |
| `torch.cat((x, y), dim)` | æŒ‰æŒ‡å®šç»´åº¦è¿æ¥å¤šä¸ªå¼ é‡ã€‚       | `z = torch.cat((x, y), dim=1)` |

**å®ä¾‹**

```python
import torch

# åˆ›å»ºä¸€ä¸ª 2D å¼ é‡
tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)
print("åŸå§‹å¼ é‡:\n", tensor)

# 1. ç´¢å¼•å’Œåˆ‡ç‰‡æ“ä½œ
print("\nã€ç´¢å¼•å’Œåˆ‡ç‰‡ã€‘")
print("è·å–ç¬¬ä¸€è¡Œ:", tensor[0]) # è·å–ç¬¬ä¸€è¡Œ
print("è·å–ç¬¬ä¸€è¡Œç¬¬ä¸€åˆ—çš„å…ƒç´ :", tensor[0, 0]) # è·å–ç‰¹å®šå…ƒç´ 
print("è·å–ç¬¬äºŒåˆ—çš„æ‰€æœ‰å…ƒç´ :", tensor[:, 1]) # è·å–ç¬¬äºŒåˆ—æ‰€æœ‰å…ƒç´ 

# 2. å½¢çŠ¶å˜æ¢æ“ä½œ
print("\nã€å½¢çŠ¶å˜æ¢ã€‘")
reshaped = tensor.view(3, 2) # æ”¹å˜å¼ é‡å½¢çŠ¶ä¸º 3x2
print("æ”¹å˜å½¢çŠ¶åçš„å¼ é‡:\n", reshaped)
flattened = tensor.flatten() # å°†å¼ é‡å±•å¹³æˆä¸€ç»´
print("å±•å¹³åçš„å¼ é‡:\n", flattened)

# 3. æ•°å­¦è¿ç®—æ“ä½œ
print("\nã€æ•°å­¦è¿ç®—ã€‘")
tensor_add = tensor + 10 # å¼ é‡åŠ æ³•
print("å¼ é‡åŠ  10:\n", tensor_add)
tensor_mul = tensor * 2 # å¼ é‡ä¹˜æ³•
print("å¼ é‡ä¹˜ 2:\n", tensor_mul)
tensor_sum = tensor.sum() # è®¡ç®—æ‰€æœ‰å…ƒç´ çš„å’Œ
print("å¼ é‡å…ƒç´ çš„å’Œ:", tensor_sum.item())

# 4. ä¸å…¶ä»–å¼ é‡çš„æ“ä½œ
print("\nã€ä¸å…¶ä»–å¼ é‡æ“ä½œã€‘")
tensor2 = torch.tensor([[1, 1, 1], [1, 1, 1]], dtype=torch.float32)
print("å¦ä¸€ä¸ªå¼ é‡:\n", tensor2)
tensor_dot = torch.matmul(tensor, tensor2.T) # å¼ é‡çŸ©é˜µä¹˜æ³•
print("çŸ©é˜µä¹˜æ³•ç»“æœ:\n", tensor_dot)

# 5. æ¡ä»¶åˆ¤æ–­å’Œç­›é€‰
print("\nã€æ¡ä»¶åˆ¤æ–­å’Œç­›é€‰ã€‘")
mask = tensor > 3 # åˆ›å»ºä¸€ä¸ªå¸ƒå°”æ©ç 
print("å¤§äº 3 çš„å…ƒç´ çš„å¸ƒå°”æ©ç :\n", mask)
filtered_tensor = tensor[tensor > 3] # ç­›é€‰å‡ºç¬¦åˆæ¡ä»¶çš„å…ƒç´ 
print("å¤§äº 3 çš„å…ƒç´ :\n", filtered_tensor)
```

è¾“å‡ºç»“æœï¼š

```
åŸå§‹å¼ é‡:
 tensor([[1., 2., 3.],
         [4., 5., 6.]])

ã€ç´¢å¼•å’Œåˆ‡ç‰‡ã€‘
è·å–ç¬¬ä¸€è¡Œ: tensor([1., 2., 3.])
è·å–ç¬¬ä¸€è¡Œç¬¬ä¸€åˆ—çš„å…ƒç´ : tensor(1.)
è·å–ç¬¬äºŒåˆ—çš„æ‰€æœ‰å…ƒç´ : tensor([2., 5.])

ã€å½¢çŠ¶å˜æ¢ã€‘
æ”¹å˜å½¢çŠ¶åçš„å¼ é‡:
 tensor([[1., 2.],
         [3., 4.],
         [5., 6.]])
å±•å¹³åçš„å¼ é‡:
 tensor([1., 2., 3., 4., 5., 6.])

ã€æ•°å­¦è¿ç®—ã€‘
å¼ é‡åŠ  10:
 tensor([[11., 12., 13.],
         [14., 15., 16.]])
å¼ é‡ä¹˜ 2:
 tensor([[ 2.,  4.,  6.],
         [ 8., 10., 12.]])
å¼ é‡å…ƒç´ çš„å’Œ: 21.0

ã€ä¸å…¶ä»–å¼ é‡æ“ä½œã€‘
å¦ä¸€ä¸ªå¼ é‡:
 tensor([[1., 1., 1.],
         [1., 1., 1.]])
çŸ©é˜µä¹˜æ³•ç»“æœ:
 tensor([[ 6.,  6.],
         [15., 15.]])

ã€æ¡ä»¶åˆ¤æ–­å’Œç­›é€‰ã€‘
å¤§äº 3 çš„å…ƒç´ çš„å¸ƒå°”æ©ç :
 tensor([[False, False, False],
         [ True,  True,  True]])
å¤§äº 3 çš„å…ƒç´ :
 tensor([4., 5., 6.])
```

------

**å¼ é‡çš„ GPU åŠ é€Ÿ**

å°†å¼ é‡è½¬ç§»åˆ° GPUï¼š

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
x = torch.tensor([1.0, 2.0, 3.0], device=device)
```

æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨ï¼š

```python
torch.cuda.is_available()  # è¿”å› True æˆ– False
```

------

**å¼ é‡ä¸ NumPy çš„äº’æ“ä½œ**

å¼ é‡ä¸ NumPy çš„äº’æ“ä½œå¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š

| **æ“ä½œ**                    | **è¯´æ˜**                                   | **ç¤ºä¾‹ä»£ç **                     |
| :-------------------------- | :----------------------------------------- | :------------------------------- |
| `torch.from_numpy(ndarray)` | å°† NumPy æ•°ç»„è½¬æ¢ä¸ºå¼ é‡ã€‚                  | `x = torch.from_numpy(np_array)` |
| `x.numpy()`                 | å°†å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„ï¼ˆä»…é™ CPU å¼ é‡ï¼‰ã€‚ | `np_array = x.numpy()`           |

**å®ä¾‹**

```python
import torch
import numpy as np

# 1. NumPy æ•°ç»„è½¬æ¢ä¸º PyTorch å¼ é‡
print("1. NumPy è½¬ä¸º PyTorch å¼ é‡")
numpy_array = np.array([[1, 2, 3], [4, 5, 6]])
print("NumPy æ•°ç»„:\n", numpy_array)

# ä½¿ç”¨ torch.from_numpy() å°† NumPy æ•°ç»„è½¬æ¢ä¸ºå¼ é‡
tensor_from_numpy = torch.from_numpy(numpy_array)
print("è½¬æ¢åçš„ PyTorch å¼ é‡:\n", tensor_from_numpy)

# ä¿®æ”¹ NumPy æ•°ç»„ï¼Œè§‚å¯Ÿå¼ é‡çš„å˜åŒ–ï¼ˆå…±äº«å†…å­˜ï¼‰
numpy_array[0, 0] = 100
print("ä¿®æ”¹åçš„ NumPy æ•°ç»„:\n", numpy_array)
print("PyTorch å¼ é‡ä¹Ÿä¼šåŒæ­¥å˜åŒ–:\n", tensor_from_numpy)

# 2. PyTorch å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„
print("\n2. PyTorch å¼ é‡è½¬ä¸º NumPy æ•°ç»„")
tensor = torch.tensor([[7, 8, 9], [10, 11, 12]], dtype=torch.float32)
print("PyTorch å¼ é‡:\n", tensor)

# ä½¿ç”¨ tensor.numpy() å°†å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„
numpy_from_tensor = tensor.numpy()
print("è½¬æ¢åçš„ NumPy æ•°ç»„:\n", numpy_from_tensor)

# ä¿®æ”¹å¼ é‡ï¼Œè§‚å¯Ÿ NumPy æ•°ç»„çš„å˜åŒ–ï¼ˆå…±äº«å†…å­˜ï¼‰
tensor[0, 0] = 77
print("ä¿®æ”¹åçš„ PyTorch å¼ é‡:\n", tensor)
print("NumPy æ•°ç»„ä¹Ÿä¼šåŒæ­¥å˜åŒ–:\n", numpy_from_tensor)

# 3. æ³¨æ„ï¼šä¸å…±äº«å†…å­˜çš„æƒ…å†µï¼ˆéœ€è¦å¤åˆ¶æ•°æ®ï¼‰
print("\n3. ä½¿ç”¨ clone() ä¿è¯ç‹¬ç«‹æ•°æ®")
tensor_independent = torch.tensor([[13, 14, 15], [16, 17, 18]], dtype=torch.float32)
numpy_independent = tensor_independent.clone().numpy() # ä½¿ç”¨ clone å¤åˆ¶æ•°æ®
print("åŸå§‹å¼ é‡:\n", tensor_independent)
tensor_independent[0, 0] = 0 # ä¿®æ”¹å¼ é‡æ•°æ®
print("ä¿®æ”¹åçš„å¼ é‡:\n", tensor_independent)
print("NumPy æ•°ç»„ï¼ˆä¸ä¼šåŒæ­¥å˜åŒ–ï¼‰:\n", numpy_independent)
```

è¾“å‡ºç»“æœï¼š

```
1. NumPy è½¬ä¸º PyTorch å¼ é‡
NumPy æ•°ç»„:
 [[1 2 3]
 [4 5 6]]
è½¬æ¢åçš„ PyTorch å¼ é‡:
 tensor([[1, 2, 3],
         [4, 5, 6]])

ä¿®æ”¹åçš„ NumPy æ•°ç»„:
 [[100   2   3]
 [  4   5   6]]
PyTorch å¼ é‡ä¹Ÿä¼šåŒæ­¥å˜åŒ–:
 tensor([[100,   2,   3],
         [  4,   5,   6]])

2. PyTorch å¼ é‡è½¬ä¸º NumPy æ•°ç»„
PyTorch å¼ é‡:
 tensor([[ 7.,  8.,  9.],
         [10., 11., 12.]])
è½¬æ¢åçš„ NumPy æ•°ç»„:
 [[ 7.  8.  9.]
 [10. 11. 12.]]

ä¿®æ”¹åçš„ PyTorch å¼ é‡:
 tensor([[77.,  8.,  9.],
         [10., 11., 12.]])
NumPy æ•°ç»„ä¹Ÿä¼šåŒæ­¥å˜åŒ–:
 [[77.  8.  9.]
 [10. 11. 12.]]

3. ä½¿ç”¨ clone() ä¿è¯ç‹¬ç«‹æ•°æ®
åŸå§‹å¼ é‡:
 tensor([[13., 14., 15.],
         [16., 17., 18.]])
ä¿®æ”¹åçš„å¼ é‡:
 tensor([[ 0., 14., 15.],
         [16., 17., 18.]])
NumPy æ•°ç»„ï¼ˆä¸ä¼šåŒæ­¥å˜åŒ–ï¼‰:
 [[13. 14. 15.]
 [16. 17. 18.]]
```



## A.5 PyTorch ç¥ç»ç½‘ç»œåŸºç¡€

ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§æ¨¡ä»¿äººè„‘å¤„ç†ä¿¡æ¯æ–¹å¼çš„è®¡ç®—æ¨¡å‹ï¼Œå®ƒç”±è®¸å¤šç›¸äº’è¿æ¥çš„èŠ‚ç‚¹ï¼ˆç¥ç»å…ƒï¼‰ç»„æˆï¼Œè¿™äº›èŠ‚ç‚¹æŒ‰å±‚æ¬¡æ’åˆ—ã€‚

ç¥ç»ç½‘ç»œçš„å¼ºå¤§ä¹‹å¤„åœ¨äºå…¶èƒ½å¤Ÿè‡ªåŠ¨ä»å¤§é‡æ•°æ®ä¸­å­¦ä¹ å¤æ‚çš„æ¨¡å¼å’Œç‰¹å¾ï¼Œæ— éœ€äººå·¥è®¾è®¡ç‰¹å¾æå–å™¨ã€‚

éšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼Œç¥ç»ç½‘ç»œå·²ç»æˆä¸ºè§£å†³è®¸å¤šå¤æ‚é—®é¢˜çš„å…³é”®æŠ€æœ¯ã€‚

**ç¥ç»å…ƒï¼ˆNeuronï¼‰**

ç¥ç»å…ƒæ˜¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬å•å…ƒï¼Œå®ƒæ¥æ”¶è¾“å…¥ä¿¡å·ï¼Œé€šè¿‡åŠ æƒæ±‚å’Œåä¸åç½®ï¼ˆbiasï¼‰ç›¸åŠ ï¼Œç„¶åé€šè¿‡æ¿€æ´»å‡½æ•°å¤„ç†ä»¥äº§ç”Ÿè¾“å‡ºã€‚

ç¥ç»å…ƒçš„æƒé‡å’Œåç½®æ˜¯ç½‘ç»œå­¦ä¹ è¿‡ç¨‹ä¸­éœ€è¦è°ƒæ•´çš„å‚æ•°ã€‚

**è¾“å…¥å’Œè¾“å‡º:**

- **è¾“å…¥ï¼ˆInputï¼‰**ï¼šè¾“å…¥æ˜¯ç½‘ç»œçš„èµ·å§‹ç‚¹ï¼Œå¯ä»¥æ˜¯ç‰¹å¾æ•°æ®ï¼Œå¦‚å›¾åƒçš„åƒç´ å€¼æˆ–æ–‡æœ¬çš„è¯å‘é‡ã€‚
- **è¾“å‡ºï¼ˆOutputï¼‰**ï¼šè¾“å‡ºæ˜¯ç½‘ç»œçš„ç»ˆç‚¹ï¼Œè¡¨ç¤ºæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œå¦‚åˆ†ç±»ä»»åŠ¡ä¸­çš„ç±»åˆ«æ ‡ç­¾ã€‚

ç¥ç»å…ƒæ¥æ”¶å¤šä¸ªè¾“å…¥ï¼ˆä¾‹å¦‚x1, x2, ..., xnï¼‰ï¼Œå¦‚æœè¾“å…¥çš„åŠ æƒå’Œå¤§äºæ¿€æ´»é˜ˆå€¼ï¼ˆactivation potentialï¼‰ï¼Œåˆ™äº§ç”ŸäºŒè¿›åˆ¶è¾“å‡ºã€‚

![img](https://raw.githubusercontent.com/GMyhf/img/main/img/1_upfpVueoUuKPkyX3PR3KBg.png)

ç¥ç»å…ƒçš„è¾“å‡ºå¯ä»¥çœ‹ä½œæ˜¯è¾“å…¥çš„åŠ æƒå’ŒåŠ ä¸Šåç½®ï¼ˆbiasï¼‰ï¼Œç¥ç»å…ƒçš„æ•°å­¦è¡¨ç¤ºï¼š

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/f0b929045ae6eef23514bd7024be62f0.png" alt="img" style="zoom:50%;" />

è¿™é‡Œï¼Œ**wj** æ˜¯æƒé‡ï¼Œ**xj** æ˜¯è¾“å…¥ï¼Œè€Œ **Bias** æ˜¯åç½®é¡¹ã€‚

**å±‚ï¼ˆLayerï¼‰**

è¾“å…¥å±‚å’Œè¾“å‡ºå±‚ä¹‹é—´çš„å±‚è¢«ç§°ä¸ºéšè—å±‚ï¼Œå±‚ä¸å±‚ä¹‹é—´çš„è¿æ¥å¯†åº¦å’Œç±»å‹æ„æˆäº†ç½‘ç»œçš„é…ç½®ã€‚

ç¥ç»ç½‘ç»œç”±å¤šä¸ªå±‚ç»„æˆï¼ŒåŒ…æ‹¬ï¼š

- **è¾“å…¥å±‚ï¼ˆInput Layerï¼‰**ï¼šæ¥æ”¶åŸå§‹è¾“å…¥æ•°æ®ã€‚
- **éšè—å±‚ï¼ˆHidden Layerï¼‰**ï¼šå¯¹è¾“å…¥æ•°æ®è¿›è¡Œå¤„ç†ï¼Œå¯ä»¥æœ‰å¤šä¸ªéšè—å±‚ã€‚
- **è¾“å‡ºå±‚ï¼ˆOutput Layerï¼‰**ï¼šäº§ç”Ÿæœ€ç»ˆçš„è¾“å‡ºç»“æœã€‚

å…¸å‹çš„ç¥ç»ç½‘ç»œæ¶æ„:

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/1_3fA77_mLNiJTSgZFhYnU0Q3K5DV4.webp" alt="img" style="zoom: 50%;" />



**å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeedforward Neural Networkï¼ŒFNNï¼‰**

å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeedforward Neural Networkï¼ŒFNNï¼‰æ˜¯ç¥ç»ç½‘ç»œå®¶æ—ä¸­çš„åŸºæœ¬å•å…ƒã€‚

å‰é¦ˆç¥ç»ç½‘ç»œç‰¹ç‚¹æ˜¯æ•°æ®ä»è¾“å…¥å±‚å¼€å§‹ï¼Œç»è¿‡ä¸€ä¸ªæˆ–å¤šä¸ªéšè—å±‚ï¼Œæœ€ååˆ°è¾¾è¾“å‡ºå±‚ï¼Œå…¨è¿‡ç¨‹æ²¡æœ‰å¾ªç¯æˆ–åé¦ˆã€‚

![img](https://raw.githubusercontent.com/GMyhf/img/main/img/neural-net.png)

**å‰é¦ˆç¥ç»ç½‘ç»œçš„åŸºæœ¬ç»“æ„ï¼š**

- **è¾“å…¥å±‚ï¼š** æ•°æ®è¿›å…¥ç½‘ç»œçš„å…¥å£ç‚¹ã€‚è¾“å…¥å±‚çš„æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªè¾“å…¥ç‰¹å¾ã€‚
- **éšè—å±‚ï¼š**ä¸€ä¸ªæˆ–å¤šä¸ªå±‚ï¼Œç”¨äºæ•è·æ•°æ®çš„éçº¿æ€§ç‰¹å¾ã€‚æ¯ä¸ªéšè—å±‚ç”±å¤šä¸ªç¥ç»å…ƒç»„æˆï¼Œæ¯ä¸ªç¥ç»å…ƒé€šè¿‡æ¿€æ´»å‡½æ•°å¢åŠ éçº¿æ€§èƒ½åŠ›ã€‚
- **è¾“å‡ºå±‚ï¼š**è¾“å‡ºç½‘ç»œçš„é¢„æµ‹ç»“æœã€‚èŠ‚ç‚¹æ•°å’Œé—®é¢˜ç±»å‹ç›¸å…³ï¼Œä¾‹å¦‚åˆ†ç±»é—®é¢˜çš„è¾“å‡ºèŠ‚ç‚¹æ•°ç­‰äºç±»åˆ«æ•°ã€‚
- **è¿æ¥æƒé‡ä¸åç½®ï¼š**æ¯ä¸ªç¥ç»å…ƒçš„è¾“å…¥é€šè¿‡æƒé‡è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¹¶åŠ ä¸Šåç½®å€¼ï¼Œç„¶åé€šè¿‡æ¿€æ´»å‡½æ•°ä¼ é€’ã€‚



**å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Network, RNNï¼‰**

å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Network, RNNï¼‰ç»œæ˜¯ä¸€ç±»ä¸“é—¨å¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œèƒ½å¤Ÿæ•è·è¾“å…¥æ•°æ®ä¸­æ—¶é—´æˆ–é¡ºåºä¿¡æ¯çš„ä¾èµ–å…³ç³»ã€‚

RNN çš„ç‰¹åˆ«ä¹‹å¤„åœ¨äºå®ƒå…·æœ‰"è®°å¿†èƒ½åŠ›"ï¼Œå¯ä»¥åœ¨ç½‘ç»œçš„éšè—çŠ¶æ€ä¸­ä¿å­˜ä¹‹å‰æ—¶é—´æ­¥çš„ä¿¡æ¯ã€‚

å¾ªç¯ç¥ç»ç½‘ç»œç”¨äºå¤„ç†éšæ—¶é—´å˜åŒ–çš„æ•°æ®æ¨¡å¼ã€‚

åœ¨ RNN ä¸­ï¼Œç›¸åŒçš„å±‚è¢«ç”¨æ¥æ¥æ”¶è¾“å…¥å‚æ•°ï¼Œå¹¶åœ¨æŒ‡å®šçš„ç¥ç»ç½‘ç»œä¸­æ˜¾ç¤ºè¾“å‡ºå‚æ•°ã€‚

<img src="https://raw.githubusercontent.com/GMyhf/img/main/img/0_xs3Dya3qQBx6IU7C.png" alt="img" style="zoom:67%;" />

PyTorch æä¾›äº†å¼ºå¤§çš„å·¥å…·æ¥æ„å»ºå’Œè®­ç»ƒç¥ç»ç½‘ç»œã€‚

ç¥ç»ç½‘ç»œåœ¨ PyTorch ä¸­æ˜¯é€šè¿‡ **torch.nn** æ¨¡å—æ¥å®ç°çš„ã€‚

**torch.nn** æ¨¡å—æä¾›äº†å„ç§ç½‘ç»œå±‚ï¼ˆå¦‚å…¨è¿æ¥å±‚ã€å·ç§¯å±‚ç­‰ï¼‰ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼Œè®©ç¥ç»ç½‘ç»œçš„æ„å»ºå’Œè®­ç»ƒå˜å¾—æ›´åŠ æ–¹ä¾¿ã€‚

![img](https://raw.githubusercontent.com/GMyhf/img/main/img/1_3DUs-90altOgaBcVJ9LTGg.png)

åœ¨ PyTorch ä¸­ï¼Œæ„å»ºç¥ç»ç½‘ç»œé€šå¸¸éœ€è¦ç»§æ‰¿ nn.Module ç±»ã€‚

nn.Module æ˜¯æ‰€æœ‰ç¥ç»ç½‘ç»œæ¨¡å—çš„åŸºç±»ï¼Œä½ éœ€è¦å®šä¹‰ä»¥ä¸‹ä¸¤ä¸ªéƒ¨åˆ†ï¼š

- **`__init__()`**ï¼šå®šä¹‰ç½‘ç»œå±‚ã€‚
- **`forward()`**ï¼šå®šä¹‰æ•°æ®çš„å‰å‘ä¼ æ’­è¿‡ç¨‹ã€‚

ç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼ˆFully Connected Networkï¼‰ï¼š

**å®ä¾‹**

```python
import torch
import torch.nn as nn

# å®šä¹‰ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œæ¨¡å‹
class SimpleNN(nn.Module):
  def init(self):
    super(SimpleNN, self).init()
    # å®šä¹‰ä¸€ä¸ªè¾“å…¥å±‚åˆ°éšè—å±‚çš„å…¨è¿æ¥å±‚
    self.fc1 = nn.Linear(2, 2) # è¾“å…¥ 2 ä¸ªç‰¹å¾ï¼Œè¾“å‡º 2 ä¸ªç‰¹å¾
    # å®šä¹‰ä¸€ä¸ªéšè—å±‚åˆ°è¾“å‡ºå±‚çš„å…¨è¿æ¥å±‚
    self.fc2 = nn.Linear(2, 1) # è¾“å…¥ 2 ä¸ªç‰¹å¾ï¼Œè¾“å‡º 1 ä¸ªé¢„æµ‹å€¼

  def forward(self, x):
    # å‰å‘ä¼ æ’­è¿‡ç¨‹
    x = torch.relu(self.fc1(x)) # ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°
    x = self.fc2(x) # è¾“å‡ºå±‚
    return x

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = SimpleNN()

# æ‰“å°æ¨¡å‹
print(model)
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```
SimpleNN(
  (fc1): Linear(in_features=2, out_features=2, bias=True)
  (fc2): Linear(in_features=2, out_features=1, bias=True)
)
```

PyTorch æä¾›äº†è®¸å¤š<mark>å¸¸è§çš„ç¥ç»ç½‘ç»œå±‚</mark>ï¼Œä»¥ä¸‹æ˜¯å‡ ä¸ªå¸¸è§çš„ï¼š

- **`nn.Linear(in_features, out_features)`**ï¼šå…¨è¿æ¥å±‚ï¼Œè¾“å…¥ `in_features` ä¸ªç‰¹å¾ï¼Œè¾“å‡º `out_features` ä¸ªç‰¹å¾ã€‚
- **`nn.Conv2d(in_channels, out_channels, kernel_size)`**ï¼š2D å·ç§¯å±‚ï¼Œç”¨äºå›¾åƒå¤„ç†ã€‚
- **`nn.MaxPool2d(kernel_size)`**ï¼š2D æœ€å¤§æ± åŒ–å±‚ï¼Œç”¨äºé™ç»´ã€‚
- **`nn.ReLU()`**ï¼šReLU æ¿€æ´»å‡½æ•°ï¼Œå¸¸ç”¨äºéšè—å±‚ã€‚
- **`nn.Softmax(dim)`**ï¼šSoftmax æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸ç”¨äºè¾“å‡ºå±‚ï¼Œé€‚ç”¨äºå¤šç±»åˆ†ç±»é—®é¢˜ã€‚



**æ¿€æ´»å‡½æ•°ï¼ˆActivation Functionï¼‰**

æ¿€æ´»å‡½æ•°å†³å®šäº†ç¥ç»å…ƒæ˜¯å¦åº”è¯¥è¢«æ¿€æ´»ã€‚å®ƒä»¬æ˜¯éçº¿æ€§å‡½æ•°ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å’Œæ‰§è¡Œæ›´å¤æ‚çš„ä»»åŠ¡ã€‚å¸¸è§çš„æ¿€æ´»å‡½æ•°åŒ…æ‹¬ï¼š

- Sigmoidï¼šç”¨äºäºŒåˆ†ç±»é—®é¢˜ï¼Œè¾“å‡ºå€¼åœ¨ 0 å’Œ 1 ä¹‹é—´ã€‚
- Tanhï¼šè¾“å‡ºå€¼åœ¨ -1 å’Œ 1 ä¹‹é—´ï¼Œå¸¸ç”¨äºè¾“å‡ºå±‚ä¹‹å‰ã€‚
- ReLUï¼ˆRectified Linear Unitï¼‰ï¼šç›®å‰æœ€æµè¡Œçš„æ¿€æ´»å‡½æ•°ä¹‹ä¸€ï¼Œå®šä¹‰ä¸º `f(x) = max(0, x)`ï¼Œæœ‰åŠ©äºè§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚
- Softmaxï¼šå¸¸ç”¨äºå¤šåˆ†ç±»é—®é¢˜çš„è¾“å‡ºå±‚ï¼Œå°†è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚

**å®ä¾‹**

```python
import torch.nn.functional as F

# ReLU æ¿€æ´»
output = F.relu(input_tensor)

# Sigmoid æ¿€æ´»
output = torch.sigmoid(input_tensor)

# Tanh æ¿€æ´»
output = torch.tanh(input_tensor)
```



**æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰**

æŸå¤±å‡½æ•°ç”¨äºè¡¡é‡æ¨¡å‹çš„é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ã€‚

å¸¸è§çš„æŸå¤±å‡½æ•°åŒ…æ‹¬ï¼š

- **å‡æ–¹è¯¯å·®ï¼ˆMSELossï¼‰**ï¼šå›å½’é—®é¢˜å¸¸ç”¨ï¼Œè®¡ç®—è¾“å‡ºä¸ç›®æ ‡å€¼çš„å¹³æ–¹å·®ã€‚
- **äº¤å‰ç†µæŸå¤±ï¼ˆCrossEntropyLossï¼‰**ï¼šåˆ†ç±»é—®é¢˜å¸¸ç”¨ï¼Œè®¡ç®—è¾“å‡ºå’ŒçœŸå®æ ‡ç­¾ä¹‹é—´çš„äº¤å‰ç†µã€‚
- **BCEWithLogitsLoss**ï¼šäºŒåˆ†ç±»é—®é¢˜ï¼Œç»“åˆäº† Sigmoid æ¿€æ´»å’ŒäºŒå…ƒäº¤å‰ç†µæŸå¤±ã€‚

**å®ä¾‹**

```python
# å‡æ–¹è¯¯å·®æŸå¤±
criterion = nn.MSELoss()

# äº¤å‰ç†µæŸå¤±
criterion = nn.CrossEntropyLoss()

# äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
criterion = nn.BCEWithLogitsLoss()
```



**ä¼˜åŒ–å™¨ï¼ˆOptimizerï¼‰**

<mark>ä¼˜åŒ–å™¨è´Ÿè´£åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°ç½‘ç»œçš„æƒé‡å’Œåç½®</mark>ã€‚

å¸¸è§çš„ä¼˜åŒ–å™¨åŒ…æ‹¬ï¼š

- SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰
- Adamï¼ˆè‡ªé€‚åº”çŸ©ä¼°è®¡ï¼‰
- RMSpropï¼ˆå‡æ–¹æ ¹ä¼ æ’­ï¼‰

**å®ä¾‹**

```python
import torch.optim as optim

# ä½¿ç”¨ SGD ä¼˜åŒ–å™¨
optimizer = optim.SGD(model.parameters(), lr=0.01)

# ä½¿ç”¨ Adam ä¼˜åŒ–å™¨
optimizer = optim.Adam(model.parameters(), lr=0.001)
```



**è®­ç»ƒè¿‡ç¨‹ï¼ˆTraining Processï¼‰**

è®­ç»ƒç¥ç»ç½‘ç»œæ¶‰åŠä»¥ä¸‹æ­¥éª¤ï¼š

1. **å‡†å¤‡æ•°æ®**ï¼šé€šè¿‡ `DataLoader` åŠ è½½æ•°æ®ã€‚
2. **å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨**ã€‚
3. **å‰å‘ä¼ æ’­**ï¼šè®¡ç®—æ¨¡å‹çš„è¾“å‡ºã€‚
4. **è®¡ç®—æŸå¤±**ï¼šä¸ç›®æ ‡è¿›è¡Œæ¯”è¾ƒï¼Œå¾—åˆ°æŸå¤±å€¼ã€‚
5. **åå‘ä¼ æ’­**ï¼šé€šè¿‡ `loss.backward()` è®¡ç®—æ¢¯åº¦ã€‚
6. **æ›´æ–°å‚æ•°**ï¼šé€šè¿‡ `optimizer.step()` æ›´æ–°æ¨¡å‹çš„å‚æ•°ã€‚
7. **é‡å¤ä¸Šè¿°æ­¥éª¤**ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šçš„è®­ç»ƒè½®æ•°ã€‚

**å®ä¾‹**

```python
\# å‡è®¾å·²ç»å®šä¹‰å¥½äº†æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨

\# è®­ç»ƒæ•°æ®ç¤ºä¾‹
X = torch.randn(10, 2) # 10 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰ 2 ä¸ªç‰¹å¾
Y = torch.randn(10, 1) # 10 ä¸ªç›®æ ‡æ ‡ç­¾

\# è®­ç»ƒè¿‡ç¨‹
for epoch in range(100):  # è®­ç»ƒ 100 è½®
  model.train() # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
  optimizer.zero_grad() # æ¸…é™¤æ¢¯åº¦
  output = model(X) # å‰å‘ä¼ æ’­
  loss = criterion(output, Y) # è®¡ç®—æŸå¤±
  loss.backward() # åå‘ä¼ æ’­
  optimizer.step() # æ›´æ–°æƒé‡
  
  if (epoch + 1) % 10 == 0:  # æ¯ 10 è½®è¾“å‡ºä¸€æ¬¡æŸå¤±
    print(f'Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}')
```



**æµ‹è¯•ä¸è¯„ä¼°**

è®­ç»ƒå®Œæˆåï¼Œéœ€è¦å¯¹æ¨¡å‹è¿›è¡Œæµ‹è¯•å’Œè¯„ä¼°ã€‚

å¸¸è§çš„æ­¥éª¤åŒ…æ‹¬ï¼š

- **è®¡ç®—æµ‹è¯•é›†çš„æŸå¤±**ï¼šæµ‹è¯•æ¨¡å‹åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šçš„è¡¨ç°ã€‚
- **è®¡ç®—å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰**ï¼šå¯¹äºåˆ†ç±»é—®é¢˜ï¼Œè®¡ç®—æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ã€‚

**å®ä¾‹**

```python
# å‡è®¾ä½ æœ‰æµ‹è¯•é›† X_test å’Œ Y_test
model.eval() # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
with torch.no_grad():  # åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ç¦ç”¨æ¢¯åº¦è®¡ç®—
  output = model(X_test)
  loss = criterion(output, Y_test)
  print(f'Test Loss: {loss.item():.4f}')
```



**ç¥ç»ç½‘ç»œç±»å‹**

1. **å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeedforward Neural Networksï¼‰**ï¼šæ•°æ®å•å‘æµåŠ¨ï¼Œä»è¾“å…¥å±‚åˆ°è¾“å‡ºå±‚ï¼Œæ— åé¦ˆè¿æ¥ã€‚
2. **å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvolutional Neural Networks, CNNsï¼‰**ï¼šé€‚ç”¨äºå›¾åƒå¤„ç†ï¼Œä½¿ç”¨å·ç§¯å±‚æå–ç©ºé—´ç‰¹å¾ã€‚
3. **å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Networks, RNNsï¼‰**ï¼šé€‚ç”¨äºåºåˆ—æ•°æ®ï¼Œå¦‚æ—¶é—´åºåˆ—åˆ†æå’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œå…è®¸ä¿¡æ¯åé¦ˆå¾ªç¯ã€‚
4. **é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLong Short-Term Memory, LSTMï¼‰**ï¼šä¸€ç§ç‰¹æ®Šçš„RNNï¼Œèƒ½å¤Ÿå­¦ä¹ é•¿æœŸä¾èµ–å…³ç³»ã€‚



## A.6 PyTorch ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œ

æœ¬ç« èŠ‚æˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•ç”¨ PyTorch å®ç°ä¸€ä¸ªç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œå®Œæˆä¸€ä¸ªäºŒåˆ†ç±»ä»»åŠ¡ã€‚

ä»¥ä¸‹å®ä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ PyTorch å®ç°ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œè¿›è¡ŒäºŒåˆ†ç±»ä»»åŠ¡è®­ç»ƒã€‚

ç½‘ç»œç»“æ„åŒ…æ‹¬è¾“å…¥å±‚ã€éšè—å±‚å’Œè¾“å‡ºå±‚ï¼Œä½¿ç”¨äº† ReLU æ¿€æ´»å‡½æ•°å’Œ Sigmoid æ¿€æ´»å‡½æ•°ã€‚

é‡‡ç”¨äº†å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°å’Œéšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨ã€‚

è®­ç»ƒè¿‡ç¨‹æ˜¯é€šè¿‡å‰å‘ä¼ æ’­ã€è®¡ç®—æŸå¤±ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°æ¥é€æ­¥è°ƒæ•´æ¨¡å‹å‚æ•°ã€‚

**å®ä¾‹**

```python
# å¯¼å…¥PyTorchåº“
import torch
import torch.nn as nn

# å®šä¹‰è¾“å…¥å±‚å¤§å°ã€éšè—å±‚å¤§å°ã€è¾“å‡ºå±‚å¤§å°å’Œæ‰¹é‡å¤§å°
n_in, n_h, n_out, batch_size = 10, 5, 1, 10

# åˆ›å»ºè™šæ‹Ÿè¾“å…¥æ•°æ®å’Œç›®æ ‡æ•°æ®
x = torch.randn(batch_size, n_in) # éšæœºç”Ÿæˆè¾“å…¥æ•°æ®
y = torch.tensor([[1.0], [0.0], [0.0],
         [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]]) # ç›®æ ‡è¾“å‡ºæ•°æ®

# åˆ›å»ºé¡ºåºæ¨¡å‹ï¼ŒåŒ…å«çº¿æ€§å±‚ã€ReLUæ¿€æ´»å‡½æ•°å’ŒSigmoidæ¿€æ´»å‡½æ•°
model = nn.Sequential(
  nn.Linear(n_in, n_h), # è¾“å…¥å±‚åˆ°éšè—å±‚çš„çº¿æ€§å˜æ¢
  nn.ReLU(),       # éšè—å±‚çš„ReLUæ¿€æ´»å‡½æ•°
  nn.Linear(n_h, n_out), # éšè—å±‚åˆ°è¾“å‡ºå±‚çš„çº¿æ€§å˜æ¢
  nn.Sigmoid()      # è¾“å‡ºå±‚çš„Sigmoidæ¿€æ´»å‡½æ•°
)

# å®šä¹‰å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°å’Œéšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨
criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # å­¦ä¹ ç‡ä¸º0.01

# æ‰§è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒ
for epoch in range(50):  # è¿­ä»£50æ¬¡
  y_pred = model(x) # å‰å‘ä¼ æ’­ï¼Œè®¡ç®—é¢„æµ‹å€¼
  loss = criterion(y_pred, y) # è®¡ç®—æŸå¤±
  print('epoch: ', epoch, 'loss: ', loss.item()) # æ‰“å°æŸå¤±å€¼

  optimizer.zero_grad() # æ¸…é›¶æ¢¯åº¦
  loss.backward() # åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦
  optimizer.step() # æ›´æ–°æ¨¡å‹å‚æ•°
```

è¾“å‡ºç»“æœç±»ä¼¼å¦‚ä¸‹ï¼š

```
epoch:  0 loss:  0.2302265167236328
epoch:  1 loss:  0.23012931644916534
epoch:  2 loss:  0.23003216087818146
epoch:  3 loss:  0.22993502020835876
epoch:  4 loss:  0.22983793914318085
...
```

å®šä¹‰ç½‘ç»œå‚æ•°ï¼š

```
n_in, n_h, n_out, batch_size = 10, 5, 1, 10
```

- `n_in`ï¼šè¾“å…¥å±‚å¤§å°ä¸º 10ï¼Œå³æ¯ä¸ªæ•°æ®ç‚¹æœ‰ 10 ä¸ªç‰¹å¾ã€‚
- `n_h`ï¼šéšè—å±‚å¤§å°ä¸º 5ï¼Œå³éšè—å±‚åŒ…å« 5 ä¸ªç¥ç»å…ƒã€‚
- `n_out`ï¼šè¾“å‡ºå±‚å¤§å°ä¸º 1ï¼Œå³è¾“å‡ºä¸€ä¸ªæ ‡é‡ï¼Œè¡¨ç¤ºäºŒåˆ†ç±»ç»“æœï¼ˆ0 æˆ– 1ï¼‰ã€‚
- `batch_size`ï¼šæ¯ä¸ªæ‰¹æ¬¡åŒ…å« 10 ä¸ªæ ·æœ¬ã€‚

ç”Ÿæˆè¾“å…¥æ•°æ®å’Œç›®æ ‡æ•°æ®ï¼š

```python
x = torch.randn(batch_size, n_in)  # éšæœºç”Ÿæˆè¾“å…¥æ•°æ®
y = torch.tensor([[1.0], [0.0], [0.0], 
                 [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]])  # ç›®æ ‡è¾“å‡ºæ•°æ®
```

- `x`ï¼šéšæœºç”Ÿæˆä¸€ä¸ªå½¢çŠ¶ä¸º `(10, 10)` çš„è¾“å…¥æ•°æ®çŸ©é˜µï¼Œè¡¨ç¤º 10 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰ 10 ä¸ªç‰¹å¾ã€‚
- `y`ï¼šç›®æ ‡è¾“å‡ºæ•°æ®ï¼ˆæ ‡ç­¾ï¼‰ï¼Œè¡¨ç¤ºæ¯ä¸ªè¾“å…¥æ ·æœ¬çš„ç±»åˆ«æ ‡ç­¾ï¼ˆ0 æˆ– 1ï¼‰ï¼Œæ˜¯ä¸€ä¸ª 10Ã—1 çš„å¼ é‡ã€‚

å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹ï¼š

```python
model = nn.Sequential(
   nn.Linear(n_in, n_h),  # è¾“å…¥å±‚åˆ°éšè—å±‚çš„çº¿æ€§å˜æ¢
   nn.ReLU(),            # éšè—å±‚çš„ReLUæ¿€æ´»å‡½æ•°
   nn.Linear(n_h, n_out),  # éšè—å±‚åˆ°è¾“å‡ºå±‚çš„çº¿æ€§å˜æ¢
   nn.Sigmoid()           # è¾“å‡ºå±‚çš„Sigmoidæ¿€æ´»å‡½æ•°
)
```

`nn.Sequential` ç”¨äºæŒ‰é¡ºåºå®šä¹‰ç½‘ç»œå±‚ã€‚

- `nn.Linear(n_in, n_h)`ï¼šå®šä¹‰è¾“å…¥å±‚åˆ°éšè—å±‚çš„çº¿æ€§å˜æ¢ï¼Œè¾“å…¥ç‰¹å¾æ˜¯ 10 ä¸ªï¼Œéšè—å±‚æœ‰ 5 ä¸ªç¥ç»å…ƒã€‚
- `nn.ReLU()`ï¼šåœ¨éšè—å±‚åæ·»åŠ  ReLU æ¿€æ´»å‡½æ•°ï¼Œå¢åŠ éçº¿æ€§ã€‚
- `nn.Linear(n_h, n_out)`ï¼šå®šä¹‰éšè—å±‚åˆ°è¾“å‡ºå±‚çš„çº¿æ€§å˜æ¢ï¼Œè¾“å‡ºä¸º 1 ä¸ªç¥ç»å…ƒã€‚
- `nn.Sigmoid()`ï¼šè¾“å‡ºå±‚ä½¿ç”¨ Sigmoid æ¿€æ´»å‡½æ•°ï¼Œå°†ç»“æœæ˜ å°„åˆ° 0 åˆ° 1 ä¹‹é—´ï¼Œç”¨äºäºŒåˆ†ç±»ä»»åŠ¡ã€‚

å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼š

```python
criterion = torch.nn.MSELoss()  # ä½¿ç”¨å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ä¸º 0.01
```

è®­ç»ƒå¾ªç¯ï¼š

```python
for epoch in range(50):  # è®­ç»ƒ50è½®
   y_pred = model(x)  # å‰å‘ä¼ æ’­ï¼Œè®¡ç®—é¢„æµ‹å€¼
   loss = criterion(y_pred, y)  # è®¡ç®—æŸå¤±
   print('epoch: ', epoch, 'loss: ', loss.item())  # æ‰“å°æŸå¤±å€¼

   optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
   loss.backward()  # åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦
   optimizer.step()  # æ›´æ–°æ¨¡å‹å‚æ•°
```

- `for epoch in range(50)`ï¼šè¿›è¡Œ 50 æ¬¡è®­ç»ƒè¿­ä»£ã€‚
- `y_pred = model(x)`ï¼šè¿›è¡Œå‰å‘ä¼ æ’­ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹å‚æ•°è®¡ç®—è¾“å…¥æ•°æ® `x` çš„é¢„æµ‹å€¼ã€‚
- `loss = criterion(y_pred, y)`ï¼šè®¡ç®—é¢„æµ‹å€¼å’Œç›®æ ‡å€¼ `y` ä¹‹é—´çš„æŸå¤±ã€‚
- `optimizer.zero_grad()`ï¼šæ¸…é™¤ä¸Šä¸€è½®è®­ç»ƒæ—¶çš„æ¢¯åº¦å€¼ã€‚
- `loss.backward()`ï¼šåå‘ä¼ æ’­ï¼Œè®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚
- `optimizer.step()`ï¼šæ ¹æ®è®¡ç®—å‡ºçš„æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ã€‚

å¯è§†åŒ–ä»£ç ï¼š

**å®ä¾‹**

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

# å®šä¹‰è¾“å…¥å±‚å¤§å°ã€éšè—å±‚å¤§å°ã€è¾“å‡ºå±‚å¤§å°å’Œæ‰¹é‡å¤§å°
n_in, n_h, n_out, batch_size = 10, 5, 1, 10

# åˆ›å»ºè™šæ‹Ÿè¾“å…¥æ•°æ®å’Œç›®æ ‡æ•°æ®
x = torch.randn(batch_size, n_in) # éšæœºç”Ÿæˆè¾“å…¥æ•°æ®
y = torch.tensor([[1.0], [0.0], [0.0],
         [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]]) # ç›®æ ‡è¾“å‡ºæ•°æ®

# åˆ›å»ºé¡ºåºæ¨¡å‹ï¼ŒåŒ…å«çº¿æ€§å±‚ã€ReLUæ¿€æ´»å‡½æ•°å’ŒSigmoidæ¿€æ´»å‡½æ•°
model = nn.Sequential(
  nn.Linear(n_in, n_h), # è¾“å…¥å±‚åˆ°éšè—å±‚çš„çº¿æ€§å˜æ¢
  nn.ReLU(),       # éšè—å±‚çš„ReLUæ¿€æ´»å‡½æ•°
  nn.Linear(n_h, n_out), # éšè—å±‚åˆ°è¾“å‡ºå±‚çš„çº¿æ€§å˜æ¢
  nn.Sigmoid()      # è¾“å‡ºå±‚çš„Sigmoidæ¿€æ´»å‡½æ•°
)

# å®šä¹‰å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°å’Œéšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨
criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # å­¦ä¹ ç‡ä¸º0.01

# ç”¨äºå­˜å‚¨æ¯è½®çš„æŸå¤±å€¼
losses = []

# æ‰§è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒ
for epoch in range(50):  # è¿­ä»£50æ¬¡
  y_pred = model(x) # å‰å‘ä¼ æ’­ï¼Œè®¡ç®—é¢„æµ‹å€¼
  loss = criterion(y_pred, y) # è®¡ç®—æŸå¤±
  losses.append(loss.item()) # è®°å½•æŸå¤±å€¼
  print(f'Epoch [{epoch+1}/50], Loss: {loss.item():.4f}') # æ‰“å°æŸå¤±å€¼

  optimizer.zero_grad() # æ¸…é›¶æ¢¯åº¦
  loss.backward() # åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦
  optimizer.step() # æ›´æ–°æ¨¡å‹å‚æ•°

# å¯è§†åŒ–æŸå¤±å˜åŒ–æ›²çº¿
plt.figure(figsize=(8, 5))
plt.plot(range(1, 51), losses, label='Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Over Epochs')
plt.legend()
plt.grid()
plt.show()

# å¯è§†åŒ–é¢„æµ‹ç»“æœä¸å®é™…ç›®æ ‡å€¼å¯¹æ¯”
y_pred_final = model(x).detach().numpy() # æœ€ç»ˆé¢„æµ‹å€¼
y_actual = y.numpy() # å®é™…å€¼

plt.figure(figsize=(8, 5))
plt.plot(range(1, batch_size + 1), y_actual, 'o-', label='Actual', color='blue')
plt.plot(range(1, batch_size + 1), y_pred_final, 'x--', label='Predicted', color='red')
plt.xlabel('Sample Index')
plt.ylabel('Value')
plt.title('Actual vs Predicted Values')
plt.legend()
plt.grid()
plt.show()
```

æ˜¾ç¤ºå¦‚ä¸‹æ‰€ç¤ºï¼š

![img](https://raw.githubusercontent.com/GMyhf/img/main/img/first_n_runoob_1.png)



![a6fa1566dbdce1daa784f116d603c117](https://raw.githubusercontent.com/GMyhf/img/main/img/a6fa1566dbdce1daa784f116d603c117.png)



**å¦å¤–ä¸€ä¸ªå®ä¾‹**

æˆ‘ä»¬å‡è®¾æœ‰ä¸€ä¸ªäºŒç»´æ•°æ®é›†ï¼Œç›®æ ‡æ˜¯æ ¹æ®ç‚¹çš„ä½ç½®å°†å®ƒä»¬åˆ†ç±»åˆ°ä¸¤ä¸ªç±»åˆ«ä¸­ï¼ˆä¾‹å¦‚ï¼Œçº¢è‰²å’Œè“è‰²ç‚¹ï¼‰ã€‚

ä»¥ä¸‹å®ä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ç¥ç»ç½‘ç»œå®Œæˆç®€å•çš„äºŒåˆ†ç±»ä»»åŠ¡ï¼Œä¸ºæ›´å¤æ‚çš„ä»»åŠ¡å¥ å®šäº†åŸºç¡€ï¼Œé€šè¿‡ PyTorch çš„æ¨¡å—åŒ–æ¥å£ï¼Œç¥ç»ç½‘ç»œçš„æ„å»ºã€è®­ç»ƒå’Œå¯è§†åŒ–éƒ½éå¸¸ç›´è§‚ã€‚

**1ã€æ•°æ®å‡†å¤‡**

é¦–å…ˆï¼Œæˆ‘ä»¬ç”Ÿæˆä¸€äº›ç®€å•çš„äºŒç»´æ•°æ®ï¼š

**å®ä¾‹**

```python
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# ç”Ÿæˆä¸€äº›éšæœºæ•°æ®
n_samples = 100
data = torch.randn(n_samples, 2) # ç”Ÿæˆ 100 ä¸ªäºŒç»´æ•°æ®ç‚¹
labels = (data[:, 0]**2 + data[:, 1]**2 < 1).float().unsqueeze(1) # ç‚¹åœ¨åœ†å†…ä¸º1ï¼Œåœ†å¤–ä¸º0

# å¯è§†åŒ–æ•°æ®
plt.scatter(data[:, 0], data[:, 1], c=labels.squeeze(), cmap='coolwarm')
plt.title("Generated Data")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
```

**æ•°æ®è¯´æ˜ï¼š**

- `data` æ˜¯è¾“å…¥çš„äºŒç»´ç‚¹ï¼Œæ¯ä¸ªç‚¹æœ‰ä¸¤ä¸ªç‰¹å¾ã€‚
- `labels` æ˜¯ç›®æ ‡åˆ†ç±»ï¼Œç‚¹åœ¨åœ†å½¢åŒºåŸŸå†…ä¸º 1ï¼Œå¦åˆ™ä¸º 0ã€‚

æ˜¾ç¤ºå¦‚ä¸‹ï¼š

![cfe5820ca03600fe0c1c18945556a892](https://raw.githubusercontent.com/GMyhf/img/main/img/cfe5820ca03600fe0c1c18945556a892.png)



**2ã€å®šä¹‰ç¥ç»ç½‘ç»œ**

ç”¨ PyTorch åˆ›å»ºä¸€ä¸ªç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œã€‚

å‰é¦ˆç¥ç»ç½‘ç»œä½¿ç”¨äº†ä¸€å±‚éšè—å±‚ï¼Œé€šè¿‡ç®€å•çš„çº¿æ€§å˜æ¢å’Œæ¿€æ´»å‡½æ•°æ•è·æ•°æ®çš„éçº¿æ€§æ¨¡å¼ã€‚

**å®ä¾‹**

```python
class SimpleNN(nn.Module):
  def __init__(self):
    super(SimpleNN, self).__init__()
    # å®šä¹‰ç¥ç»ç½‘ç»œçš„å±‚
    self.fc1 = nn.Linear(2, 4) # è¾“å…¥å±‚æœ‰ 2 ä¸ªç‰¹å¾ï¼Œéšè—å±‚æœ‰ 4 ä¸ªç¥ç»å…ƒ
    self.fc2 = nn.Linear(4, 1) # éšè—å±‚è¾“å‡ºåˆ° 1 ä¸ªç¥ç»å…ƒï¼ˆç”¨äºäºŒåˆ†ç±»ï¼‰
    self.sigmoid = nn.Sigmoid() # äºŒåˆ†ç±»æ¿€æ´»å‡½æ•°

  def forward(self, x):
    x = torch.relu(self.fc1(x)) # ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°
    x = self.sigmoid(self.fc2(x)) # è¾“å‡ºå±‚ä½¿ç”¨ Sigmoid æ¿€æ´»å‡½æ•°
    return x

# å®ä¾‹åŒ–æ¨¡å‹
model = SimpleNN()
```



**3ã€å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨**

**å®ä¾‹**

```python
# å®šä¹‰äºŒåˆ†ç±»çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.BCELoss() # äºŒå…ƒäº¤å‰ç†µæŸå¤±
optimizer = optim.SGD(model.parameters(), lr=0.1) # ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨
```



**4ã€è®­ç»ƒæ¨¡å‹**

ç”¨æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œè®©å®ƒå­¦ä¼šåˆ†ç±»ã€‚

**å®ä¾‹**

```python
# è®­ç»ƒ
epochs = 100
for epoch in range(epochs):
  # å‰å‘ä¼ æ’­
  outputs = model(data)
  loss = criterion(outputs, labels)

  # åå‘ä¼ æ’­
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  # æ¯ 10 è½®æ‰“å°ä¸€æ¬¡æŸå¤±
  if (epoch + 1) % 10 == 0:
    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')
```



**5ã€æµ‹è¯•æ¨¡å‹å¹¶å¯è§†åŒ–ç»“æœ**

æˆ‘ä»¬æµ‹è¯•æ¨¡å‹ï¼Œå¹¶åœ¨å›¾åƒä¸Šç»˜åˆ¶å†³ç­–è¾¹ç•Œã€‚

**å®ä¾‹**

```python
# å¯è§†åŒ–å†³ç­–è¾¹ç•Œ
def plot_decision_boundary(model, data):
  x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1
  y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1
  xx, yy = torch.meshgrid(torch.arange(x_min, x_max, 0.1), torch.arange(y_min, y_max, 0.1), indexing='ij')
  grid = torch.cat([xx.reshape(-1, 1), yy.reshape(-1, 1)], dim=1)
  predictions = model(grid).detach().numpy().reshape(xx.shape)
  plt.contourf(xx, yy, predictions, levels=[0, 0.5, 1], cmap='coolwarm', alpha=0.7)
  plt.scatter(data[:, 0], data[:, 1], c=labels.squeeze(), cmap='coolwarm', edgecolors='k')
  plt.title("Decision Boundary")
  plt.show()

plot_decision_boundary(model, data)
```



**6ã€å®Œæ•´ä»£ç **

å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š

å®ä¾‹

```python
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# ç”Ÿæˆä¸€äº›éšæœºæ•°æ®
n_samples = 100
data = torch.randn(n_samples, 2)  # ç”Ÿæˆ 100 ä¸ªäºŒç»´æ•°æ®ç‚¹
labels = (data[:, 0]**2 + data[:, 1]**2 < 1).float().unsqueeze(1)  # ç‚¹åœ¨åœ†å†…ä¸º1ï¼Œåœ†å¤–ä¸º0

# å¯è§†åŒ–æ•°æ®
plt.scatter(data[:, 0], data[:, 1], c=labels.squeeze(), cmap='coolwarm')
plt.title("Generated Data")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# å®šä¹‰å‰é¦ˆç¥ç»ç½‘ç»œ
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        # å®šä¹‰ç¥ç»ç½‘ç»œçš„å±‚
        self.fc1 = nn.Linear(2, 4)  # è¾“å…¥å±‚æœ‰ 2 ä¸ªç‰¹å¾ï¼Œéšè—å±‚æœ‰ 4 ä¸ªç¥ç»å…ƒ
        self.fc2 = nn.Linear(4, 1)  # éšè—å±‚è¾“å‡ºåˆ° 1 ä¸ªç¥ç»å…ƒï¼ˆç”¨äºäºŒåˆ†ç±»ï¼‰
        self.sigmoid = nn.Sigmoid()  # äºŒåˆ†ç±»æ¿€æ´»å‡½æ•°

    def forward(self, x):
        x = torch.relu(self.fc1(x))  # ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°
        x = self.sigmoid(self.fc2(x))  # è¾“å‡ºå±‚ä½¿ç”¨ Sigmoid æ¿€æ´»å‡½æ•°
        return x

# å®ä¾‹åŒ–æ¨¡å‹
model = SimpleNN()

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.BCELoss()  # äºŒå…ƒäº¤å‰ç†µæŸå¤±
optimizer = optim.SGD(model.parameters(), lr=0.1)  # ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨

# è®­ç»ƒ
epochs = 2000
for epoch in range(epochs):
    # å‰å‘ä¼ æ’­
    outputs = model(data)
    loss = criterion(outputs, labels)

    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # æ¯ 10 è½®æ‰“å°ä¸€æ¬¡æŸå¤±
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')

# å¯è§†åŒ–å†³ç­–è¾¹ç•Œ
def plot_decision_boundary(model, data):
    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1
    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1
    xx, yy = torch.meshgrid(torch.arange(x_min, x_max, 0.1), torch.arange(y_min, y_max, 0.1), indexing='ij')
    grid = torch.cat([xx.reshape(-1, 1), yy.reshape(-1, 1)], dim=1)
    predictions = model(grid).detach().numpy().reshape(xx.shape)
    plt.contourf(xx, yy, predictions, levels=[0, 0.5, 1], cmap='coolwarm', alpha=0.7)
    plt.scatter(data[:, 0], data[:, 1], c=labels.squeeze(), cmap='coolwarm', edgecolors='k')
    plt.title("Decision Boundary")
    plt.show()

plot_decision_boundary(model, data)
```

è®­ç»ƒæ—¶çš„æŸå¤±è¾“å‡ºï¼š

```
Epoch [10/2000], Loss: 0.6547
Epoch [20/2000], Loss: 0.6451
Epoch [30/2000], Loss: 0.6363
...
Epoch [1980/2000], Loss: 0.0664
Epoch [1990/2000], Loss: 0.0662
Epoch [2000/2000], Loss: 0.0659
```

å›¾ä¸­æ˜¾ç¤ºäº†åŸå§‹æ•°æ®ç‚¹ï¼ˆçº¢è‰²å’Œè“è‰²ï¼‰ï¼Œä»¥åŠæ¨¡å‹å­¦ä¹ åˆ°çš„åˆ†ç±»è¾¹ç•Œã€‚

![e9e452ad8bc0bc8b241d0c6c2d7d5d31](https://raw.githubusercontent.com/GMyhf/img/main/img/e9e452ad8bc0bc8b241d0c6c2d7d5d31.png)





# å‚è€ƒæ–‡çŒ®

[1] Dartmouth workshop - Wikipedia

https://en.wikipedia.org/wiki/Dartmouth_workshop

[2]OpenAI Presents GPT-3, a 175 Billion Parameters Language Model | NVIDIA Technical Blog. July 07, 2020.

https://developer.nvidia.com/blog/openai-presents-gpt-3-a-175-billion-parameters-language-model/

[3] GPT-4 Technical Report. March 27, 2023.

https://cdn.openai.com/papers/gpt-4.pdf

[4] Transformer (deep learning architecture) - Wikipedia

https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)

[5] GPT-4 | OpenAI. March 14, 2023.

https://openai.com/index/gpt-4-research/